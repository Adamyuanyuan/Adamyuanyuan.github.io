<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[大数据团队scala代码规范]]></title>
      <url>http://flume.cn/2017/01/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9B%A2%E9%98%9Fscala%E4%BB%A3%E7%A0%81%E8%A7%84%E8%8C%83/</url>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h3 id="一、-scala代码规范"><a href="#一、-scala代码规范" class="headerlink" title="一、 scala代码规范"></a>一、 scala代码规范</h3><p>Scala 是一种强大到令人难以置信的多范式编程语言。目前我们团队有很多项目需要使用scala语言进行编程，尤其是Spark相关开发项目，为了能够统一scala相关开发，本人基于Spark 贡献者及 <a href="http://databricks.com/" target="_blank" rel="external">Databricks</a> 工程团队总结出了以下指南，本人增加了单元测试的规范，希望对大家的开发有帮助。当然，这个指南并非绝对，根据我们团队需求与实际经验，持续更新。</p>
<h3 id="二、-选择此规范的理由"><a href="#二、-选择此规范的理由" class="headerlink" title="二、 选择此规范的理由"></a>二、 选择此规范的理由</h3><p>很早就有大家统一编程规范的想法，网上也有一些关于编程规范的文档供参考。最终选择了以 Databricks 公司的编程规范为模板制作。理由如下：</p>
<ul>
<li>目前公司使用scala语言主要用在Spark的开发，而这份指南是Spark 贡献者及 Databricks 工程团队一起工作时总结出来的；</li>
<li>跟我们之前写得代码，以及IDEA等编译器自动格式化相差不大；</li>
<li>这份指南经过多次的修改和总结，经过了实践的检验；</li>
<li>这份指南相比较而言简明概要，易于理解，遵循 Java 的地方就没有赘述，比较解耦，适用于根据公司代码体系来修改；</li>
</ul>
<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a><a name="TOC">目录</a></h2><ol>
<li><a href="#history">文档历史</a></li>
<li><a href="#syntactic">语法风格</a><ul>
<li><a href="#naming">命名约定</a></li>
<li><a href="#variable-naming">变量命名约定</a></li>
<li><a href="#linelength">一行长度</a></li>
<li><a href="#rule_of_30">30 法则</a></li>
<li><a href="#indent">空格与缩进</a></li>
<li><a href="#blanklines">空行</a></li>
<li><a href="#parentheses">括号</a></li>
<li><a href="#curly">大括号</a></li>
<li><a href="#long_literal">长整型字面量</a></li>
<li><a href="#doc">文档风格</a></li>
<li><a href="#ordering_class">类内秩序</a></li>
<li><a href="#imports">Imports</a></li>
<li><a href="#pattern-matching">模式匹配</a></li>
<li><a href="#infix">中缀方法</a></li>
<li><a href="#anonymous">匿名方法</a></li>
</ul>
</li>
<li><a href="#lang">Scala 语言特性</a><ul>
<li><a href="#case_class_immutability">样例类与不可变性</a></li>
<li><a href="#apply_method">apply 方法</a></li>
<li><a href="#override_modifier">override 修饰符</a></li>
<li><a href="#destruct_bind">解构绑定</a></li>
<li><a href="#call_by_name">按名称传参</a></li>
<li><a href="#multi-param-list">多参数列表</a></li>
<li><a href="#symbolic_methods">符号方法 (运算符重载)</a></li>
<li><a href="#type_inference">类型推导</a></li>
<li><a href="#return">Return 语句</a></li>
<li><a href="#recursion">递归及尾递归</a></li>
<li><a href="#implicits">Implicits</a></li>
<li><a href="#exception">异常处理 (Try 还是 try)</a></li>
<li><a href="#option">Options</a></li>
<li><a href="#chaining">单子链接</a></li>
</ul>
</li>
<li><a href="#concurrency">并发</a><ul>
<li><a href="#concurrency-scala-collection">Scala concurrent.Map</a></li>
<li><a href="#concurrency-sync-vs-map">显式同步 vs 并发集合</a></li>
<li><a href="#concurrency-sync-vs-atomic">显式同步 vs 原子变量 vs @volatile</a></li>
<li><a href="#concurrency-private-this">私有字段</a></li>
<li><a href="#concurrency-isolation">隔离</a></li>
</ul>
</li>
<li><a href="#perf">性能</a><ul>
<li><a href="#perf-microbenchmarks">Microbenchmarks</a></li>
<li><a href="#perf-whileloops">Traversal 与 zipWithIndex</a></li>
<li><a href="#perf-option">Option 与 null</a></li>
<li><a href="#perf-collection">Scala 集合库</a></li>
<li><a href="#perf-private">private[this]</a></li>
</ul>
</li>
<li><a href="#java">与 Java 的互操作性</a><ul>
<li><a href="#java-missing-features">Scala 中缺失的 Java 特性</a></li>
<li><a href="#java-traits">Traits 与抽象类</a></li>
<li><a href="#java-type-alias">类型别名</a></li>
<li><a href="#java-default-param-values">默认参数值</a></li>
<li><a href="#java-multi-param-list">多参数列表</a></li>
<li><a href="#java-varargs">可变参数</a></li>
<li><a href="#java-implicits">Implicits</a></li>
<li><a href="#java-companion-object">伴生对象, 静态方法与字段</a></li>
</ul>
</li>
<li><a href="#misc">其它</a><ul>
<li><a href="#misc_currentTimeMillis_vs_nanoTime">优先使用 nanoTime 而非 currentTimeMillis</a></li>
<li><a href="#misc_uri_url">优先使用 URI 而非 URL</a></li>
</ul>
</li>
<li><a href="#unit-test">单元测试</a><ul>
<li><a href="#unit-test-framework">单元测试框架</a></li>
<li><a href="#unit-test-style">单元测试风格</a></li>
</ul>
</li>
</ol>
<h2 id="文档历史"><a href="#文档历史" class="headerlink" title="文档历史"></a><a name="history">文档历史</a></h2><ul>
<li>2017-01-10: 最初版本。<a href="https://github.com/databricks/scala-style-guide/blob/master/README-ZH.md" target="_blank" rel="external">Databricks版本</a></li>
<li>2017-01-11: 增加 <a href="#unit-test">单元测试</a> 一节。</li>
</ul>
<h2 id="语法风格"><a href="#语法风格" class="headerlink" title="语法风格"></a><a name="syntactic">语法风格</a></h2><h3 id="命名约定"><a href="#命名约定" class="headerlink" title="命名约定"></a><a name="naming">命名约定</a></h3><p>我们主要遵循 Java 和 Scala 的标准命名约定。</p>
<ul>
<li><p>类，trait, 对象应该遵循 Java 中类的命名约定，即 PascalCase 风格。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClusterManager</span></span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Expression</span></span></div></pre></td></tr></table></figure>
</li>
<li><p>包名应该遵循 Java 中包名的命名约定，即使用全小写的 ASCII 字母。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> com.databricks.resourcemanager</div></pre></td></tr></table></figure>
</li>
<li><p>方法/函数应当使用驼峰式风格命名。</p>
</li>
<li><p>常量命名使用全大写字母，并将它们放在伴生对象中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">Configuration</span> </span>&#123;</div><div class="line">  <span class="keyword">val</span> <span class="type">DEFAULT_PORT</span> = <span class="number">10000</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>枚举命名与类命名一致，使用 PascalCase 风格。</p>
</li>
<li><p>注解也应遵循 Java 中的约定，即使用 PascalCase 风格。注意，这一点与 Scala 的官方指南不同。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">MyAnnotation</span> <span class="keyword">extends</span> <span class="title">StaticAnnotation</span></span></div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="变量命名约定"><a href="#变量命名约定" class="headerlink" title="变量命名约定"></a><a name="variable-naming">变量命名约定</a></h3><ul>
<li><p>变量命名应当遵循驼峰式命名方法，并且变量名应当是不言而喻的，即变量名可以直观地反应它的涵义。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> serverPort = <span class="number">1000</span></div><div class="line"><span class="keyword">val</span> clientPort = <span class="number">2000</span></div></pre></td></tr></table></figure>
</li>
<li><p>可以在小段的局部代码中使用单字符的变量名，比如在小段的循环体中（例如 10 行以内的代码），“i” 常常被用作循环索引。然而，即使在小段的代码中，也不要使用 “l” （Larry 中的 l）作为标识符，因为它看起来和 “1”，“|”，“I” 很像，难以区分，容易搞错。</p>
</li>
</ul>
<h3 id="一行长度"><a href="#一行长度" class="headerlink" title="一行长度"></a><a name="linelength">一行长度</a></h3><ul>
<li>一行长度的上限是 100 个字符。</li>
<li>唯一的例外是 import 语句和 URL (即便如此，也尽量将它们保持在 100 个字符以下)。</li>
</ul>
<h3 id="30-法则"><a href="#30-法则" class="headerlink" title="30 法则"></a><a name="rule_of_30">30 法则</a></h3><p>「如果一个元素包含的子元素超过 30 个，那么极有可能出现了严重的问题」 - <a href="http://www.amazon.com/Refactoring-Large-Software-Projects-Restructurings/dp/0470858923" target="_blank" rel="external">Refactoring in Large Software Projects</a>。</p>
<p>一般来说:</p>
<ul>
<li>一个方法包含的代码行数不宜超过 30 行。</li>
<li>一个类包含的方法数量不宜超过 30 个。</li>
</ul>
<h3 id="空格与缩进"><a href="#空格与缩进" class="headerlink" title="空格与缩进"></a><a name="indent">空格与缩进</a></h3><ul>
<li><p>一般情况下，使用两个空格的缩进。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (<span class="literal">true</span>) &#123;</div><div class="line">  println(<span class="string">"Wow!"</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>对于方法声明，如果一行无法容纳下所有的参数，那么使用 4 个空格来缩进它们。返回类型可以与最后一个参数在同一行，也可以放在下一行，使用两个空格缩进。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">newAPIHadoopFile</span></span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">F</span> &lt;: <span class="type">NewInputFormat</span>[<span class="type">K</span>, <span class="type">V</span>]](</div><div class="line">    path: <span class="type">String</span>,</div><div class="line">    fClass: <span class="type">Class</span>[<span class="type">F</span>],</div><div class="line">    kClass: <span class="type">Class</span>[<span class="type">K</span>],</div><div class="line">    vClass: <span class="type">Class</span>[<span class="type">V</span>],</div><div class="line">    conf: <span class="type">Configuration</span> = hadoopConfiguration): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = &#123;</div><div class="line">  <span class="comment">// method body</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">newAPIHadoopFile</span></span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">F</span> &lt;: <span class="type">NewInputFormat</span>[<span class="type">K</span>, <span class="type">V</span>]](</div><div class="line">    path: <span class="type">String</span>,</div><div class="line">    fClass: <span class="type">Class</span>[<span class="type">F</span>],</div><div class="line">    kClass: <span class="type">Class</span>[<span class="type">K</span>],</div><div class="line">    vClass: <span class="type">Class</span>[<span class="type">V</span>],</div><div class="line">    conf: <span class="type">Configuration</span> = hadoopConfiguration)</div><div class="line">  : <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = &#123;</div><div class="line">  <span class="comment">// method body</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>如果一行无法容纳下类头（即 extends 后面那部分），则把它们放到新的一行，用两个空格缩进，然后在类内空一行再开始函数或字段的定义（或是包的导入）。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>(<span class="params"></span></span></div><div class="line">    val param1: <span class="type">String</span>,  // 4 space indent for parameters</div><div class="line">    val param2: <span class="type">String</span>,</div><div class="line">    val param3: <span class="type">Array</span>[<span class="type">Byte</span>])</div><div class="line">  <span class="keyword">extends</span> <span class="type">FooInterface</span>  <span class="comment">// 2 space here</span></div><div class="line">  <span class="keyword">with</span> <span class="type">Logging</span> &#123;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">firstMethod</span></span>(): <span class="type">Unit</span> = &#123; ... &#125;  <span class="comment">// blank line above</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>不要使用垂直对齐。它使你的注意力放在代码的错误部分并增大了后人修改代码的难度。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Don't align vertically</span></div><div class="line"><span class="keyword">val</span> plus     = <span class="string">"+"</span></div><div class="line"><span class="keyword">val</span> minus    = <span class="string">"-"</span></div><div class="line"><span class="keyword">val</span> multiply = <span class="string">"*"</span></div><div class="line"></div><div class="line"><span class="comment">// Do the following</span></div><div class="line"><span class="keyword">val</span> plus = <span class="string">"+"</span></div><div class="line"><span class="keyword">val</span> minus = <span class="string">"-"</span></div><div class="line"><span class="keyword">val</span> multiply = <span class="string">"*"</span></div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="空行"><a href="#空行" class="headerlink" title="空行"></a><a name="blanklines">空行</a></h3><ul>
<li>一个空行可以出现在：<ul>
<li>连续的类成员或初始化器（initializers）之间：字段，构造函数，方法，嵌套类，静态初始化器及实例初始化器。<ul>
<li>例外：连续的两个字段之间的空行是可选的（前提是它们之间没有其它代码），这一类空行主要为这些字段做逻辑上的分组。</li>
</ul>
</li>
<li>在方法体内，根据需要，使用空行来为语句创建逻辑上的分组。</li>
<li>在类的第一个成员之前或最后一个成员之后，空行都是可选的（既不鼓励也不阻止）。</li>
</ul>
</li>
<li>使用一个或两个空行来分隔不同类的定义。</li>
<li>不鼓励使用过多的空行。</li>
</ul>
<h3 id="括号"><a href="#括号" class="headerlink" title="括号"></a><a name="parentheses">括号</a></h3><ul>
<li><p>方法声明应该加括号（即使没有参数列表），除非它们是没有副作用（状态改变，IO 操作都认为是有副作用的）的访问器（accessor）。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Job</span> </span>&#123;</div><div class="line">  <span class="comment">// Wrong: killJob changes state. Should have ().</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">killJob</span></span>: <span class="type">Unit</span></div><div class="line"></div><div class="line">  <span class="comment">// Correct:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">killJob</span></span>(): <span class="type">Unit</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>函数调用应该与函数声明在形式上保持一致，也就是说，如果一个方法声明时带了括号，那调用时也要把括号带上。注意这不仅仅是语法层面的人为约定，当返回对象中定义了 <code>apply</code> 方法时，这一点还会影响正确性。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(args: <span class="type">String</span>*): <span class="type">Int</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bar</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">foo</span></span>: <span class="type">Foo</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">new</span> <span class="type">Bar</span>().foo  <span class="comment">// This returns a Foo</span></div><div class="line"><span class="keyword">new</span> <span class="type">Bar</span>().foo()  <span class="comment">// This returns an Int!</span></div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="大括号"><a href="#大括号" class="headerlink" title="大括号"></a><a name="curly">大括号</a></h3><p>即使条件语句或循环语句只有一行时，也请使用大括号。唯一的例外是，当你把 if/else 作为一个单行的三元操作符来使用并且没有副作用时，这时你可以不加大括号。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Correct:</span></div><div class="line"><span class="keyword">if</span> (<span class="literal">true</span>) &#123;</div><div class="line">  println(<span class="string">"Wow!"</span>)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Correct:</span></div><div class="line"><span class="keyword">if</span> (<span class="literal">true</span>) statement1 <span class="keyword">else</span> statement2</div><div class="line"></div><div class="line"><span class="comment">// Correct:</span></div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">  foo()</div><div class="line">&#125; <span class="keyword">catch</span> &#123;</div><div class="line">  ...</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Wrong:</span></div><div class="line"><span class="keyword">if</span> (<span class="literal">true</span>)</div><div class="line">  println(<span class="string">"Wow!"</span>)</div><div class="line"></div><div class="line"><span class="comment">// Wrong:</span></div><div class="line"><span class="keyword">try</span> foo() <span class="keyword">catch</span> &#123;</div><div class="line">  ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="长整型字面量"><a href="#长整型字面量" class="headerlink" title="长整型字面量"></a><a name="long_literal">长整型字面量</a></h3><p>长整型字面量使用大写的 <code>L</code> 作为后缀，不要使用小写，因为它和数字 <code>1</code> 长得很像，常常难以区分。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> longValue = <span class="number">5432</span>L  <span class="comment">// Do this</span></div><div class="line"></div><div class="line"><span class="keyword">val</span> longValue = <span class="number">5432</span>l  <span class="comment">// Do NOT do this</span></div></pre></td></tr></table></figure>
<h3 id="文档风格"><a href="#文档风格" class="headerlink" title="文档风格"></a><a name="doc">文档风格</a></h3><p>使用 Java Doc 风格，而非 Scala Doc 风格。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/** This is a correct one-liner, short description. */</span></div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * This is correct multi-line JavaDoc comment. And</div><div class="line"> * this is my second line, and if I keep typing, this would be</div><div class="line"> * my third line.</div><div class="line"> */</div><div class="line"></div><div class="line"><span class="comment">/** In Spark, we don't use the ScalaDoc style so this</span></div><div class="line">  * is not correct.</div><div class="line">  */</div></pre></td></tr></table></figure>
<h3 id="类内秩序"><a href="#类内秩序" class="headerlink" title="类内秩序"></a><a name="ordering_class">类内秩序</a></h3><p>如果一个类很长，包含许多的方法，那么在逻辑上把它们分成不同的部分并加上注释头，以此组织它们。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataFrame</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="comment">///////////////////////////////////////////////////////////////////////////</span></div><div class="line">  <span class="comment">// DataFrame operations</span></div><div class="line">  <span class="comment">///////////////////////////////////////////////////////////////////////////</span></div><div class="line"></div><div class="line">  ...</div><div class="line"></div><div class="line">  <span class="comment">///////////////////////////////////////////////////////////////////////////</span></div><div class="line">  <span class="comment">// RDD operations</span></div><div class="line">  <span class="comment">///////////////////////////////////////////////////////////////////////////</span></div><div class="line"></div><div class="line">  ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>当然，强烈不建议把一个类写得这么长，一般只有在构建某些公共 API 时才允许这么做。</p>
<h3 id="Imports"><a href="#Imports" class="headerlink" title="Imports"></a><a name="imports">Imports</a></h3><ul>
<li><strong>导入时避免使用通配符</strong>, 除非你需要导入超过 6 个实体或者隐式方法。通配符导入会使代码在面对外部变化时不够健壮。</li>
<li>始终使用绝对路径来导入包 (如：<code>scala.util.Random</code>) ，而不是相对路径 (如：<code>util.Random</code>)。</li>
<li>此外，导入语句按照以下顺序排序：<ul>
<li><code>java.*</code> 和 <code>javax.*</code></li>
<li><code>scala.*</code></li>
<li>第三方库 (<code>org.*</code>, <code>com.*</code>, 等)</li>
<li>项目中的类 (对于 Spark 项目，即 <code>com.databricks.*</code> 或 <code>org.apache.spark</code>)</li>
</ul>
</li>
<li>在每一组导入语句内，按照字母序进行排序。</li>
<li><p>你可以使用 IntelliJ 的「import organizer」来自动处理，请使用以下配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">java</div><div class="line">javax</div><div class="line">_______ blank line _______</div><div class="line">scala</div><div class="line">_______ blank line _______</div><div class="line">all other imports</div><div class="line">_______ blank line _______</div><div class="line">com.databricks  // or org.apache.spark if you are working on spark</div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="模式匹配"><a href="#模式匹配" class="headerlink" title="模式匹配"></a><a name="pattern-matching">模式匹配</a></h3><ul>
<li><p>如果整个方法就是一个模式匹配表达式，可能的话，可以把 match 关键词与方法声明放在同一行，以此减少一级缩进。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span></span>(msg: <span class="type">Message</span>): <span class="type">Unit</span> = msg <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>当以闭包形式调用一个函数时，如果只有一个 case 语句，那么把 case 语句与函数调用放在同一行。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">list.zipWithIndex.map &#123; <span class="keyword">case</span> (elem, i) =&gt;</div><div class="line">  <span class="comment">// ...</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果有多个 case 语句，把它们缩进并且包起来。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">list.map &#123;</div><div class="line">  <span class="keyword">case</span> a: <span class="type">Foo</span> =&gt;  ...</div><div class="line">  <span class="keyword">case</span> b: <span class="type">Bar</span> =&gt;  ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>如果唯一的目的就是想匹配某个对象的类型，那么不要展开所有的参数来做模式匹配，这样会使得重构变得更加困难，代码更容易出错。</p>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Pokemon</span>(<span class="params">name: <span class="type">String</span>, weight: <span class="type">Int</span>, hp: <span class="type">Int</span>, attack: <span class="type">Int</span>, defense: <span class="type">Int</span></span>)</span></div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Human</span>(<span class="params">name: <span class="type">String</span>, hp: <span class="type">Int</span></span>)</span></div><div class="line"></div><div class="line"><span class="comment">// 不要像下面那样做，因为</span></div><div class="line"><span class="comment">// 1. 当 pokemon 加入一个新的字段，我们需要改变下面的模式匹配代码</span></div><div class="line"><span class="comment">// 2. 非常容易发生误匹配，尤其是当所有字段的类型都一样的时候</span></div><div class="line">targets.foreach &#123;</div><div class="line">  <span class="keyword">case</span> target @ <span class="type">Pokemon</span>(_, _, hp, _, defense) =&gt;</div><div class="line">    <span class="keyword">val</span> loss = sys.min(<span class="number">0</span>, myAttack - defense)</div><div class="line">    target.copy(hp = hp - loss)</div><div class="line">  <span class="keyword">case</span> target @ <span class="type">Human</span>(_, hp) =&gt;</div><div class="line">    target.copy(hp = hp - myAttack)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 像下面这样做就好多了:</span></div><div class="line">targets.foreach &#123;</div><div class="line">  <span class="keyword">case</span> target: <span class="type">Pokemon</span> =&gt;</div><div class="line">    <span class="keyword">val</span> loss = sys.min(<span class="number">0</span>, myAttack - target.defense)</div><div class="line">    target.copy(hp = target.hp - loss)</div><div class="line">  <span class="keyword">case</span> target: <span class="type">Human</span> =&gt;</div><div class="line">    target.copy(hp = target.hp - myAttack)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="中缀方法"><a href="#中缀方法" class="headerlink" title="中缀方法"></a><a name="infix">中缀方法</a></h3><p><strong>避免中缀表示法</strong>，除非是符号方法（即运算符重载）。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Correct</span></div><div class="line">list.map(func)</div><div class="line">string.contains(<span class="string">"foo"</span>)</div><div class="line"></div><div class="line"><span class="comment">// Wrong</span></div><div class="line">list map (func)</div><div class="line">string contains <span class="string">"foo"</span></div><div class="line"></div><div class="line"><span class="comment">// 重载的运算符应该以中缀形式调用</span></div><div class="line">arrayBuffer += elem</div></pre></td></tr></table></figure>
<h3 id="匿名方法"><a href="#匿名方法" class="headerlink" title="匿名方法"></a><a name="anonymous">匿名方法</a></h3><p>对于匿名方法，<strong>避免使用过多的小括号和花括号</strong>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Correct</span></div><div class="line">list.map &#123; item =&gt;</div><div class="line">  ...</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Correct</span></div><div class="line">list.map(item =&gt; ...)</div><div class="line"></div><div class="line"><span class="comment">// Wrong</span></div><div class="line">list.map(item =&gt; &#123;</div><div class="line">  ...</div><div class="line">&#125;)</div><div class="line"></div><div class="line"><span class="comment">// Wrong</span></div><div class="line">list.map &#123; item =&gt; &#123;</div><div class="line">  ...</div><div class="line">&#125;&#125;</div><div class="line"></div><div class="line"><span class="comment">// Wrong</span></div><div class="line">list.map(&#123; item =&gt; ... &#125;)</div></pre></td></tr></table></figure>
<h2 id="Scala-语言特性"><a href="#Scala-语言特性" class="headerlink" title="Scala 语言特性"></a><a name="lang">Scala 语言特性</a></h2><h3 id="样例类与不可变性"><a href="#样例类与不可变性" class="headerlink" title="样例类与不可变性"></a><a name="case_class_immutability">样例类与不可变性</a></h3><p>样例类（case class）本质也是普通的类，编译器会自动地为它加上以下支持：</p>
<ul>
<li>构造器参数的公有 getter 方法</li>
<li>拷贝构造函数</li>
<li>构造器参数的模式匹配</li>
<li>默认的 toString/hash/equals 实现</li>
</ul>
<p>对于样例类来说，构造器参数不应设为可变的，可以使用拷贝构造函数达到同样的效果。使用可变的样例类容易出错，例如，哈希表中，对象根据旧的哈希值被放在错误的位置上。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// This is OK</span></div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></div><div class="line"></div><div class="line"><span class="comment">// This is NOT OK</span></div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, var age: <span class="type">Int</span></span>)</span></div><div class="line"></div><div class="line"><span class="comment">// 通过拷贝构造函数创建一个新的实例来改变其中的值</span></div><div class="line"><span class="keyword">val</span> p1 = <span class="type">Person</span>(<span class="string">"Peter"</span>, <span class="number">15</span>)</div><div class="line"><span class="keyword">val</span> p2 = p2.copy(age = <span class="number">16</span>)</div></pre></td></tr></table></figure>
<h3 id="apply-方法"><a href="#apply-方法" class="headerlink" title="apply 方法"></a><a name="apply_method">apply 方法</a></h3><p>避免在类里定义 apply 方法。这些方法往往会使代码的可读性变差，尤其是对于不熟悉 Scala 的人。它也难以被 IDE（或 grep）所跟踪。在最坏的情况下，它还可能影响代码的正确性，正如你在<a href="#parentheses">括号</a>一节中看到的。</p>
<p>然而，将 apply 方法作为工厂方法定义在伴生对象中是可以接受的。在这种情况下，apply 方法应该返回其伴生类的类型。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">TreeNode</span> </span>&#123;</div><div class="line">  <span class="comment">// 下面这种定义是 OK 的</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(name: <span class="type">String</span>): <span class="type">TreeNode</span> = ...</div><div class="line"></div><div class="line">  <span class="comment">// 不要像下面那样定义，因为它没有返回其伴生类的类型：TreeNode</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(name: <span class="type">String</span>): <span class="type">String</span> = ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="override-修饰符"><a href="#override-修饰符" class="headerlink" title="override 修饰符"></a><a name="override_modifier">override 修饰符</a></h3><p>无论是覆盖具体的方法还是实现抽象的方法，始终都为方法加上 override 修饰符。实现抽象方法时，不加 override 修饰符，Scala 编译器也不会报错。即便如此，我们也应该始终把 override 修饰符加上，以此显式地表示覆盖行为。以此避免由于方法签名不同（而你也难以发现）而导致没有覆盖到本应覆盖的方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Parent</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">hello</span></span>(data: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    print(data)</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Child</span> <span class="keyword">extends</span> <span class="title">Parent</span> </span>&#123;</div><div class="line">  <span class="keyword">import</span> scala.collection.<span class="type">Map</span></div><div class="line"></div><div class="line">  <span class="comment">// 下面的方法没有覆盖 Parent.hello,</span></div><div class="line">  <span class="comment">// 因为两个 Map 的类型是不同的。</span></div><div class="line">  <span class="comment">// 如果我们加上 override 修饰符，编译器就会帮你找出问题并报错。</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">hello</span></span>(data: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    print(<span class="string">"This is supposed to override the parent method, but it is actually not!"</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="解构绑定"><a href="#解构绑定" class="headerlink" title="解构绑定"></a><a name="destruct_bind">解构绑定</a></h3><p>解构绑定（有时也叫元组提取）是一种在一个表达式中为两个变量赋值的便捷方式。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> (a, b) = (<span class="number">1</span>, <span class="number">2</span>)</div></pre></td></tr></table></figure>
<p>然而，请不要在构造函数中使用它们，尤其是当 <code>a</code> 和 <code>b</code> 需要被标记为 <code>transient</code> 的时候。Scala 编译器会产生一个额外的 Tuple2 字段，而它并不是暂态的（transient）。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span> </span>&#123;</div><div class="line">  <span class="comment">// 以下代码无法 work，因为编译器会产生一个非暂态的 Tuple2 指向 a 和 b</span></div><div class="line">  <span class="meta">@transient</span> <span class="keyword">private</span> <span class="keyword">val</span> (a, b) = someFuncThatReturnsTuple2()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="按名称传参"><a href="#按名称传参" class="headerlink" title="按名称传参"></a><a name="call_by_name">按名称传参</a></h3><p><strong>避免使用按名传参</strong>. 显式地使用 <code>() =&gt; T</code> 。</p>
<p>背景：Scala 允许按名称来定义方法参数，例如：以下例子是可以成功执行的：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">print</span></span>(value: =&gt; <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  println(value)</div><div class="line">  println(value + <span class="number">1</span>)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">var</span> a = <span class="number">0</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inc</span></span>(): <span class="type">Int</span> = &#123;</div><div class="line">  a += <span class="number">1</span></div><div class="line">  a</div><div class="line">&#125;</div><div class="line"></div><div class="line">print(inc())</div></pre></td></tr></table></figure>
<p>在上面的代码中，<code>inc()</code> 以闭包的形式传递给 <code>print</code> 函数，并且在 <code>print</code> 函数中被执行了两次，而不是以数值 <code>1</code> 传入。按名传参的一个主要问题是在方法调用处，我们无法区分是按名传参还是按值传参。因此无法确切地知道这个表达式是否会被执行（更糟糕的是它可能会被执行多次）。对于带有副作用的表达式来说，这一点是非常危险的。</p>
<h3 id="多参数列表"><a href="#多参数列表" class="headerlink" title="多参数列表"></a><a name="multi-param-list">多参数列表</a></h3><p><strong>避免使用多参数列表</strong>。它们使运算符重载变得复杂，并且会使不熟悉 Scala 的程序员感到困惑。例如：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Avoid this!</span></div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)(<span class="params">secret: <span class="type">String</span></span>)</span></div></pre></td></tr></table></figure>
<p>一个值得注意的例外是，当在定义底层库时，可以使用第二个参数列表来存放隐式（implicit）参数。尽管如此，<a href="#implicits">我们应该避免使用 implicits</a>！</p>
<h3 id="符号方法（运算符重载）"><a href="#符号方法（运算符重载）" class="headerlink" title="符号方法（运算符重载）"></a><a name="symbolic_methods">符号方法（运算符重载）</a></h3><p><strong>不要使用符号作为方法名</strong>，除非你是在定义算术运算的方法（如：<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>），否则在任何其它情况下，都不要使用。符号化的方法名让人难以理解方法的意图是什么，来看下面两个例子：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 符号化的方法名难以理解</span></div><div class="line">channel ! msg</div><div class="line">stream1 &gt;&gt;= stream2</div><div class="line"></div><div class="line"><span class="comment">// 下面的方法意图则不言而喻</span></div><div class="line">channel.send(msg)</div><div class="line">stream1.join(stream2)</div></pre></td></tr></table></figure>
<h3 id="类型推导"><a href="#类型推导" class="headerlink" title="类型推导"></a><a name="type_inference">类型推导</a></h3><p>Scala 的类型推导，尤其是左侧类型推导以及闭包推导，可以使代码变得更加简洁。尽管如此，也有一些情况我们是需要显式地声明类型的：</p>
<ul>
<li><strong>公有方法应该显式地声明类型</strong>，编译器推导出来的类型往往会使你大吃一惊。</li>
<li><strong>隐式方法应该显式地声明类型</strong>，否则在增量编译时，它会使 Scala 编译器崩溃。</li>
<li><strong>如果变量或闭包的类型并非显而易见，请显式声明类型</strong>。一个不错的判断准则是，如果评审代码的人无法在 3 秒内确定相应实体的类型，那么你就应该显式地声明类型。</li>
</ul>
<h3 id="Return-语句"><a href="#Return-语句" class="headerlink" title="Return 语句"></a><a name="return">Return 语句</a></h3><p><strong>闭包中避免使用 return</strong>。<code>return</code> 会被编译器转成 <code>scala.runtime.NonLocalReturnControl</code> 异常的 <code>try/catch</code> 语句，这可能会导致意外行为。请看下面的例子：</p>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>(rpc: <span class="type">WebSocketRPC</span>): <span class="type">Option</span>[<span class="type">Response</span>] = &#123;</div><div class="line">  tableFut.onComplete &#123; table =&gt;</div><div class="line">    <span class="keyword">if</span> (table.isFailure) &#123;</div><div class="line">      <span class="keyword">return</span> <span class="type">None</span> <span class="comment">// Do not do that!</span></div><div class="line">    &#125; <span class="keyword">else</span> &#123; ... &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>.onComplete</code> 方法接收一个匿名闭包并把它传递到一个不同的线程中。这个闭包最终会抛出一个 <code>NonLocalReturnControl</code> 异常，并在 <strong>一个不同的线程中</strong>被捕获，而这里执行的方法却没有任何影响。</p>
<p>然而，也有少数情况我们是推荐使用 <code>return</code> 的。</p>
<ul>
<li><p>使用 <code>return</code> 来简化控制流，避免增加一级缩进。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">doSomething</span></span>(obj: <span class="type">Any</span>): <span class="type">Any</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (obj eq <span class="literal">null</span>) &#123;</div><div class="line">    <span class="keyword">return</span> <span class="literal">null</span></div><div class="line">  &#125;</div><div class="line">  <span class="comment">// do something ...</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>使用 <code>return</code> 来提前终止循环，这样就不用额外构造状态标志。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</div><div class="line">  <span class="keyword">if</span> (cond) &#123;</div><div class="line">    <span class="keyword">return</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="递归及尾递归"><a href="#递归及尾递归" class="headerlink" title="递归及尾递归"></a><a name="recursion">递归及尾递归</a></h3><p><strong>避免使用递归</strong>，除非问题可以非常自然地用递归来描述（比如，图和树的遍历）。</p>
<p>对于那些你意欲使之成为尾递归的方法，请加上 <code>@tailrec</code> 注解以确保编译器去检查它是否真的是尾递归（你会非常惊讶地看到，由于使用了闭包和函数变换，许多看似尾递归的代码事实并非尾递归）。</p>
<p>大多数的代码使用简单的循环和状态机会更容易推理，使用尾递归反而可能会使它更加繁琐且难以理解。例如，下面的例子中，命令式的代码比尾递归版本的代码要更加易读：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Tail recursive version.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">max</span></span>(data: <span class="type">Array</span>[<span class="type">Int</span>]): <span class="type">Int</span> = &#123;</div><div class="line">  <span class="meta">@tailrec</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">max0</span></span>(data: <span class="type">Array</span>[<span class="type">Int</span>], pos: <span class="type">Int</span>, max: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</div><div class="line">    <span class="keyword">if</span> (pos == data.length) &#123;</div><div class="line">      max</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      max0(data, pos + <span class="number">1</span>, <span class="keyword">if</span> (data(pos) &gt; max) data(pos) <span class="keyword">else</span> max)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  max0(data, <span class="number">0</span>, <span class="type">Int</span>.<span class="type">MinValue</span>)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Explicit loop version</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">max</span></span>(data: <span class="type">Array</span>[<span class="type">Int</span>]): <span class="type">Int</span> = &#123;</div><div class="line">  <span class="keyword">var</span> max = <span class="type">Int</span>.<span class="type">MinValue</span></div><div class="line">  <span class="keyword">for</span> (v &lt;- data) &#123;</div><div class="line">    <span class="keyword">if</span> (v &gt; max) &#123;</div><div class="line">      max = v</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  max</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Implicits"><a href="#Implicits" class="headerlink" title="Implicits"></a><a name="implicits">Implicits</a></h3><p><strong>避免使用 implicit</strong>，除非：</p>
<ul>
<li>你在构建领域特定的语言（DSL）</li>
<li>你在隐式类型参数中使用它（如：<code>ClassTag</code>，<code>TypeTag</code>）</li>
<li>你在你自己的类中使用它（意指不要污染外部空间），以此减少类型转换的冗余度（如：Scala 闭包到 Java 闭包的转换）。</li>
</ul>
<p>当使用 implicit 时，我们应该确保另一个工程师可以直接理解使用语义，而无需去阅读隐式定义本身。Implicit 有着非常复杂的解析规则，这会使代码变得极其难以理解。Twitter 的 Effective Scala 指南中写道：「如果你发现你在使用 implicit，始终停下来问一下你自己，是否可以在不使用 implicit 的条件下达到相同的效果」。</p>
<p>如果你必需使用它们（比如：丰富 DSL），那么不要重载隐式方法，即确保每个隐式方法有着不同的名字，这样使用者就可以选择性地导入它们。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 别这么做，这样使用者无法选择性地只导入其中一个方法。</span></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">ImplicitHolder</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">toRdd</span></span>(seq: <span class="type">Seq</span>[<span class="type">Int</span>]): <span class="type">RDD</span>[<span class="type">Int</span>] = ...</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">toRdd</span></span>(seq: <span class="type">Seq</span>[<span class="type">Long</span>]): <span class="type">RDD</span>[<span class="type">Long</span>] = ...</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 应该将它们定义为不同的名字：</span></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">ImplicitHolder</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">intSeqToRdd</span></span>(seq: <span class="type">Seq</span>[<span class="type">Int</span>]): <span class="type">RDD</span>[<span class="type">Int</span>] = ...</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">longSeqToRdd</span></span>(seq: <span class="type">Seq</span>[<span class="type">Long</span>]): <span class="type">RDD</span>[<span class="type">Long</span>] = ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="异常处理-Try-还是-try"><a href="#异常处理-Try-还是-try" class="headerlink" title="异常处理 (Try 还是 try)"></a><a name="exception">异常处理 (Try 还是 try)</a></h2><ul>
<li><p>不要捕获 Throwable 或 Exception 类型的异常。请使用 <code>scala.util.control.NonFatal</code>：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">  ...</div><div class="line">&#125; <span class="keyword">catch</span> &#123;</div><div class="line">  <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</div><div class="line">    <span class="comment">// 异常处理；注意 NonFatal 无法匹配 InterruptedException 类型的异常</span></div><div class="line">  <span class="keyword">case</span> e: <span class="type">InterruptedException</span> =&gt;</div><div class="line">    <span class="comment">// 处理 InterruptedException</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这能保证我们不会去捕获 <code>NonLocalReturnControl</code> 异常（正如在<a href="#return">Return 语句</a>中所解释的）。</p>
</li>
<li><p>不要在 API 中使用 <code>Try</code>，即，不要在任何方法中返回 Try。对于异常执行，请显式地抛出异常，并使用 Java 风格的 try/catch 做异常处理。</p>
<p>背景资料：Scala 提供了单子（monadic）错误处理（通过 <code>Try</code>，<code>Success</code> 和 <code>Failure</code>），这样便于做链式处理。然而，根据我们的经验，发现使用它通常会带来更多的嵌套层级，使得代码难以阅读。此外，对于预期错误还是异常，在语义上常常是不明晰的。因此，我们不鼓励使用 <code>Try</code> 来做错误处理，尤其是以下情况：</p>
<p>一个人为的例子：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserService</span> </span>&#123;</div><div class="line">  <span class="comment">/** Look up a user's profile in the user database. */</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get</span></span>(userId: <span class="type">Int</span>): <span class="type">Try</span>[<span class="type">User</span>]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>以下的写法会更好：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserService</span> </span>&#123;</div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Look up a user's profile in the user database.</div><div class="line">   * @return None if the user is not found.</div><div class="line">   * @throws DatabaseConnectionException when we have trouble connecting to the database/</div><div class="line">   */</div><div class="line">  <span class="meta">@throws</span>(<span class="type">DatabaseConnectionException</span>)</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get</span></span>(userId: <span class="type">Int</span>): <span class="type">Option</span>[<span class="type">User</span>]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>第二种写法非常明显地能让调用者知道需要处理哪些错误情况。</p>
</li>
</ul>
<h3 id="Options"><a href="#Options" class="headerlink" title="Options"></a><a name="option">Options</a></h3><ul>
<li>如果一个值可能为空，那么请使用 <code>Option</code>。相对于 <code>null</code>，<code>Option</code> 显式地表明了一个 API 的返回值可能为空。</li>
<li><p>构造 <code>Option</code> 值时，请使用 <code>Option</code> 而非 <code>Some</code>，以防那个值为 <code>null</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">myMethod1</span></span>(input: <span class="type">String</span>): <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">Option</span>(transform(input))</div><div class="line"></div><div class="line"><span class="comment">// This is not as robust because transform can return null, and then</span></div><div class="line"><span class="comment">// myMethod2 will return Some(null).</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">myMethod2</span></span>(input: <span class="type">String</span>): <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">Some</span>(transform(input))</div></pre></td></tr></table></figure>
</li>
<li><p>不要使用 None 来表示异常，有异常时请显式抛出。</p>
</li>
<li>不要在一个 <code>Option</code> 值上直接调用 <code>get</code> 方法，除非你百分百确定那个 <code>Option</code> 值不是 <code>None</code>。</li>
</ul>
<h3 id="单子链接"><a href="#单子链接" class="headerlink" title="单子链接"></a><a name="chaining">单子链接</a></h3><p>单子链接是 Scala 的一个强大特性。Scala 中几乎一切都是单子（如：集合，Option，Future，Try 等），对它们的操作可以链接在一起。这是一个非常强大的概念，但你应该谨慎使用，尤其是：</p>
<ul>
<li>避免链接（或嵌套）超过 3 个操作。</li>
<li>如果需要花超过 5 秒钟来理解其中的逻辑，那么你应该尽量去想想有没什么办法在不使用单子链接的条件下来达到相同的效果。一般来说，你需要注意的是：不要滥用 <code>flatMap</code> 和 <code>fold</code>。</li>
<li>链接应该在 flatMap 之后断开（因为类型发生了变化）。</li>
</ul>
<p>通过给中间结果显式地赋予一个变量名，将链接断开变成一种更加过程化的风格，能让单子链接更加易于理解。来看下面的例子：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">val data: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]</span>)</span></div><div class="line"><span class="keyword">val</span> database = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Person</span>]</div><div class="line"><span class="comment">// Sometimes the client can store "null" value in the  store "address"</span></div><div class="line"></div><div class="line"><span class="comment">// A monadic chaining approach</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAddress</span></span>(name: <span class="type">String</span>): <span class="type">Option</span>[<span class="type">String</span>] = &#123;</div><div class="line">  database.get(name).flatMap &#123; elem =&gt;</div><div class="line">    elem.data.get(<span class="string">"address"</span>)</div><div class="line">      .flatMap(<span class="type">Option</span>.apply)  <span class="comment">// handle null value</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 尽管代码会长一些，但以下方法可读性更高</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAddress</span></span>(name: <span class="type">String</span>): <span class="type">Option</span>[<span class="type">String</span>] = &#123;</div><div class="line">  <span class="keyword">if</span> (!database.contains(name)) &#123;</div><div class="line">    <span class="keyword">return</span> <span class="type">None</span></div><div class="line">  &#125;</div><div class="line"></div><div class="line">  database(name).data.get(<span class="string">"address"</span>) <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(<span class="literal">null</span>) =&gt; <span class="type">None</span>  <span class="comment">// handle null value</span></div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(addr) =&gt; <span class="type">Option</span>(addr)</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="type">None</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="并发"><a href="#并发" class="headerlink" title="并发"></a><a name="concurrency">并发</a></h2><h3 id="Scala-concurrent-Map"><a href="#Scala-concurrent-Map" class="headerlink" title="Scala concurrent.Map"></a><a name="concurrency-scala-collection">Scala concurrent.Map</a></h3><p><strong>优先考虑使用 <code>java.util.concurrent.ConcurrentHashMap</code> 而非 <code>scala.collection.concurrent.Map</code></strong>。尤其是 <code>scala.collection.concurrent.Map</code> 中的 <code>getOrElseUpdate</code> 方法要慎用，它并非原子操作（这个问题在 Scala 2.11.16 中 fix 了：<a href="https://issues.scala-lang.org/browse/SI-7943" target="_blank" rel="external">SI-7943</a>）。由于我们做的所有项目都需要在 Scala 2.10 和 Scala 2.11 上使用，因此要避免使用 <code>scala.collection.concurrent.Map</code>。</p>
<h3 id="显式同步-vs-并发集合"><a href="#显式同步-vs-并发集合" class="headerlink" title="显式同步 vs 并发集合"></a><a name="concurrency-sync-vs-map">显式同步 vs 并发集合</a></h3><p>有 3 种推荐的方法来安全地并发访问共享状态。<strong>不要混用它们</strong>，因为这会使程序变得难以推理，并且可能导致死锁。</p>
<ul>
<li><p><code>java.util.concurrent.ConcurrentHashMap</code>：当所有的状态都存储在一个 map 中，并且有高程度的竞争时使用。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> map = <span class="keyword">new</span> java.util.concurrent.<span class="type">ConcurrentHashMap</span>[<span class="type">String</span>, <span class="type">String</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>java.util.Collections.synchronizedMap</code>：使用情景：当所有状态都存储在一个 map 中，并且预期不存在竞争情况，但你仍想确保代码在并发下是安全的。如果没有竞争出现，JVM 的 JIT 编译器能够通过偏置锁（biased locking）移除同步开销。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> map = java.util.<span class="type">Collections</span>.synchronizedMap(<span class="keyword">new</span> java.util.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>])</div></pre></td></tr></table></figure>
</li>
<li><p>通过同步所有临界区进行显式同步，可用于监视多个变量。与 2 相似，JVM 的 JIT 编译器能够通过偏置锁（biased locking）移除同步开销。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Manager</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">var</span> count = <span class="number">0</span></div><div class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> map = <span class="keyword">new</span> java.util.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>]</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(key: <span class="type">String</span>, value: <span class="type">String</span>): <span class="type">Unit</span> = synchronized &#123;</div><div class="line">    map.put(key, value)</div><div class="line">    count += <span class="number">1</span></div><div class="line">  &#125;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getCount</span></span>: <span class="type">Int</span> = synchronized &#123; count &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
<p>注意，对于 case 1 和 case 2，不要让集合的视图或迭代器从保护区域逃逸。这可能会以一种不明显的方式发生，比如：返回了 <code>Map.keySet</code> 或 <code>Map.values</code>。如果需要传递集合的视图或值，生成一份数据拷贝再传递。</p>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> map = java.util.<span class="type">Collections</span>.synchronizedMap(<span class="keyword">new</span> java.util.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>])</div><div class="line"></div><div class="line"><span class="comment">// This is broken!</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">values</span></span>: <span class="type">Iterable</span>[<span class="type">String</span>] = map.values</div><div class="line"></div><div class="line"><span class="comment">// Instead, copy the elements</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">values</span></span>: <span class="type">Iterable</span>[<span class="type">String</span>] = map.synchronized &#123; <span class="type">Seq</span>(map.values: _*) &#125;</div></pre></td></tr></table></figure>
<h3 id="显式同步-vs-原子变量-vs-volatile"><a href="#显式同步-vs-原子变量-vs-volatile" class="headerlink" title="显式同步 vs 原子变量 vs @volatile"></a><a name="concurrency-sync-vs-atomic">显式同步 vs 原子变量 vs @volatile</a></h3><p><code>java.util.concurrent.atomic</code> 包提供了对基本类型的无锁访问，比如：<code>AtomicBoolean</code>, <code>AtomicInteger</code> 和 <code>AtomicReference</code>。</p>
<p>始终优先考虑使用原子变量而非 <code>@volatile</code>，它们是相关功能的严格超集并且从代码上看更加明显。原子变量的底层实现使用了 <code>@volatile</code>。</p>
<p>优先考虑使用原子变量而非显式同步的情况：（1）一个对象的所有临界区更新都被限制在单个变量里并且预期会有竞争情况出现。原子变量是无锁的并且允许更为有效的竞争。（2）同步被明确地表示为 <code>getAndSet</code> 操作。例如：</p>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// good: 明确又有效地表达了下面的并发代码只执行一次</span></div><div class="line"><span class="keyword">val</span> initialized = <span class="keyword">new</span> <span class="type">AtomicBoolean</span>(<span class="literal">false</span>)</div><div class="line">...</div><div class="line"><span class="keyword">if</span> (!initialized.getAndSet(<span class="literal">true</span>)) &#123;</div><div class="line">  ...</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// poor: 下面的同步就没那么明晰，而且会出现不必要的同步</span></div><div class="line"><span class="keyword">val</span> initialized = <span class="literal">false</span></div><div class="line">...</div><div class="line"><span class="keyword">var</span> wasInitialized = <span class="literal">false</span></div><div class="line">synchronized &#123;</div><div class="line">  wasInitialized = initialized</div><div class="line">  initialized = <span class="literal">true</span></div><div class="line">&#125;</div><div class="line"><span class="keyword">if</span> (!wasInitialized) &#123;</div><div class="line">  ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="私有字段"><a href="#私有字段" class="headerlink" title="私有字段"></a><a name="concurrency-private-this">私有字段</a></h3><p>注意，<code>private</code> 字段仍然可以被相同类的其它实例所访问，所以仅仅通过 <code>this.synchronized</code>（或 <code>synchronized</code>）来保护它从技术上来说是不够的，不过你可以通过 <code>private[this]</code> 修饰私有字段来达到目的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 以下代码仍然是不安全的。</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">var</span> count: <span class="type">Int</span> = <span class="number">0</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">inc</span></span>(): <span class="type">Unit</span> = synchronized &#123; count += <span class="number">1</span> &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 以下代码是安全的。</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">var</span> count: <span class="type">Int</span> = <span class="number">0</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">inc</span></span>(): <span class="type">Unit</span> = synchronized &#123; count += <span class="number">1</span> &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="隔离"><a href="#隔离" class="headerlink" title="隔离"></a><a name="concurrency-isolation">隔离</a></h3><p>一般来说，并发和同步逻辑应该尽可能地被隔离和包含起来。这实际上意味着：</p>
<ul>
<li>避免在 API 层面、面向用户的方法以及回调中暴露同步原语。</li>
<li>对于复杂模块，创建一个小的内部模块来包含并发原语。</li>
</ul>
<h2 id="性能"><a href="#性能" class="headerlink" title="性能"></a><a name="perf">性能</a></h2><p>对于你写的绝大多数代码，性能都不应该成为一个问题。然而，对于一些性能敏感的代码，以下有一些小建议：</p>
<h3 id="Microbenchmarks"><a href="#Microbenchmarks" class="headerlink" title="Microbenchmarks"></a><a name="perf-microbenchmarks">Microbenchmarks</a></h3><p>由于 Scala 编译器和 JVM JIT 编译器会对你的代码做许多神奇的事情，因此要写出一个好的微基准程序（microbenchmark）是极其困难的。更多的情况往往是你的微基准程序并没有测量你想要测量的东西。</p>
<p>如果你要写一个微基准程序，请使用 <a href="http://openjdk.java.net/projects/code-tools/jmh/" target="_blank" rel="external">jmh</a>。请确保你阅读了<a href="http://hg.openjdk.java.net/code-tools/jmh/file/tip/jmh-samples/src/main/java/org/openjdk/jmh/samples/" target="_blank" rel="external">所有的样例</a>，这样你才理解微基准程序中「死代码」移除、常量折叠以及循环展开的效果。</p>
<h3 id="Traversal-与-zipWithIndex"><a href="#Traversal-与-zipWithIndex" class="headerlink" title="Traversal 与 zipWithIndex"></a><a name="perf-whileloops">Traversal 与 zipWithIndex</a></h3><p>使用 <code>while</code> 循环而非 <code>for</code> 循环或函数变换（如：<code>map</code>、<code>foreach</code>），for 循环和函数变换非常慢（由于虚函数调用和装箱的缘故）。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">val</span> arr = <span class="comment">// array of ints</span></div><div class="line"><span class="comment">// 偶数位置的数置零</span></div><div class="line"><span class="keyword">val</span> newArr = list.zipWithIndex.map &#123; <span class="keyword">case</span> (elem, i) =&gt;</div><div class="line">  <span class="keyword">if</span> (i % <span class="number">2</span> == <span class="number">0</span>) <span class="number">0</span> <span class="keyword">else</span> elem</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 这是上面代码的高性能版本</span></div><div class="line"><span class="keyword">val</span> newArr = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](arr.length)</div><div class="line"><span class="keyword">var</span> i = <span class="number">0</span></div><div class="line"><span class="keyword">val</span> len = newArr.length</div><div class="line"><span class="keyword">while</span> (i &lt; len) &#123;</div><div class="line">  newArr(i) = <span class="keyword">if</span> (i % <span class="number">2</span> == <span class="number">0</span>) <span class="number">0</span> <span class="keyword">else</span> arr(i)</div><div class="line">  i += <span class="number">1</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Option-与-null"><a href="#Option-与-null" class="headerlink" title="Option 与 null"></a><a name="perf-option">Option 与 null</a></h3><p>对于性能有要求的代码，优先考虑使用 <code>null</code> 而不是 <code>Option</code>，以此避免虚函数调用以及装箱操作。用 Nullable 注解明确标示出可能为 <code>null</code> 的值。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span> </span>&#123;</div><div class="line">  <span class="meta">@javax</span>.annotation.<span class="type">Nullable</span></div><div class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">var</span> nullableField: <span class="type">Bar</span> = _</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Scala-集合库"><a href="#Scala-集合库" class="headerlink" title="Scala 集合库"></a><a name="perf-collection">Scala 集合库</a></h3><p>对于性能有要求的代码，优先考虑使用 Java 集合库而非 Scala 集合库，因为一般来说，Scala 集合库要比 Java 的集合库慢。</p>
<h3 id="private-this"><a href="#private-this" class="headerlink" title="private[this]"></a><a name="perf-private">private[this]</a></h3><p>对于性能有要求的代码，优先考虑使用 <code>private[this]</code> 而非 <code>private</code>。<code>private[this]</code> 生成一个字段而非生成一个访问方法。根据我们的经验，JVM JIT 编译器并不总是会内联 <code>private</code> 字段的访问方法，因此通过使用<br><code>private[this]</code> 来确保没有虚函数调用会更保险。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> field1 = ...</div><div class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> field2 = ...</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">perfSensitiveMethod</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">var</span> i = <span class="number">0</span></div><div class="line">    <span class="keyword">while</span> (i &lt; <span class="number">1000000</span>) &#123;</div><div class="line">      field1  <span class="comment">// This might invoke a virtual method call</span></div><div class="line">      field2  <span class="comment">// This is just a field access</span></div><div class="line">      i += <span class="number">1</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="与-Java-的互操作性"><a href="#与-Java-的互操作性" class="headerlink" title="与 Java 的互操作性"></a><a name="java">与 Java 的互操作性</a></h2><p>本节内容介绍的是构建 Java 兼容 API 的准则。如果你构建的组件并不需要与 Java 有交互，那么请无视这一节。这一节的内容主要是从我们开发 Spark 的 Java API 的经历中得出的。</p>
<h3 id="Scala-中缺失的-Java-特性"><a href="#Scala-中缺失的-Java-特性" class="headerlink" title="Scala 中缺失的 Java 特性"></a><a name="java-missing-features">Scala 中缺失的 Java 特性</a></h3><p>以下的 Java 特性在 Scala 中是没有的，如果你需要使用以下特性，请在 Java 中定义它们。然而，需要提醒一点的是，你无法为 Java 源文件生成 ScalaDoc。</p>
<ul>
<li>静态字段</li>
<li>静态内部类</li>
<li>Java 枚举</li>
<li>注解</li>
</ul>
<h3 id="Traits-与抽象类"><a href="#Traits-与抽象类" class="headerlink" title="Traits 与抽象类"></a><a name="java-traits">Traits 与抽象类</a></h3><p>对于允许从外部实现的接口，请记住以下几点：</p>
<ul>
<li>包含了默认方法实现的 trait 是无法在 Java 中使用的，请使用抽象类来代替。</li>
<li>一般情况下，请避免使用 trait，除非你百分百确定这个接口即使在未来也不会有默认的方法实现。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 以下默认实现无法在 Java 中使用</span></div><div class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Listener</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">onTermination</span></span>(): <span class="type">Unit</span> = &#123; ... &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 可以在 Java 中使用</span></div><div class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Listener</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">onTermination</span></span>(): <span class="type">Unit</span> = &#123; ... &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="类型别名"><a href="#类型别名" class="headerlink" title="类型别名"></a><a name="java-type-alias">类型别名</a></h3><p>不要使用类型别名，它们在字节码和 Java 中是不可见的。</p>
<h3 id="默认参数值"><a href="#默认参数值" class="headerlink" title="默认参数值"></a><a name="java-default-param-values">默认参数值</a></h3><p>不要使用默认参数值，通过重载方法来代替。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 打破了与 Java 的互操作性</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(ratio: <span class="type">Double</span>, withReplacement: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">T</span>] = &#123; ... &#125;</div><div class="line"></div><div class="line"><span class="comment">// 以下方法是 work 的</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(ratio: <span class="type">Double</span>, withReplacement: <span class="type">Boolean</span>): <span class="type">RDD</span>[<span class="type">T</span>] = &#123; ... &#125;</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(ratio: <span class="type">Double</span>): <span class="type">RDD</span>[<span class="type">T</span>] = sample(ratio, withReplacement = <span class="literal">false</span>)</div></pre></td></tr></table></figure>
<h3 id="多参数列表-1"><a href="#多参数列表-1" class="headerlink" title="多参数列表"></a><a name="java-multi-param-list">多参数列表</a></h3><p>不要使用多参数列表。</p>
<h3 id="可变参数"><a href="#可变参数" class="headerlink" title="可变参数"></a><a name="java-varargs">可变参数</a></h3><ul>
<li><p>为可变参数方法添加 <code>@scala.annotation.varargs</code> 注解，以确保它能在 Java 中使用。Scala 编译器会生成两个方法，一个给 Scala 使用（字节码参数是一个 Seq），另一个给 Java 使用（字节码参数是一个数组）。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@scala</span>.annotation.varargs</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">select</span></span>(exprs: <span class="type">Expression</span>*): <span class="type">DataFrame</span> = &#123; ... &#125;</div></pre></td></tr></table></figure>
</li>
<li><p>需要注意的一点是，由于 Scala 编译器的一个 bug（<a href="https://issues.scala-lang.org/browse/SI-1459" target="_blank" rel="external">SI-1459</a>，<a href="https://issues.scala-lang.org/browse/SI-9013" target="_blank" rel="external">SI-9013</a>），抽象的变参方法是无法在 Java 中使用的。</p>
</li>
<li><p>重载变参方法时要小心，用另一个类型去重载变参方法会破坏源码的兼容性。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Database</span> </span>&#123;</div><div class="line">  <span class="meta">@scala</span>.annotation.varargs</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">remove</span></span>(elems: <span class="type">String</span>*): <span class="type">Unit</span> = ...</div><div class="line"></div><div class="line">  <span class="comment">// 当调用无参的 remove 方法时会出问题。</span></div><div class="line">  <span class="meta">@scala</span>.annotation.varargs</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">remove</span></span>(elems: <span class="type">People</span>*): <span class="type">Unit</span> = ...</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// remove 方法有歧义，因此编译不过。</span></div><div class="line"><span class="keyword">new</span> <span class="type">Database</span>().remove()</div></pre></td></tr></table></figure>
<p>一种解决方法是，在可变参数前显式地定义第一个参数：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Database</span> </span>&#123;</div><div class="line">  <span class="meta">@scala</span>.annotation.varargs</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">remove</span></span>(elems: <span class="type">String</span>*): <span class="type">Unit</span> = ...</div><div class="line"></div><div class="line">  <span class="comment">// 以下重载是 OK 的。</span></div><div class="line">  <span class="meta">@scala</span>.annotation.varargs</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">remove</span></span>(elem: <span class="type">People</span>, elems: <span class="type">People</span>*): <span class="type">Unit</span> = ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Implicits-1"><a href="#Implicits-1" class="headerlink" title="Implicits"></a><a name="java-implicits">Implicits</a></h3><p>不要为类或方法使用 implicit，包括了不要使用 <code>ClassTag</code> 和 <code>TypeTag</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">JavaFriendlyAPI</span> </span>&#123;</div><div class="line">  <span class="comment">// 以下定义对 Java 是不友好的，因为方法中包含了一个隐式参数（ClassTag）。</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">convertTo</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](): <span class="type">T</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="伴生对象，静态方法与字段"><a href="#伴生对象，静态方法与字段" class="headerlink" title="伴生对象，静态方法与字段"></a><a name="java-companion-object">伴生对象，静态方法与字段</a></h3><p>当涉及到伴生对象和静态方法/字段时，有几件事情是需要注意的：</p>
<ul>
<li><p>伴生对象在 Java 中的使用是非常别扭的（伴生对象 <code>Foo</code> 会被定义为 <code>Foo$</code> 类内的一个类型为 <code>Foo$</code> 的静态字段 <code>MODULE$</code>）。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">Foo</span></span></div><div class="line"></div><div class="line"><span class="comment">// 等价于以下的 Java 代码</span></div><div class="line">public <span class="class"><span class="keyword">class</span> <span class="title">Foo$</span> </span>&#123;</div><div class="line">  <span class="type">Foo</span>$ <span class="type">MODULE</span>$ = <span class="comment">// 对象的实例化</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果非要使用伴生对象，可以在一个单独的类中创建一个 Java 静态字段。</p>
</li>
<li><p>不幸的是，没有办法在 Scala 中定义一个 JVM 静态字段。请创建一个 Java 文件来定义它。</p>
</li>
<li><p>伴生对象里的方法会被自动转成伴生类里的静态方法，除非方法名有冲突。确保静态方法正确生成的最好方式是用 Java 写一个测试文件，然后调用生成的静态方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">method2</span></span>(): <span class="type">Unit</span> = &#123; ... &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">Foo</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">method1</span></span>(): <span class="type">Unit</span> = &#123; ... &#125;  <span class="comment">// 静态方法 Foo.method1 会被创建（字节码）</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">method2</span></span>(): <span class="type">Unit</span> = &#123; ... &#125;  <span class="comment">// 静态方法 Foo.method2 不会被创建</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// FooJavaTest.java (in test/scala/com/databricks/...)</span></div><div class="line">public <span class="class"><span class="keyword">class</span> <span class="title">FooJavaTest</span> </span>&#123;</div><div class="line">  public static void compileTest() &#123;</div><div class="line">    <span class="type">Foo</span>.method1();  <span class="comment">// 正常编译</span></div><div class="line">    <span class="type">Foo</span>.method2();  <span class="comment">// 编译失败，因为 method2 并没有生成</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>样例对象（case object） MyClass 的类型并不是 MyClass。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">object</span> <span class="title">MyClass</span></span></div><div class="line"></div><div class="line"><span class="comment">// Test.java</span></div><div class="line"><span class="keyword">if</span> (<span class="type">MyClass</span>$.<span class="type">MODULE</span> instanceof <span class="type">MyClass</span>) &#123;</div><div class="line">  <span class="comment">// 上述条件始终为 false</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>要实现正确的类型层级结构，请定义一个伴生类，然后用一个样例对象去继承它：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span></span></div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">object</span> <span class="title">MyClass</span> <span class="keyword">extends</span> <span class="title">MyClass</span></span></div></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a><a name="misc">其它</a></h2><h3 id="优先使用-nanoTime-而非-currentTimeMillis"><a href="#优先使用-nanoTime-而非-currentTimeMillis" class="headerlink" title="优先使用 nanoTime 而非 currentTimeMillis"></a><a name="misc_currentTimeMillis_vs_nanoTime">优先使用 nanoTime 而非 currentTimeMillis</a></h3><p>当要计算<em>持续时间</em>或者检查<em>超时</em>的时候，避免使用 <code>System.currentTimeMillis()</code>。请使用 <code>System.nanoTime()</code>，即使你对亚毫秒级的精度并不感兴趣。</p>
<p><code>System.currentTimeMillis()</code> 返回的是当前的时钟时间，并且会跟进系统时钟的改变。因此，负的时钟调整可能会导致超时而挂起很长一段时间（直到时钟时间赶上先前的值）。这种情况可能发生在网络已经中断一段时间，ntpd 走过了一步之后。最典型的例子是，在系统启动的过程中，DHCP 花费的时间要比平常的长。这可能会导致非常难以理解且难以重现的问题。而 <code>System.nanoTime()</code> 则可以保证是单调递增的，与时钟变化无关。</p>
<p>注意事项：</p>
<ul>
<li>永远不要序列化一个绝对的 <code>nanoTime()</code> 值或是把它传递给另一个系统。绝对的 <code>nanoTime()</code> 值是无意义的、与系统相关的，并且在系统重启时会重置。</li>
<li>绝对的 <code>nanoTime()</code> 值并不保证总是正数（但 <code>t2 - t1</code> 能确保总是产生正确的值）。</li>
<li><code>nanoTime()</code> 每 292 年就会重新计算起。所以，如果你的 Spark 任务需要花非常非常非常长的时间，你可能需要别的东西来处理了：）</li>
</ul>
<h3 id="优先使用-URI-而非-URL"><a href="#优先使用-URI-而非-URL" class="headerlink" title="优先使用 URI 而非 URL"></a><a name="misc_uri_url">优先使用 URI 而非 URL</a></h3><p>当存储服务的 URL 时，你应当使用 <code>URI</code> 来表示。</p>
<p><code>URL</code> 的<a href="http://docs.oracle.com/javase/7/docs/api/java/net/URL.html#equals(java.lang.Object" target="_blank" rel="external">相等性检查</a>)实际上执行了一次网络调用（这是阻塞的）来解析 IP 地址。<code>URI</code> 类在表示能力上是 <code>URL</code> 的超集，并且它执行的是字段的相等性检查。</p>
<h2 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a><a name="unit-test">单元测试</a></h2><h3 id="单元测试框架"><a href="#单元测试框架" class="headerlink" title="单元测试框架"></a><a name="unit-test-framework">单元测试框架</a></h3><p>ScalaTest几乎已经成为Scala语言默认的测试框架，这主要源于它提供了多种表达力超强的测试风格，能够满足各种层次的需求包括单元测试、BDD、验收测试、数据驱动测试。我们也使用<a href="http://www.scalatest.org/" target="_blank" rel="external">ScalaTest</a>测试框架。使用的时候在<code>pom.xml</code>中添加如下类似引用：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">&lt;!-- test: https://mvnrepository.com/artifact/org.scalatest/scalatest_2.10 --&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scalatest<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scalatest_2.10<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div></pre></td></tr></table></figure>
<h3 id="测试风格的选择"><a href="#测试风格的选择" class="headerlink" title="测试风格的选择"></a><a name="unit-test-style">测试风格的选择</a></h3><p>ScalaTest一共提供了七种测试风格，分别为：FunSuite，FlatSpec，FunSpec，WordSpec，FreeSpec，PropSpec和FeatureSpec。这就好像使用相同的原料做成不同美味乃至不同菜系的佳肴，你可以根据自己的口味进行选择。我们统一推荐使用FunSuite的方式，因为它更灵活，而且更符合传统测试方法的风格，区别仅在于test()方法可以接受一个闭包:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.scalatest.<span class="type">FunSuite</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SetSuite</span> <span class="keyword">extends</span> <span class="title">FunSuite</span> </span>&#123;</div><div class="line"></div><div class="line">  test(<span class="string">"An empty Set should have size 0"</span>) &#123;</div><div class="line">    assert(<span class="type">Set</span>.empty.size == <span class="number">0</span>)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  test(<span class="string">"Invoking head on an empty Set should produce NoSuchElementException"</span>) &#123;</div><div class="line">    assertThrows[<span class="type">NoSuchElementException</span>] &#123;</div><div class="line">      <span class="type">Set</span>.empty.head</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>当然，如果你有必须的理由选择其它测试风格的话，本规则并不强制。</p>
<h1 id="三、-引用"><a href="#三、-引用" class="headerlink" title="三、 引用"></a>三、 引用</h1><p><a href="https://github.com/databricks/scala-style-guide/blob/master/README-ZH.md" target="_blank" rel="external">Databricks Scala 编程风格指南</a>(团队最终选择的模板)<br><a href="http://twitter.github.io/effectivescala/index-cn.html" target="_blank" rel="external">Effective Scala</a>(Twitter Scala资料，值得参考)<br><a href="https://zhangyi.gitbooks.io/thinking-in-scala/content/scala-convention.html" target="_blank" rel="external">Thinking in Scala–Scala编程规范</a>(个人整理，可以参考)<br><a href="http://www.scala-lang.org/docu/files/Scala%E8%AF%AD%E8%A8%80%E8%A7%84%E8%8C%83.pdf" target="_blank" rel="external">scala-lang–Scala语言规范.pdf</a>(官方原版的中文文档，共127页，过于复杂琐碎，可以参考)<br><a href="http://docs.scala-lang.org/style/" target="_blank" rel="external">Scala官网Style Guide</a>(官方原版，其它版本基本上都是基于此版进行的改进)<br><a href="https://segmentfault.com/a/1190000000420018" target="_blank" rel="external">分析GitHub上托管的scala开源代码统计</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark源码阅读之——StreamingContext详解]]></title>
      <url>http://flume.cn/2017/01/03/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E4%B9%8B%E2%80%94%E2%80%94StreamingContext%E8%AF%A6%E8%A7%A3/</url>
      <content type="html"><![CDATA[<p><a href="https://github.com/lw-lin/CoolplaySpark/tree/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97" target="_blank" rel="external">Spark Streaming 源码解析系列</a>很好地解析了Spark Streaming框架的源码，遗留了一点关于StreamingContext的解析，我基于自己的理解，简要阐述如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">本系列内容适用范围：</div><div class="line">* 2016.12.28 update, Spark 2.1 全系列 √ (2.1.0)</div><div class="line">* 2016.11.14 update, Spark 2.0 全系列 √ (2.0.0, 2.0.1, 2.0.2)</div><div class="line">* 2016.11.07 update, Spark 1.6 全系列 √ (1.6.0, 1.6.1, 1.6.2, 1.6.3)</div></pre></td></tr></table></figure>
<p>阅读本文前，请一定先阅读 <a href="0.1 Spark Streaming 实现思路与模块概述.md">Spark Streaming 实现思路与模块概述</a> 一文，其中概述了 Spark Streaming 的 4 大模块的基本作用，有了全局概念后再看本文对 <code>StreamingContext</code> 细节的解释。</p>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p><img src="040.png" alt="image"></p>
<p>如各个模块的架构图所示，<code>StreamingContext</code> 是 Spark Streaming 提供给用户 code 的、与前述 4 个模块交互的一个简单和统一的入口，是Spark Streaming程序与Spark Core的连接器，下面我们用这段11行的完整 <a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#a-quick-example" target="_blank" rel="external">quick example</a>，来说明用户 code 是怎么通过 <code>StreamingContext</code> 与前面几个模块进行交互的：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark._</div><div class="line"><span class="keyword">import</span> org.apache.spark.streaming._</div><div class="line"></div><div class="line"><span class="comment">// 首先配置一下本 quick example 将跑在本机，app name 是 NetworkWordCount</span></div><div class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</div><div class="line"><span class="comment">// batchDuration 设置为 1 秒，然后创建一个 streaming 入口</span></div><div class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</div><div class="line"></div><div class="line"><span class="comment">// ssc.socketTextStream() 将创建一个 SocketInputDStream；这个 InputDStream 的 SocketReceiver 将监听本机 9999 端口</span></div><div class="line"><span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</div><div class="line"></div><div class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))      <span class="comment">// DStream transformation</span></div><div class="line"><span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))     <span class="comment">// DStream transformation</span></div><div class="line"><span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)    <span class="comment">// DStream transformation</span></div><div class="line">wordCounts.print()                           <span class="comment">// DStream output</span></div><div class="line"><span class="comment">// 上面 4 行利用 DStream transformation 构造出了 lines -&gt; words -&gt; pairs -&gt; wordCounts -&gt; .print() 这样一个 DStreamGraph</span></div><div class="line"><span class="comment">// 但注意，到目前是定义好了产生数据的 SocketReceiver，以及一个 DStreamGraph，这些都是静态的</span></div><div class="line"></div><div class="line"><span class="comment">// 下面这行 start() 将在幕后启动 JobScheduler, 进而启动 JobGenerator 和 ReceiverTracker</span></div><div class="line"><span class="comment">// ssc.start()</span></div><div class="line"><span class="comment">//    -&gt; JobScheduler.start()</span></div><div class="line"><span class="comment">//        -&gt; JobGenerator.start();    开始不断生成一个一个 batch</span></div><div class="line"><span class="comment">//        -&gt; ReceiverTracker.start(); 开始往 executor 上分布 ReceiverSupervisor 了，也会进一步创建和启动 Receiver</span></div><div class="line">ssc.start()</div><div class="line"></div><div class="line"><span class="comment">// 然后用户 code 主线程就 block 在下面这行代码了</span></div><div class="line"><span class="comment">// block 的后果就是，后台的 JobScheduler 线程周而复始的产生一个一个 batch 而不停息</span></div><div class="line"><span class="comment">// 也就是在这里，我们前面静态定义的 DStreamGraph 的 print()，才一次一次被在 RDD 实例上调用，一次一次打印出当前 batch 的结果</span></div><div class="line">ssc.awaitTermination()</div></pre></td></tr></table></figure>
<p>从上述样例程序可知，程序的前两行创建了一个新的 <code>StreamingContext</code>，第三行通过 <code>ssc.socketTextStream</code>通过ssc暴露的方法创建了一个<code>ReceiverInputDStream</code>，接着基于DStream的各种方法对数据进行了操作，最后通过 <code>ssc.start</code> 方法启动了Spark Streaming 程序，最后一句<code>ssc.awaitTermination()</code>将用户 code 主线程 block 住，由后台的 JobScheduler 线程周而复始的产生一个一个 batch 而不停息地处理，除非发生异常。</p>
<p>我们可以发现，StreamingContext主要包含以下内容：</p>
<ul>
<li>StreamingContext的创建(构造函数)</li>
<li>StreamingContext的初始化(成员)</li>
<li>StreamingContext的状态控制(函数)</li>
</ul>
<h3 id="StreamingContext的创建"><a href="#StreamingContext的创建" class="headerlink" title="StreamingContext的创建"></a>StreamingContext的创建</h3><p>首先我们来看<code>StreamingContext</code>的构造函数，主要由三个参数，分别是：</p>
<ul>
<li>SparkContext：SparkStreaming的最终处理是交给SparkContext的；</li>
<li>Checkpoint：检查点，用于错误恢复；</li>
<li>Duration：设定Streaming每个批次的积累时间。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">StreamingContext</span> <span class="title">private</span>[streaming] (<span class="params"></span></span></div><div class="line">    _sc: <span class="type">SparkContext</span>,</div><div class="line">    _cp: <span class="type">Checkpoint</span>,</div><div class="line">    _batchDur: <span class="type">Duration</span></div><div class="line">  ) <span class="keyword">extends</span> <span class="title">Logging</span> &#123;</div></pre></td></tr></table></figure>
<p><code>StreamingContext</code> 有以下几种不同的创建方式：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment">// 1. 通过已经存在的SparkContext创建.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(sparkContext: <span class="type">SparkContext</span>, batchDuration: <span class="type">Duration</span>) = &#123;</div><div class="line">  <span class="keyword">this</span>(sparkContext, <span class="literal">null</span>, batchDuration)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 2. 通过SparkConf中的配置信息来来创建.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(conf: <span class="type">SparkConf</span>, batchDuration: <span class="type">Duration</span>) = &#123;</div><div class="line">  <span class="keyword">this</span>(<span class="type">StreamingContext</span>.createNewSparkContext(conf), <span class="literal">null</span>, batchDuration)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 3. 通过一些必要参数来创建</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(</div><div class="line">    master: <span class="type">String</span>,</div><div class="line">    appName: <span class="type">String</span>,</div><div class="line">    batchDuration: <span class="type">Duration</span>,</div><div class="line">    sparkHome: <span class="type">String</span> = <span class="literal">null</span>,</div><div class="line">    jars: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Nil</span>,</div><div class="line">    environment: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>()) = &#123;</div><div class="line">  <span class="keyword">this</span>(<span class="type">StreamingContext</span>.createNewSparkContext(master, appName, sparkHome, jars, environment),</div><div class="line">       <span class="literal">null</span>, batchDuration)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 4. 从checkpoint文件中读取来重新创建</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(path: <span class="type">String</span>, hadoopConf: <span class="type">Configuration</span>) =</div><div class="line">  <span class="keyword">this</span>(<span class="literal">null</span>, <span class="type">CheckpointReader</span>.read(path, <span class="keyword">new</span> <span class="type">SparkConf</span>(), hadoopConf).orNull, <span class="literal">null</span>)</div><div class="line"></div><div class="line"><span class="comment">// ...</span></div><div class="line"><span class="comment">// 5. 已经存在的SparkContext，通过读取checkpoint文件重新创建</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(path: <span class="type">String</span>, sparkContext: <span class="type">SparkContext</span>) = &#123;</div><div class="line">  <span class="keyword">this</span>(</div><div class="line">    sparkContext,</div><div class="line">    <span class="type">CheckpointReader</span>.read(path, sparkContext.conf, sparkContext.hadoopConfiguration).orNull,</div><div class="line">    <span class="literal">null</span>)</div><div class="line">&#125;</div><div class="line"><span class="comment">// ...</span></div><div class="line"><span class="comment">// 6. 值得一提的是，StreamingContext对象提供了一个构造方法，如果存在Checkpoint就通过Checkpoint创建，否则新建一个StreamingContext</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getOrCreate</span></span>(</div><div class="line">    checkpointPath: <span class="type">String</span>,</div><div class="line">    creatingFunc: () =&gt; <span class="type">StreamingContext</span>,</div><div class="line">    hadoopConf: <span class="type">Configuration</span> = <span class="type">SparkHadoopUtil</span>.get.conf,</div><div class="line">    createOnError: <span class="type">Boolean</span> = <span class="literal">false</span></div><div class="line">  ): <span class="type">StreamingContext</span> = &#123;</div><div class="line">  <span class="keyword">val</span> checkpointOption = <span class="type">CheckpointReader</span>.read(</div><div class="line">    checkpointPath, <span class="keyword">new</span> <span class="type">SparkConf</span>(), hadoopConf, createOnError)</div><div class="line">  checkpointOption.map(<span class="keyword">new</span> <span class="type">StreamingContext</span>(<span class="literal">null</span>, _, <span class="literal">null</span>)).getOrElse(creatingFunc())</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>整理上述的文件创建过程，可以看出，StreamingContext的创建是一定要包含SparkContext的，同理也可以推出，Spark Streaming最终实际是交给SparkContext来处理的，Spark Streaming更像是Spark Core的一个应用程序。<br><code>StreamingContext</code>的创建主要分为两类：</p>
<ul>
<li>1: 通过SparkContext建立新的StreamingContext，需要指定<code>batchDuration</code>时间；</li>
<li>2: 从checkpoint文件中读取的Checkpoint对象中创建StreamingContext，用于异常情况下的恢复，<code>batchDuration</code>在Checkpoint中已经保存，所以可以不用显示指定。</li>
</ul>
<p>所以说，要构建StreamingContext，就必须要以上两者至少选一，下面的代码也说明了这点：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">require(_sc != <span class="literal">null</span> || _cp != <span class="literal">null</span>,</div><div class="line">  <span class="string">"Spark Streaming cannot be initialized with both SparkContext and checkpoint as null"</span>)</div></pre></td></tr></table></figure>
<p>注意：由于SparkStreaming至少需要一个线程来接收数据，所以local与local[1]模式下是不可以启动的。</p>
<h3 id="StreamingContext的初始化"><a href="#StreamingContext的初始化" class="headerlink" title="StreamingContext的初始化"></a>StreamingContext的初始化</h3><p>StreamingContext在创建后会进行一些初始化（静态定义）的工作，定义一些静态的数据结构，由图可知，StreamingContext主要持有 DStreamGraph 与 JobScheduler 的对象：</p>
<p>如下是graph的初始化定义代码，如果之前存在 Checkpoint ，则graph从 Checkpoint 得到，否则创建一个新的graph<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[streaming] <span class="keyword">val</span> graph: <span class="type">DStreamGraph</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (isCheckpointPresent) &#123;</div><div class="line">    _cp.graph.setContext(<span class="keyword">this</span>)</div><div class="line">    <span class="comment">// 遍历从Checkpoint数据中恢复出RDD</span></div><div class="line">    _cp.graph.restoreCheckpointData()</div><div class="line">    _cp.graph</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    require(_batchDur != <span class="literal">null</span>, <span class="string">"Batch duration for StreamingContext cannot be null"</span>)</div><div class="line">    <span class="keyword">val</span> newGraph = <span class="keyword">new</span> <span class="type">DStreamGraph</span>()</div><div class="line">    newGraph.setBatchDuration(_batchDur)</div><div class="line">    newGraph</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>如下是初始化jobScheduler的代码：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[streaming] <span class="keyword">val</span> scheduler = <span class="keyword">new</span> <span class="type">JobScheduler</span>(<span class="keyword">this</span>)</div></pre></td></tr></table></figure></p>
<p>除了上述两个主要成员，StreamingContext还包含以下成员：</p>
<ul>
<li>ContextWaiter：用于等待任务执行结束；</li>
<li>StreamingJobProgressListener：用于监听StreamingJob，用以更新StreamingTab的显示；</li>
<li>StreamingTab：用于生成SparkUI中Streaming那一页标签；</li>
<li>StreamingSource： 流式计算的测量数据源metrics。</li>
</ul>
<p>除了定义上述成员，StreamingContext还进行了Checkpoint,创建了Checkpoint目录：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">conf.getOption(<span class="string">"spark.streaming.checkpoint.directory"</span>).foreach(checkpoint)</div></pre></td></tr></table></figure>
<p>checkpoint方法如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 创建Checkpoint目录</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkpoint</span></span>(directory: <span class="type">String</span>) &#123;</div><div class="line">  <span class="keyword">if</span> (directory != <span class="literal">null</span>) &#123;</div><div class="line">    <span class="keyword">val</span> path = <span class="keyword">new</span> <span class="type">Path</span>(directory)</div><div class="line">    <span class="keyword">val</span> fs = path.getFileSystem(sparkContext.hadoopConfiguration)</div><div class="line">    fs.mkdirs(path)</div><div class="line">    <span class="keyword">val</span> fullPath = fs.getFileStatus(path).getPath().toString</div><div class="line">    sc.setCheckpointDir(fullPath)</div><div class="line">    checkpointDir = fullPath</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    checkpointDir = <span class="literal">null</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="StreamingContext的控制"><a href="#StreamingContext的控制" class="headerlink" title="StreamingContext的控制"></a>StreamingContext的控制</h3><p>StreamingContext作为控制面板，给用户提供了许多控制方法，就像控制面板上的按钮，让我们来开发spark Streaming程序，主要有以下方法：</p>
<ul>
<li>sparkContext： 获得ssc所属的SparkContext</li>
<li>remember(duration: Duration)：通过设置 <code>graph.remember(duration)</code> 来设置<code>rememberDuration</code></li>
</ul>
<p>简单解释一下<code>rememberDuration</code>，Spark Streaming 会在每个Batch任务结束时进行一次清理动作<code>clearMetadata</code>，每个DStream 都会被扫描，先清理输出Dstream，接着清理输入DStream，清理的时候，根据<code>rememberDuration</code>来计算出oldRDD然后清理。<code>rememberDuration</code> 有默认值，大体是<code>slideDuration</code>，也就是DStream生成RDD的时间间隔，如果设置了checkpointDuration 则是2*checkpointDuration，手动指定的值要大于默认值才会生效。</p>
<p>接下来是定义一些定义输入流的方法，主要有：</p>
<ul>
<li>receiverStream<a href="receiver: Receiver[T]" target="_blank" rel="external">T: ClassTag</a>：创建一个用户自定义的Receiver；</li>
<li>socketTextStream：创建TCP socketReceiver，默认是utf-8的文本格式，以’\n’分隔；</li>
<li>socketStream：创建TCP socketReceiver，用户提供自己的转化函数；</li>
<li>rawSocketStream：创建SocketReceiver，相比上着，没有中间的解码转化所以比较高效；</li>
<li>fileStream：创建监控HDFS目录的InputDStream，通过检测文件的修改时间来判断是否是新文件；</li>
<li>binaryRecordsStream：创建二进制文件的监听InputDStream，使用了fileStream方法；</li>
<li>queueStream：创建一个RDD队列流，底层调用了UnionRDD的方法将这些RDD转化为一个RDD，开启oneAtATime参数则每个RDD只取一个值，可以用于调试和测试；</li>
</ul>
<p>在InputDStream的构造过程中，会将此输入流InputDStream添加到DStreamGraph的inputStreams数据结构中，<br><figure class="highlight scala"><figcaption><span>InputDStream.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ssc.graph.addInputStream(<span class="keyword">this</span>)</div></pre></td></tr></table></figure></p>
<p>还定义了一些DStream的其它方法：</p>
<ul>
<li>union： 多个DStreams合成一个DStream，底层调用了ssc.sc.union(rdds)；</li>
<li>transform： 根据自定义的transformFunc生成新的DStream；</li>
<li>addStreamingListener： 在listenerBus上增加一个StreamingListener对象，供JobScheduler的StreamingListenerBus对象监听输入流的ReceiverRateController对象；</li>
</ul>
<p>还定义了一些控制启动与关闭的方法：</p>
<ul>
<li>start：启动StreamingContext。</li>
</ul>
<p>StreamingContext的start方法启动过程中，会判断StreamingContext的状态，它有三个状态INITIALIZED、ACTIVE、STOP。只有状态为INITAILIZED才允许启动，主要有以下步骤：</p>
<ol>
<li>验证graph是否有效；</li>
<li>设置Checkpoint；</li>
<li>使用新的线程异步启动 JobScheduler ，启动后将状态由初始化状态INITIALIZED改为ACTIVE状态，JobScheduler的启动请见<a href="https://github.com/lw-lin/CoolplaySpark/blob/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97/2.2%20JobGenerator%20%E8%AF%A6%E8%A7%A3.md" target="_blank" rel="external">JobGenerator 详解</a>；</li>
<li>同时添加Streaming的shutdownHookRef，用于程序的异常终止，StreamingContext的shutdownHook优先级比SparkContext的值大1；</li>
<li>往metricsSystem中注册streamingSource测量数据源；</li>
<li>添加生成SparkUI中Streaming相关标签</li>
</ol>
<ul>
<li>awaitTermination：等待Streaming程序的停止；</li>
<li>stop：停止SparkStreaming程序，其中可以传入参数以表示是否同时停止相关的SparkContext，默认为true(这个参数在文件名流转化为数据流的时候应该设置为false，通过spark.streaming.stopSparkContextByDefault来设置);还有一个参数是是否优雅地停止(等待其它已经接收到的数据处理完毕再停止)；</li>
</ul>
<h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><p><a href="http://lqding.blog.51cto.com/9123978/1771017" target="_blank" rel="external">http://lqding.blog.51cto.com/9123978/1771017</a><br><a href="https://github.com/lw-lin/CoolplaySpark/tree/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97" target="_blank" rel="external">https://github.com/lw-lin/CoolplaySpark/tree/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97</a><br><a href="http://lqding.blog.51cto.com/9123978/1773912" target="_blank" rel="external">http://lqding.blog.51cto.com/9123978/1773912</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[spark中读取hdfs文件简记]]></title>
      <url>http://flume.cn/2016/12/19/spark%E4%B8%AD%E8%AF%BB%E5%8F%96hdfs%E6%96%87%E4%BB%B6%E7%AE%80%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>使用spark的API读取hdfs的方法是：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> lines: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(filePath)</div></pre></td></tr></table></figure></p>
<p>如果该文件不存在，就会报错，报错多次，就会奔溃</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">16/12/16 16:53:12 ERROR JobScheduler: Error running job streaming job 1481878392000 ms.0</div><div class="line">org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://ns/data/.../4811_000.txt</div><div class="line">	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)</div><div class="line">	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)</div><div class="line">	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)</div><div class="line">	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)</div><div class="line">	at org.apache.spark.rdd.RDD$<span class="variable">$anonfun</span><span class="variable">$partitions</span><span class="variable">$2</span>.apply(RDD.scala:239)</div><div class="line">	at org.apache.spark.rdd.RDD$<span class="variable">$anonfun</span><span class="variable">$partitions</span><span class="variable">$2</span>.apply(RDD.scala:237)</div><div class="line">	at scala.Option.getOrElse(Option.scala:120)</div><div class="line"></div><div class="line">	...</div><div class="line"></div><div class="line">16/12/16 16:53:12 ERROR ApplicationMaster: User class threw exception: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://ns/data...00.txt</div><div class="line">org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://ns/data/hjpt/...000.txt</div><div class="line">	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)</div><div class="line">	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)</div><div class="line">	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)</div><div class="line">	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)</div><div class="line">	at org.apache.spark.rdd.RDD$<span class="variable">$anonfun</span><span class="variable">$partitions</span><span class="variable">$2</span>.apply(RDD.scala:239)</div><div class="line">	at org.apache.spark.rdd.RDD$<span class="variable">$anonfun</span><span class="variable">$partitions</span><span class="variable">$2</span>.apply(RDD.scala:237)</div><div class="line">	at scala.Option.getOrElse(Option.scala:120)</div><div class="line">	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)</div></pre></td></tr></table></figure>
<p>由于spark中hdfs读是lazy的，所以无法使用try-catch把它装住，即使用try-catch将其包住也会报错。<br>所以目前使用的解决方法是需要在读取该文件之前检验该文件是否存在。<br>在spark中的实现不需要再次指定hadoopConf，只要从sc中拿就可以了：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> conf = sc.hadoopConfiguration</div><div class="line"><span class="keyword">val</span> fs = org.apache.hadoop.fs.<span class="type">FileSystem</span>.get(conf)</div><div class="line"><span class="keyword">val</span> exists = fs.exists(<span class="keyword">new</span> org.apache.hadoop.fs.<span class="type">Path</span>(<span class="string">"/path/on/hdfs/to/SUCCESS.txt"</span>))</div></pre></td></tr></table></figure>
<p>实际实现如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line"><span class="keyword">val</span> sc: <span class="type">SparkContext</span> = eachRdd.sparkContext</div><div class="line"><span class="keyword">val</span> hadoopConf: <span class="type">Configuration</span> = sc.hadoopConfiguration</div><div class="line"><span class="keyword">val</span> fs: <span class="type">FileSystem</span> = org.apache.hadoop.fs.<span class="type">FileSystem</span>.get(hadoopConf)</div><div class="line"></div><div class="line"><span class="comment">// 这里是否不需要collect？</span></div><div class="line"><span class="keyword">val</span> lines: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">FtpMap</span>)] = oiddRdd.collect()</div><div class="line"><span class="comment">// 文件名流转化为文件数据流</span></div><div class="line">lines.foreach &#123;</div><div class="line">eachFileJson: (<span class="type">String</span>, <span class="type">FtpMap</span>) =&gt; &#123;</div><div class="line">  <span class="keyword">val</span> topic: <span class="type">String</span> = eachFileJson._1</div><div class="line">  printLog.info(<span class="string">"topic: "</span> + topic)</div><div class="line">  <span class="keyword">val</span> fileJson = eachFileJson._2</div><div class="line">  <span class="keyword">val</span> filePath = fileJson.file_path</div><div class="line">  <span class="keyword">val</span> fileExists: <span class="type">Boolean</span> = <span class="keyword">try</span> &#123;</div><div class="line">    fs.exists(<span class="keyword">new</span> org.apache.hadoop.fs.<span class="type">Path</span>(filePath))</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; &#123;</div><div class="line">      printLog.error(<span class="string">"Exception: filePath:"</span> + filePath + <span class="string">" e:"</span> + e)</div><div class="line">      <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (fileExists) &#123;</div><div class="line">    <span class="keyword">val</span> lines: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(filePath)</div><div class="line">    ...</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>值得注意的是：</p>
<p>如果读取一个hdfs目录下的所有文件，当文件的数目非常多，比如说有一亿个文件，由于spark读hdfs文件是lazy的，它在读取hdfs下该文件的列表的时候，会先将这个列表保存到内存中，但是如果在这期间hdfs文件被删除了，则还是会发生文件不存在的错误，所以以后遇到这类问题的时候要注意。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[通过经纬度计算距离算法的scala实现]]></title>
      <url>http://flume.cn/2016/11/24/%E9%80%9A%E8%BF%87%E7%BB%8F%E7%BA%AC%E5%BA%A6%E8%AE%A1%E7%AE%97%E8%B7%9D%E7%A6%BB%E7%AE%97%E6%B3%95%E7%9A%84scala%E5%AE%9E%E7%8E%B0/</url>
      <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>给定一个景点的经纬度，给定距离，给定形状，判断其它点是否在某个区域内：</p>
<h2 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h2><p>使用通用的地球上两点距离函数，圆形只需要判断距离，正方形需要计算两次距离（指定经度与景点的经度一样，计算是否在范围内，然后指定纬度与景点的纬度一样，计算是否在范围内，如果都在范围内，则代表该点在景区范围内），其它形状基于这个基础类推</p>
<h3 id="距离函数"><a href="#距离函数" class="headerlink" title="距离函数"></a>距离函数</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Created by jhck on 2016/10/9.</div><div class="line">  *</div><div class="line">  * 求地球上两点间的距离 返回的是 double 格式的 km</div><div class="line">  * 第一点经纬度为（lat1,lng1），第二点经纬度为（lat2,lng2），地球平均半径R=6378.137</div><div class="line">  * 按照0度经线的基准，东经取经度的正值(Longitude)，西经取经度负值(-Longitude)，北纬取90-纬度值(90- Latitude)，南纬取90+纬度值(90+Latitude)，</div><div class="line">  * 则经过上述处理过后的两点被计为(MLon1, MLat1)和(MLon2, MLat2)。那么根据三角推导，可以得到计算两点距离的如下公式：</div><div class="line">  * C = sin(MLat1)*sin(MLat2)*cos(MLon1-MLon2) + cos(MLat1)*cos(MLat2)，Distance = R*Arccos(C)*Pi/180</div><div class="line">  * 如果仅对经度作正负的处理，而不对纬度作90-Latitude(假设都是北半球，南半球只有澳洲具有应用意义)的处理，那么公式将是：</div><div class="line">  * C = sin(LatA)*sin(LatB) + cos(LatA)*cos(LatB)*cos(MLonA-MLonB)，Distance = R*Arccos(C)*Pi/180</div><div class="line">  * 三角函数的输入和输出都采用弧度值，那么公式还可以写作:</div><div class="line">  * C = sin(Lat1*Pi/180)*sin(Lat2*Pi/180) + cos(Lat1*Pi/180)*cos(Lat2*Pi/180)*cos((MLon1-MLon2)*Pi/180)，Distance = R*Arccos(C)*Pi/180</div><div class="line">  * rad()函数求弧度，Distance（）函数求距离</div><div class="line">  *</div><div class="line">  * 结果验证工具地址 http://www.storyday.com/wp-content/uploads/2008/09/latlung_dis.html</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">distance</span></span>(lat1: <span class="type">Double</span>, lng1: <span class="type">Double</span>, lat2: <span class="type">Double</span>, lng2: <span class="type">Double</span>): <span class="type">Double</span> = &#123;</div><div class="line">  <span class="keyword">val</span> <span class="type">EARTH_RADIUS</span> = <span class="number">6378.137</span></div><div class="line"></div><div class="line">  <span class="keyword">val</span> radLat1 = rad(lat1)</div><div class="line">  <span class="keyword">val</span> radLat2 = rad(lat2)</div><div class="line">  <span class="keyword">val</span> a = rad(lat1) - rad(lat2)</div><div class="line">  <span class="keyword">val</span> b = rad(lng1) - rad(lng2)</div><div class="line">  <span class="keyword">val</span> distance = <span class="type">EARTH_RADIUS</span> * <span class="number">2</span> * <span class="type">Math</span>.asin(<span class="type">Math</span>.sqrt(<span class="type">Math</span>.pow(<span class="type">Math</span>.sin(a / <span class="number">2</span>), <span class="number">2</span>) + <span class="type">Math</span>.cos(radLat1) * <span class="type">Math</span>.cos(radLat2) * <span class="type">Math</span>.pow(<span class="type">Math</span>.sin((b) / <span class="number">2</span>), <span class="number">2</span>)))</div><div class="line">  <span class="comment">//    printLog.debug("lat1: " + lat1 + " lng1: " + lng1 + " lat2: " + lat2 + " lng2: " + lng2)</span></div><div class="line">  printLog.debug(<span class="string">"distance:"</span> + distance)</div><div class="line">  distance</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark踩坑之Streaming在Kerberos的hadoop中renew失败]]></title>
      <url>http://flume.cn/2016/11/24/Spark%E8%B8%A9%E5%9D%91%E4%B9%8BStreaming%E5%9C%A8Kerberos%E7%9A%84hadoop%E4%B8%ADrenew%E5%A4%B1%E8%B4%A5/</url>
      <content type="html"><![CDATA[<h2 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h2><p>SparkStreaming任务的Kerberos环境下两天后出现 AMRMTOKEN INVALID</p>
<p>早上回来，跑在yarn上面的Streaming程序莫名奇妙崩溃了，yarn logs 查看日志，700万行，发现在每个executor的最后报错如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">16/11/01 15:02:05 INFO executor.CoarseGrainedExecutorBackend: Registered signal handlers <span class="keyword">for</span> [TERM, HUP, INT]</div><div class="line">16/11/01 15:02:06 INFO spark.SecurityManager: Changing view acls to: wzfw</div><div class="line">16/11/01 15:02:06 INFO spark.SecurityManager: Changing modify acls to: wzfw</div><div class="line">16/11/01 15:02:06 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(wzfw); users with modify permissions: Set(wzfw)</div><div class="line">...</div><div class="line">16/11/02 03:51:33 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL 15: SIGTERM</div><div class="line">...</div><div class="line">16/11/02 03:51:34 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL 15: SIGTERM</div><div class="line">16/11/02 03:51:34 WARN executor.CoarseGrainedExecutorBackend: An unknown (NM-304-SA5212M4-BIGDATA-519:44457) driver disconnected.</div><div class="line">16/11/02 03:51:34 ERROR executor.CoarseGrainedExecutorBackend: Driver 10.142.116.19:44457 disassociated! Shutting down.</div></pre></td></tr></table></figure></p>
<p>这个报错应该只是表象，原因应该出自driver，定位到driver： NM-304-SA5212M4-BIGDATA-519<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">16/11/02 03:51:34 WARN executor.CoarseGrainedExecutorBackend: An unknown (NM-304-SA5212M4-BIGDATA-519:44457) driver disconnected.</div><div class="line">16/11/02 03:51:34 ERROR executor.CoarseGrainedExecutorBackend: Driver 10.142.116.19:44457 disassociated! Shutting down.</div><div class="line">16/11/02 03:51:34 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL 15: SIGTERM</div><div class="line">16/11/02 03:51:34 ERROR client.TransportClient: Failed to send RPC 5285936577870185909 to NM-304-SA5212M4-BIGDATA-519/10.142.116.19:44457: java.nio.channels.ClosedChannelException</div><div class="line">java.nio.channels.ClosedChannelException</div><div class="line">16/11/02 03:51:34 WARN netty.NettyRpcEndpointRef: Error sending message [message = Heartbeat(53,[Lscala.Tuple2;@519f65d9,BlockManagerId(53, NM-304-SA5212M4-BIGDATA-382, 29163))] <span class="keyword">in</span> 1 attempts</div><div class="line">java.io.IOException: Failed to send RPC 5285936577870185909 to NM-304-SA5212M4-BIGDATA-519/10.142.116.19:44457: java.nio.channels.ClosedChannelException</div><div class="line">        at org.apache.spark.network.client.TransportClient<span class="variable">$3</span>.operationComplete(TransportClient.java:239)</div><div class="line">        at org.apache.spark.network.client.TransportClient<span class="variable">$3</span>.operationComplete(TransportClient.java:226)</div><div class="line">        at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)</div><div class="line">        at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)</div><div class="line">        at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)</div><div class="line">        at io.netty.channel.AbstractChannel<span class="variable">$AbstractUnsafe</span>.safeSetFailure(AbstractChannel.java:801)</div><div class="line">        at io.netty.channel.AbstractChannel<span class="variable">$AbstractUnsafe</span>.write(AbstractChannel.java:699)</div><div class="line">        at io.netty.channel.DefaultChannelPipeline<span class="variable">$HeadContext</span>.write(DefaultChannelPipeline.java:1122)</div><div class="line">        at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)</div><div class="line">        at io.netty.channel.AbstractChannelHandlerContext.access<span class="variable">$1900</span>(AbstractChannelHandlerContext.java:32)</div><div class="line">        at io.netty.channel.AbstractChannelHandlerContext<span class="variable">$AbstractWriteTask</span>.write(AbstractChannelHandlerContext.java:908)</div><div class="line">        at io.netty.channel.AbstractChannelHandlerContext<span class="variable">$WriteAndFlushTask</span>.write(AbstractChannelHandlerContext.java:960)</div><div class="line">        at io.netty.channel.AbstractChannelHandlerContext<span class="variable">$AbstractWriteTask</span>.run(AbstractChannelHandlerContext.java:893)</div><div class="line">        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)</div><div class="line">        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)</div><div class="line">        at io.netty.util.concurrent.SingleThreadEventExecutor<span class="variable">$2</span>.run(SingleThreadEventExecutor.java:111)</div><div class="line">        at java.lang.Thread.run(Thread.java:745)</div><div class="line">Caused by: java.nio.channels.ClosedChannelException</div></pre></td></tr></table></figure></p>
<p>找到driver的日志：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Container: container_1477044851292_468337_02_000004 on NM-304-SA5212M4-BIGDATA-519_8041</div><div class="line">=========================================================================================</div><div class="line">LogType:stderr</div><div class="line">Log Upload Time:星期三 十一月 02 03:51:47 +0800 2016</div><div class="line">LogLength:194115722</div><div class="line">Log Contents:</div></pre></td></tr></table></figure></p>
<p>根据时间定位：（vim中在driver中搜索 16\/11\/02 03:51:3）</p>
<p>发现问题出题的源头：<br>在 16/11/02 03:40 之前，driver的日志还是比较稳定的，但是在 此之后，频繁出现下述异常：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line">16/11/01 15:11:43 INFO ApplicationMaster: Registered signal handlers <span class="keyword">for</span> [TERM, HUP, INT]</div><div class="line">16/11/01 15:11:43 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1477044851292_468337_000002</div><div class="line">16/11/01 15:11:45 INFO SecurityManager: Changing view acls to: wzfw</div><div class="line">16/11/01 15:11:45 INFO SecurityManager: Changing modify acls to: wzfw</div><div class="line">16/11/01 15:11:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(wzfw); users with modify permissions: Set(wzfw)</div><div class="line">...</div><div class="line">16/11/02 03:41:17 WARN Client: Exception encountered <span class="keyword">while</span> connecting to the server : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager<span class="variable">$InvalidToken</span>): Invalid AMRMToken from appattempt_1477044851292_468337_000002</div><div class="line">16/11/02 03:41:17 INFO RetryInvocationHandler: Exception <span class="keyword">while</span> invoking allocate of class ApplicationMasterProtocolPBClientImpl over rm1. Trying to fail over immediately.</div><div class="line">org.apache.hadoop.security.token.SecretManager<span class="variable">$InvalidToken</span>: Invalid AMRMToken from appattempt_1477044851292_468337_000002</div><div class="line">        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</div><div class="line">        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)</div><div class="line">        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class="line">        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)</div><div class="line">        at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)</div><div class="line">        at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:104)</div><div class="line">        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:79)</div><div class="line">        at sun.reflect.GeneratedMethodAccessor137.invoke(Unknown Source)</div><div class="line">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</div><div class="line">        at java.lang.reflect.Method.invoke(Method.java:606)</div><div class="line">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)</div><div class="line">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)</div><div class="line">        at com.sun.proxy.<span class="variable">$Proxy19</span>.allocate(Unknown Source)</div><div class="line">        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)</div><div class="line">        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)</div><div class="line">        at org.apache.spark.deploy.yarn.ApplicationMaster$<span class="variable">$anon</span><span class="variable">$1</span>.run(ApplicationMaster.scala:384)</div><div class="line">Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager<span class="variable">$InvalidToken</span>): Invalid AMRMToken from appattempt_1477044851292_468337_000002</div><div class="line">        at org.apache.hadoop.ipc.Client.call(Client.java:1468)</div><div class="line">        at org.apache.hadoop.ipc.Client.call(Client.java:1399)</div><div class="line">        at org.apache.hadoop.ipc.ProtobufRpcEngine<span class="variable">$Invoker</span>.invoke(ProtobufRpcEngine.java:232)</div><div class="line">        at com.sun.proxy.<span class="variable">$Proxy18</span>.allocate(Unknown Source)</div><div class="line">        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)</div><div class="line">        ... 9 more</div><div class="line">16/11/02 03:41:17 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm2</div><div class="line">16/11/02 03:41:17 INFO RetryInvocationHandler: Exception <span class="keyword">while</span> invoking allocate of class ApplicationMasterProtocolPBClientImpl over rm2 after 1 fail over attempts. Trying to fail over after sleeping <span class="keyword">for</span> 22672ms.</div><div class="line">java.net.ConnectException: Call From NM-304-SA5212M4-BIGDATA-519/10.142.116.19 to NM-304-RH5885V3-BIGDATA-008:8030 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</div><div class="line">        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</div><div class="line">        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)</div><div class="line">        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class="line">        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)</div><div class="line">        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)</div><div class="line">        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)</div><div class="line">        at org.apache.hadoop.ipc.Client.call(Client.java:1472)</div><div class="line">        at org.apache.hadoop.ipc.Client.call(Client.java:1399)</div><div class="line">        at org.apache.hadoop.ipc.ProtobufRpcEngine<span class="variable">$Invoker</span>.invoke(ProtobufRpcEngine.java:232)</div><div class="line">        at com.sun.proxy.<span class="variable">$Proxy18</span>.allocate(Unknown Source)</div><div class="line">        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)</div><div class="line">        at sun.reflect.GeneratedMethodAccessor137.invoke(Unknown Source)</div><div class="line">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</div><div class="line">        at java.lang.reflect.Method.invoke(Method.java:606)</div><div class="line">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)</div><div class="line">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)</div><div class="line">        at com.sun.proxy.<span class="variable">$Proxy19</span>.allocate(Unknown Source)</div><div class="line">        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)</div><div class="line">        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)</div><div class="line">        at org.apache.spark.deploy.yarn.ApplicationMaster$<span class="variable">$anon</span><span class="variable">$1</span>.run(ApplicationMaster.scala:384)</div><div class="line">Caused by: java.net.ConnectException: Connection refused</div><div class="line">        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</div><div class="line">        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)</div><div class="line">        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)</div><div class="line">        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)</div><div class="line">        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)</div><div class="line">        at org.apache.hadoop.ipc.Client<span class="variable">$Connection</span>.setupConnection(Client.java:607)</div><div class="line">        at org.apache.hadoop.ipc.Client<span class="variable">$Connection</span>.setupIOstreams(Client.java:705)</div><div class="line">        at org.apache.hadoop.ipc.Client<span class="variable">$Connection</span>.access<span class="variable">$2800</span>(Client.java:368)</div><div class="line">        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)</div><div class="line">        at org.apache.hadoop.ipc.Client.call(Client.java:1438)</div><div class="line">        ... 13 more</div><div class="line">16/11/02 03:41:20 INFO JobScheduler: Added <span class="built_in">jobs</span> <span class="keyword">for</span> time 1478029280000 ms</div></pre></td></tr></table></figure>
<p>然后这些日志持续出现了十分钟左右，当然spark的stage是依旧在增加，依旧在运行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line">16/11/02 03:51:27 INFO RetryInvocationHandler: Exception <span class="keyword">while</span> invoking allocate of class ApplicationMasterProtocolPBClientImpl over rm1 after 14 fail over attempts. Trying to fail over immediately.</div><div class="line">org.apache.hadoop.security.token.SecretManager<span class="variable">$InvalidToken</span>: Invalid AMRMToken from appattempt_1477044851292_468337_000002</div><div class="line">        at sun.reflect.GeneratedConstructorAccessor74.newInstance(Unknown Source)</div><div class="line">        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class="line">        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)</div><div class="line">        at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)</div><div class="line">        at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:104)</div><div class="line">        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:79)</div><div class="line">        at sun.reflect.GeneratedMethodAccessor137.invoke(Unknown Source)</div><div class="line">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</div><div class="line">        at java.lang.reflect.Method.invoke(Method.java:606)</div><div class="line">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)</div><div class="line">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)</div><div class="line">        at com.sun.proxy.<span class="variable">$Proxy19</span>.allocate(Unknown Source)</div><div class="line">        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)</div><div class="line">        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)</div><div class="line">        at org.apache.spark.deploy.yarn.ApplicationMaster$<span class="variable">$anon</span><span class="variable">$1</span>.run(ApplicationMaster.scala:384)</div><div class="line">Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager<span class="variable">$InvalidToken</span>): Invalid AMRMToken from appattempt_1477044851292_468337_000002</div><div class="line">        at org.apache.hadoop.ipc.Client.call(Client.java:1468)</div><div class="line">        at org.apache.hadoop.ipc.Client.call(Client.java:1399)</div><div class="line">        at org.apache.hadoop.ipc.ProtobufRpcEngine<span class="variable">$Invoker</span>.invoke(ProtobufRpcEngine.java:232)</div><div class="line">        at com.sun.proxy.<span class="variable">$Proxy18</span>.allocate(Unknown Source)</div><div class="line">        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)</div><div class="line">        ... 9 more</div><div class="line">16/11/02 03:51:27 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm2</div><div class="line">16/11/02 03:51:27 INFO RetryInvocationHandler: Exception <span class="keyword">while</span> invoking allocate of class ApplicationMasterProtocolPBClientImpl over rm2 after 15 fail over attempts. Trying to fail over after sleeping <span class="keyword">for</span> 31772ms.</div><div class="line">java.net.ConnectException: Call From NM-304-SA5212M4-BIGDATA-519/10.142.116.19 to NM-304-RH5885V3-BIGDATA-008:8030 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</div><div class="line">        at sun.reflect.GeneratedConstructorAccessor75.newInstance(Unknown Source)</div><div class="line">        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class="line">        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)</div><div class="line">        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)</div><div class="line">        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)</div><div class="line">        at org.apache.hadoop.ipc.Client.call(Client.java:1472)</div><div class="line">        at org.apache.hadoop.ipc.Client.call(Client.java:1399)</div><div class="line">        at org.apache.hadoop.ipc.ProtobufRpcEngine<span class="variable">$Invoker</span>.invoke(ProtobufRpcEngine.java:232)</div><div class="line">        at com.sun.proxy.<span class="variable">$Proxy18</span>.allocate(Unknown Source)</div><div class="line">        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)</div><div class="line">        at sun.reflect.GeneratedMethodAccessor137.invoke(Unknown Source)</div><div class="line">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</div><div class="line">        at java.lang.reflect.Method.invoke(Method.java:606)</div><div class="line">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)</div><div class="line">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)</div><div class="line">        at com.sun.proxy.<span class="variable">$Proxy19</span>.allocate(Unknown Source)</div><div class="line">        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)</div><div class="line">        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)</div><div class="line">        at org.apache.spark.deploy.yarn.ApplicationMaster$<span class="variable">$anon</span><span class="variable">$1</span>.run(ApplicationMaster.scala:384)</div><div class="line">Caused by: java.net.ConnectException: Connection refused</div><div class="line">        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</div><div class="line">        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)</div><div class="line">        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)</div><div class="line">        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)</div><div class="line">        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)</div><div class="line">        at org.apache.hadoop.ipc.Client<span class="variable">$Connection</span>.setupConnection(Client.java:607)</div><div class="line">        at org.apache.hadoop.ipc.Client<span class="variable">$Connection</span>.setupIOstreams(Client.java:705)</div><div class="line">        at org.apache.hadoop.ipc.Client<span class="variable">$Connection</span>.access<span class="variable">$2800</span>(Client.java:368)</div><div class="line">        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)</div><div class="line">        at org.apache.hadoop.ipc.Client.call(Client.java:1438)</div><div class="line">        ... 13 more</div></pre></td></tr></table></figure>
<p>然后在最后收到这个信号就挂了：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">16/11/02 03:51:31 INFO WriteAheadLogManager  <span class="keyword">for</span> Thread: Attempting to clear 0 old <span class="built_in">log</span> files <span class="keyword">in</span> hdfs://ns/user/wzfw/checkpoint/receivedBlockMetadata older than 1478029880000:</div><div class="line">16/11/02 03:51:31 INFO InputInfoTracker: remove old batch metadata: 1478029870000 ms</div><div class="line">16/11/02 03:51:32 INFO ApplicationMaster: Final app status: FAILED, <span class="built_in">exit</span>Code: 16</div><div class="line">16/11/02 03:51:32 WARN ApplicationMaster: Reporter thread fails 2 time(s) <span class="keyword">in</span> a row.</div><div class="line">java.lang.reflect.UndeclaredThrowableException</div><div class="line">        at com.sun.proxy.<span class="variable">$Proxy19</span>.allocate(Unknown Source)</div><div class="line">        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)</div><div class="line">        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)</div><div class="line">        at org.apache.spark.deploy.yarn.ApplicationMaster$<span class="variable">$anon</span><span class="variable">$1</span>.run(ApplicationMaster.scala:384)</div><div class="line">Caused by: java.lang.InterruptedException: sleep interrupted</div><div class="line">        at java.lang.Thread.sleep(Native Method)</div><div class="line">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:151)</div><div class="line">        ... 4 more</div><div class="line">16/11/02 03:51:32 ERROR ApplicationMaster: RECEIVED SIGNAL 15: SIGTERM</div><div class="line">16/11/02 03:51:33 INFO StreamingContext: Invoking stop(stopGracefully=<span class="literal">false</span>) from shutdown hook</div><div class="line">16/11/02 03:51:33 INFO JobGenerator: Stopping JobGenerator immediately</div></pre></td></tr></table></figure>
<p>后来又跑了一次，又因为同样的原因挂了，这次统计了时间：<br>slaver：<br>程序启动时间：16/11/02 22:58:27<br>程序结束时间： 16/11/03 22:51:34</p>
<p>driver<br>程序启动时间：16/11/02 22:58:18<br>遇到Kerberos问题时间： 16/11/03 22:41:23<br>程序结束时间：16/11/03 22:51:35</p>
<p>上面的时间是有不正确的地方的，因为我启动的时间肯定不是16/11/02 22:58:18（那个时候我已经回家了），我是在 11/02号当天上午启动，所以一个很可能的原因是，在22:58的这个时候，因为种种原因，程序崩溃重启，换了一个driver。</p>
<h2 id="可能的原因"><a href="#可能的原因" class="headerlink" title="可能的原因"></a>可能的原因</h2><p>google一下，网上查到有类似的原因：<br>storm on yarn在这个问题中存在一些bug；<br><a href="https://community.cloudera.com/t5/Batch-Processing-and-Workflow/Long-running-yarn-app-storm-yarn-exits-with-Invalid-AMRMToken/td-p/26717" target="_blank" rel="external">https://community.cloudera.com/t5/Batch-Processing-and-Workflow/Long-running-yarn-app-storm-yarn-exits-with-Invalid-AMRMToken/td-p/26717</a></p>
<p><a href="http://www.codeflitting.com/blog/article/%E4%B8%BA%E9%95%BF%E6%97%B6%E9%97%B4%E8%BF%90%E8%A1%8C%E7%9A%84%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E9%85%8D%E7%BD%AE%20YARN" target="_blank" rel="external">http://www.codeflitting.com/blog/article/%E4%B8%BA%E9%95%BF%E6%97%B6%E9%97%B4%E8%BF%90%E8%A1%8C%E7%9A%84%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E9%85%8D%E7%BD%AE%20YARN</a></p>
<p><a href="https://issues.apache.org/jira/browse/YARN-3103" target="_blank" rel="external">https://issues.apache.org/jira/browse/YARN-3103</a></p>
<p>根据日志以及网上资料，初步认为与Kerberos过期认证有关，AMRMToken无效，也就是说，在运行了一段时期（一天多）后，AM到RM的token无效了，然后无效很多次后，后续的任务分配不到container，所以程序就挂了，突然想到，出现这种错误的原因有以下几种；</p>
<ol>
<li><p>大约12个小时以前，SparkStreaming程序因为后面的flume测试守护进程，导致换了一个driver，是否是由于driver改动后，导致token失效？</p>
</li>
<li><p>yarn 2.6.0有一个相关的bug，<a href="https://issues.apache.org/jira/browse/YARN-3103" target="_blank" rel="external">https://issues.apache.org/jira/browse/YARN-3103</a><br>AMRMClientImpl.updateAMRMToken updates the token service before storing it to the credentials, so the token is mapped using the newly updated service rather than the empty service that was used when the RM created the original AMRM token. This leads to two AMRM tokens in the credentials and can still fail if the AMRMTokenSelector picks the wrong one.<br>In addition the AMRMClientImpl grabs the login user rather than the current user when security is enabled, so it’s likely the UGI being updated is not the UGI that will be used when reconnecting to the RM.<br>The end result is that AMs can fail with invalid token errors when trying to reconnect to an RM after a new AMRM secret has been activated.<br>大概意思是，更新token的时候会有两个token，如果选择了错误的token，就会导致出错</p>
</li>
<li><p>hadoop设置仅允许 hdfs 用户的委派令牌保留最大生存期 7 （default）天，这始终是不够的<br>需要修改一下yarn的配置，增加如下参数：</p>
</li>
</ol>
<p>将 ResourceManager 配置为对应 HDFS NameNode 的代理用户，以便在现有令牌超过其最大生存期时，ResourceManager 可以请求新的令牌。YARN 随后能够代表 hdfs 用户继续执行本地化和日志聚合</p>
<figure class="highlight xml"><figcaption><span>yarn-site.xml</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.proxy-user-privileges.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line"></div><div class="line">``` xml core-site.xml</div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.yarn.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.yarn.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<p>然后重启YARN和HDFS服务</p>
<h2 id="问题的暂时解决"><a href="#问题的暂时解决" class="headerlink" title="问题的暂时解决"></a>问题的暂时解决</h2><p>由于项目紧张，我写了一个脚本，每隔一分钟监控yarn上面运行的spark Streaming程序，如果挂了就重新启动起来，虽然也不影响后续的结果。后来观察，基本上是一天一挂或者两天一挂，不过这也太不靠谱了。</p>
<h2 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h2><p>后来发现都不是上述原因，同事的前爱奇艺的同事遇到过同样的问题：原因是：<br><strong> NameNode采用了HA后，AM与Namenode通信使用的token的结构变为HA token,HA token中会有两个private token，代表两台namenode服务。 当AM更新token时，会调用hadoop客户端的addDelegationTokens更新token。但addDelegationTokens存在问题，其只会更新HA token，不会更新private token，而AM向NameNode发起请求时，会使用private token，导致出现异常。</strong></p>
<p>发生这个问题需要以下几个条件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">1. NameNode HA is enabled.</div><div class="line">2. Kerberos is enabled.</div><div class="line">3. HDFS Delegation Token (not Keytab or TGT) is used to communicate with NameNode.</div><div class="line">4. We want to update the HDFS Delegation Token <span class="keyword">for</span> long running applicatons. HDFS Client will generate private tokens <span class="keyword">for</span> each NameNode. When we update the HDFS Delegation Token, these private tokens will not be updated, <span class="built_in">which</span> will cause token expired.</div></pre></td></tr></table></figure></p>
<h3 id="hadoop修复方案"><a href="#hadoop修复方案" class="headerlink" title="hadoop修复方案"></a>hadoop修复方案</h3><p>这是hadoop的一个bug，在hadoop 2.9的新版本中修复了这个问题，所以，给hadoop打个patch就好了：</p>
<p>Hadoop: <a href="https://issues.apache.org/jira/browse/HDFS-9276" target="_blank" rel="external">https://issues.apache.org/jira/browse/HDFS-9276</a><br>hadoop2.9修复了private token不更新问题</p>
<h3 id="spark修复方案"><a href="#spark修复方案" class="headerlink" title="spark修复方案"></a>spark修复方案</h3><p>Spark： <a href="https://github.com/apache/spark/pull/9168/files" target="_blank" rel="external">https://github.com/apache/spark/pull/9168/files</a><br>主动使FileSystem对象关闭，再重新建立。FileSystem对象建立期间会重新设置private token</p>
<p>由于hadoop变动对生产环境的影响很大，所以我们选择了spark的修复方案，修改了源码后打包编译上线，问题解决；</p>
<p>感谢聪哥，也感谢那位在爱奇艺的发现并解决了这个问题的大神，没有你们，我要惨了。</p>
<p>与Kerberos对干的日子很酸爽</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark踩坑之Streaming程序实时读写哨兵模式的Redis]]></title>
      <url>http://flume.cn/2016/11/24/Spark%E8%B8%A9%E5%9D%91%E4%B9%8BStreaming%E7%A8%8B%E5%BA%8F%E5%AE%9E%E6%97%B6%E8%AF%BB%E5%86%99%E5%93%A8%E5%85%B5%E6%A8%A1%E5%BC%8F%E7%9A%84Redis/</url>
      <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Spark Streaming程序使用Redis保存出现在景区中的用户，用以识别用户是否是第一次进入景区，要分别进行读操作和写操作，为了程序的高可用性，Redis我们使用的是哨兵模式（满满的，都是坑）；</p>
<p>Redis哨兵模式，顾名思义，就是Redis有三台机器作为一主两备，然后有三个哨兵（三个端口），告诉你Master是哪一台，然后你再去访问master，这样的话，一台机器由于负载发生了主备切换，对我们来说，对外的端口是统一的（哨兵）；</p>
<p>我们Redis的版本是3.2.4，连接Redis，我们的程序使用了开源的jedis作为redis的连接器，jedis的版本是2.8.1，关于jedis的使用，我参考的是jedis源码的单元测试程序，这个可以在github中找到。</p>
<h2 id="踩坑历程"><a href="#踩坑历程" class="headerlink" title="踩坑历程"></a>踩坑历程</h2><h3 id="hgetAll不能读取太大量的数据"><a href="#hgetAll不能读取太大量的数据" class="headerlink" title="hgetAll不能读取太大量的数据"></a>hgetAll不能读取太大量的数据</h3><p>在设计程序的时候，我们使用了hash作为数据的存储结构，该hash可以存储40亿条记录，一开始，为了减少对Redis的压力，我们选择的策略是对于每个batch，我们都统一读取全部的数据，然后在内存中筛选计算的方法，可以极大减少对redis的访问；</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> sentinelPool = <span class="type">InternalRedisClient</span>.getSentinelPool</div><div class="line"><span class="keyword">var</span> phoneMap: util.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="keyword">new</span> util.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>]()</div><div class="line"><span class="comment">// printLog.debug( "sentinelPool NumIdle0: " + sentinelPool.getNumIdle + " Active0: " + sentinelPool.getNumActive)</span></div><div class="line"><span class="keyword">var</span> jedis1: <span class="type">Jedis</span> = <span class="literal">null</span></div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">  jedis1 = sentinelPool.getResource</div><div class="line">  <span class="comment">// printLog.debug( "sentinelPool NumIdle1: " + sentinelPool.getNumIdle + " Active1: " + sentinelPool.getNumActive)</span></div><div class="line">  <span class="comment">// 这里当redis中该HSet数据量很大的时候（五十万条），一次读取需要很长时间，对性能影响很大，</span></div><div class="line">  <span class="comment">// 故当数据条数大于一定值的时候，不用此方法</span></div><div class="line">  phoneMap = jedis1.hgetAll(redisHashKey)</div><div class="line">  <span class="comment">// printLog.debug( "sentinelPool NumIdle3: " + sentinelPool.getNumIdle + " Active3: " + sentinelPool.getNumActive)</span></div><div class="line">  printLog.debug(<span class="string">"phoneSet_1: "</span> + phoneMap)</div><div class="line">&#125; <span class="keyword">finally</span> &#123;</div><div class="line">  <span class="keyword">if</span> (jedis1 != <span class="literal">null</span>) &#123;</div><div class="line">    printLog.debug(<span class="string">"close jedis1"</span>)</div><div class="line">    jedis1.close()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">但是当程序运行一段时间后，发现延迟很高，我们也没有想到数据量会有这么大，发现当数据量超过<span class="number">10</span>W条这个级别后，每次读取全部数据的耗时将会很大，而且对redis的压力反而变大了，所以我们弃用了一次读取所有数据的方法，但是，如果数据量少与<span class="number">10</span>万条级别的话，一次读取，也是值得考虑的；</div><div class="line"></div><div class="line">### jedis的sentinel连接池在spark <span class="type">Streaming</span>中连接池无法释放的问题</div><div class="line"></div><div class="line">我们一开始采用的是通过jedis的sentinel连接池的方法连接redis；</div><div class="line">在连接哨兵模式的redis的时候，由于<span class="type">SparkStreaming</span>程序中每个container并不会关闭，导致在spark的transform方法中，jedis sentinel 连接池在<span class="type">Streaming</span>下出现连接池资源释放不了的bug，为了解决这个问题，我们弃用了jedis的sentinel连接池，每<span class="number">10</span>秒手动询问redis的哨兵<span class="type">Master</span>的地址，然后手动与redis进行连接，最终解决了这个问题。</div><div class="line">手动实现jedis sentinel的连接的代码如下：</div><div class="line"></div><div class="line">``` scala</div><div class="line"><span class="comment">// 由于redis sentinel 建立连接池不释放的坑，最后弃用连接池，自己实现寻找master的逻辑</span></div><div class="line"><span class="keyword">val</span> masterHostPort = getRedisMasterHostPortList(redisConf)</div><div class="line"><span class="keyword">val</span> masterHost = masterHostPort(<span class="number">0</span>)</div><div class="line"><span class="keyword">val</span> masterPort = masterHostPort(<span class="number">1</span>).toInt</div><div class="line">...</div><div class="line"><span class="comment">// 得到master后，就可以直接建立redis连接了</span></div><div class="line"><span class="keyword">val</span> paRdd3: <span class="type">Iterator</span>[(<span class="type">String</span>, (<span class="type">String</span>, <span class="type">String</span>))] = paRdd2.flatMap &#123; eachKV =&gt;</div><div class="line">。。。</div><div class="line">	<span class="keyword">var</span> jedis1: <span class="type">Jedis</span> = <span class="literal">null</span></div><div class="line">	<span class="keyword">var</span> redis1Val: <span class="type">String</span> = <span class="literal">null</span></div><div class="line">	<span class="keyword">try</span> &#123;</div><div class="line">	  jedis1 = <span class="keyword">new</span> <span class="type">Jedis</span>(masterHost, masterPort)</div><div class="line">	  redis1Val = jedis1.hget(redisHashKey, mdn)</div><div class="line">	&#125; <span class="keyword">catch</span> &#123;</div><div class="line">	  <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; &#123;</div><div class="line">	    printLog.error(<span class="string">"jedis1 error：e"</span> + e)</div><div class="line">	  &#125;</div><div class="line">	&#125; <span class="keyword">finally</span> &#123;</div><div class="line">	  <span class="keyword">if</span> (jedis1 != <span class="literal">null</span>) &#123;</div><div class="line">	    jedis1.close()</div><div class="line">	  &#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>询问哨兵Master在哪里的代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">    * 得到redis sentinel 模式的master信息</div><div class="line">    *</div><div class="line">    * @param redisConf</div><div class="line">    * @return</div><div class="line">    */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getRedisMasterHostPortList</span></span>(redisConf: <span class="type">RedisConf</span>): util.<span class="type">List</span>[<span class="type">String</span>] = &#123;</div><div class="line">    printLog.debug(<span class="string">"Trying to find master from available Sentinels..."</span>)</div><div class="line">    <span class="comment">//    @transient var masterFound: Boolean = false</span></div><div class="line">    <span class="keyword">var</span> masterFound: <span class="type">Boolean</span> = <span class="literal">false</span></div><div class="line">    <span class="keyword">val</span> sentinelList: <span class="type">Array</span>[<span class="type">String</span>] = redisConf.sentinels.split(<span class="string">"\\|"</span>)</div><div class="line">    <span class="keyword">for</span> (sentinel &lt;- sentinelList <span class="keyword">if</span> !masterFound) &#123;</div><div class="line">      <span class="keyword">var</span> jedis: <span class="type">Jedis</span> = <span class="literal">null</span></div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        jedis = <span class="keyword">new</span> <span class="type">Jedis</span>(sentinel.split(<span class="string">":"</span>)(<span class="number">0</span>), sentinel.split(<span class="string">":"</span>)(<span class="number">1</span>).toInt)</div><div class="line">        <span class="keyword">val</span> masterAddr: util.<span class="type">List</span>[<span class="type">String</span>] = jedis.sentinelGetMasterAddrByName(redisConf.masterName)</div><div class="line">        <span class="comment">// connected to sentinel...</span></div><div class="line">        <span class="keyword">if</span> (masterAddr == <span class="literal">null</span> || masterAddr.size != <span class="number">2</span>) &#123;</div><div class="line">          printLog.error(<span class="string">"Can not get master addr, master name: "</span> + redisConf.masterName + <span class="string">". Sentinel: "</span> + sentinel + <span class="string">"."</span>)</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          masterFound = <span class="literal">true</span></div><div class="line">          printLog.debug(<span class="string">"masterAddr: "</span> + masterAddr)</div><div class="line">          <span class="keyword">return</span> masterAddr</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">catch</span> &#123;</div><div class="line">        <span class="keyword">case</span> e: <span class="type">JedisException</span> =&gt; &#123;</div><div class="line">          <span class="comment">// resolves #1036, it should handle JedisException there's another chance</span></div><div class="line">          <span class="comment">// of raising JedisDataException</span></div><div class="line">          printLog.error(<span class="string">"Cannot get master address from sentinel running @ "</span> + sentinel + <span class="string">". Reason: "</span> + e + <span class="string">". Trying next one."</span>)</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">finally</span> &#123;</div><div class="line">        <span class="keyword">if</span> (jedis != <span class="literal">null</span>) &#123;</div><div class="line">          jedis.close()</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="literal">null</span></div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<h3 id="Redis的压力负载"><a href="#Redis的压力负载" class="headerlink" title="Redis的压力负载"></a>Redis的压力负载</h3><p>后来我们对程序进行了加压测试，发现当压力增大后redis的主备切换非常频繁，原来在redis中，slaver会定时跟master通信，询问其是否健康，如果不健康，slaver就会强制进行主备切换，当redis访问频繁的时候，master来不及及时响应slaver的请求，就会导致主备切换频繁，解决这个问题的方法是修改询问时间，默认是 5s，适度调高就可以了。</p>
<p>当然redis还有一些其它优化参数，这里不做讨论；</p>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p><a href="https://github.com/xetorthio/jedis" target="_blank" rel="external">https://github.com/xetorthio/jedis</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark踩坑之Streaming程序实时读取数据库]]></title>
      <url>http://flume.cn/2016/11/24/Spark%E8%B8%A9%E5%9D%91%E4%B9%8BStreaming%E7%A8%8B%E5%BA%8F%E5%AE%9E%E6%97%B6%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E5%BA%93/</url>
      <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>一个SparkStreaming的项目，由于需要从Mysql数据库中实时读取一些信息，然后生成特定的数据结构进行动态的处理，在此过程中踩了一些坑，谨记：</p>
<h4 id="初始方案"><a href="#初始方案" class="headerlink" title="初始方案"></a>初始方案</h4><p>在rdd的每个partition中创建一个内部数据库连接池单例对象InternalMDBManager，然后使用一个连接池</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> dStream2: <span class="type">DStream</span>[(<span class="type">String</span>, (<span class="type">String</span>, <span class="type">String</span>))] = dStream1.transform &#123;</div><div class="line">  rdd1 =&gt; &#123;</div><div class="line">    <span class="keyword">val</span> numPartitions = rdd1.getNumPartitions</div><div class="line">    printLog.info(<span class="string">"numPartitions: "</span> + numPartitions)</div><div class="line"></div><div class="line">    <span class="keyword">val</span> rdd3: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">String</span>, <span class="type">String</span>))] = rdd1.mapPartitions &#123;</div><div class="line">      paRdd1 =&gt; &#123;</div><div class="line"></div><div class="line"></div><div class="line">        <span class="comment">/**</span></div><div class="line">          * Author: wangxiaogang</div><div class="line">          *</div><div class="line">          * 由于需要在spark中分布式读取Mysql的数据，所以需要创建可以分布式分发的单例对象连接池</div><div class="line">          */</div><div class="line">        <span class="class"><span class="keyword">object</span> <span class="title">InternalMDBManager</span> <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</div><div class="line">          <span class="meta">@transient</span> <span class="keyword">private</span> <span class="keyword">var</span> pool: <span class="type">ComboPooledDataSource</span> = _</div><div class="line"></div><div class="line">          <span class="comment">/**</span></div><div class="line">            * 从连接池获取连接</div><div class="line">            *</div><div class="line">            * @return</div><div class="line">            */</div><div class="line">          <span class="function"><span class="keyword">def</span> <span class="title">getConnection</span></span>: <span class="type">Connection</span> = &#123;</div><div class="line">            <span class="keyword">try</span> &#123;</div><div class="line">              pool.getConnection()</div><div class="line">            &#125; <span class="keyword">catch</span> &#123;</div><div class="line">              <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; ex.printStackTrace()</div><div class="line">                <span class="literal">null</span></div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line"></div><div class="line">          <span class="function"><span class="keyword">def</span> <span class="title">closeConnection</span></span>(connection: <span class="type">Connection</span>): <span class="type">Unit</span> = &#123;</div><div class="line">            <span class="keyword">if</span> (!connection.isClosed) connection.close()</div><div class="line">          &#125;</div><div class="line"></div><div class="line">          <span class="comment">/**</span></div><div class="line">            * 创建连接池</div><div class="line">            *</div><div class="line">            * @param sqlPoolConf</div><div class="line">            */</div><div class="line">          <span class="function"><span class="keyword">def</span> <span class="title">makePool</span></span>(sqlPoolConf: <span class="type">SqlPoolConf</span>): <span class="type">Unit</span> = &#123;</div><div class="line">            <span class="keyword">if</span> (pool == <span class="literal">null</span>) &#123;</div><div class="line">              <span class="keyword">try</span> &#123;</div><div class="line">                pool = <span class="keyword">new</span> <span class="type">ComboPooledDataSource</span>(<span class="literal">true</span>)</div><div class="line">                pool.setJdbcUrl(sqlPoolConf.jdbcUrl)</div><div class="line">                pool.setDriverClass(sqlPoolConf.driverClass)</div><div class="line">                pool.setUser(sqlPoolConf.user)</div><div class="line">                pool.setPassword(sqlPoolConf.password)</div><div class="line">                pool.setMaxPoolSize(sqlPoolConf.maxPoolSize)</div><div class="line">                pool.setMinPoolSize(sqlPoolConf.minPoolSize)</div><div class="line">                pool.setAcquireIncrement(sqlPoolConf.acquireIncrement)</div><div class="line">                pool.setInitialPoolSize(sqlPoolConf.initialPoolSize)</div><div class="line">                pool.setMaxIdleTime(sqlPoolConf.maxIdleTime)</div><div class="line">              &#125; <span class="keyword">catch</span> &#123;</div><div class="line">                <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; ex.printStackTrace()</div><div class="line">              &#125;</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="type">InternalRedisClient</span>.makeSentinelPool(redisConf)</div><div class="line"></div><div class="line">        <span class="type">InternalMDBManager</span>.makePool(sqlPoolConf)</div><div class="line">        <span class="keyword">val</span> sqlConn = sqlPool.getConnection</div><div class="line">        <span class="keyword">val</span> sqlConn = <span class="type">InternalMDBManager</span>.getConnection</div><div class="line"></div><div class="line">        。。。</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法听起来是极好的，在每个jvm中创建一个连接池，然后不同的批数据使用共同的连接池，但是在实践的过程中发现，在foreachRDD中使用这种方法是可以的，网上有很多类似的例子；<br>但是在transform方法中，如果在map，flatMap，filter等方法外面建立连接池，会出现连接池无法释放的问题，无论你如何使用finally释放，都释放不了；<br>解决方法是<strong>避免使用连接池，将数据库建立与释放操作封装到同一个函数里</strong>，在我的问题里，因为对数据库的操作不会很频繁，所以不需要引入连接池，这样将会及时释放数据库资源。</p>
<p>最终方案是如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPositionSubDataMap</span></span>(sqlPoolConf: <span class="type">SqlPoolConf</span>): util.<span class="type">HashMap</span>[<span class="type">Int</span>, util.<span class="type">LinkedList</span>[<span class="type">PositionSubData</span>]] = &#123;</div><div class="line"><span class="keyword">val</span> currentTime: <span class="type">Long</span> = getCurrentTime</div><div class="line"><span class="comment">// todo: 如果数据量比较大的话，判断时间语句直接放到mysql查询的时候</span></div><div class="line"><span class="keyword">val</span> sqlStr =</div><div class="line"><span class="string">""</span><span class="string">"some sqls"</span><span class="string">""</span></div><div class="line"></div><div class="line"><span class="comment">// 这里告诉我们，写代码的时候不要盲目建立资源池，不要简单的东西复杂化</span></div><div class="line"><span class="keyword">val</span> url = sqlPoolConf.jdbcUrl</div><div class="line"><span class="keyword">val</span> user = sqlPoolConf.user</div><div class="line"><span class="keyword">val</span> password = sqlPoolConf.password</div><div class="line"><span class="keyword">var</span> sqlConn: <span class="type">Connection</span> = <span class="literal">null</span></div><div class="line"><span class="comment">//    var pstmt: PreparedStatement = null</span></div><div class="line"><span class="comment">//    var rs: ResultSet = null</span></div><div class="line"><span class="keyword">val</span> positionSubDataMap: util.<span class="type">HashMap</span>[<span class="type">Int</span>, util.<span class="type">LinkedList</span>[<span class="type">PositionSubData</span>]] = <span class="keyword">new</span> util.<span class="type">HashMap</span>[<span class="type">Int</span>, util.<span class="type">LinkedList</span>[<span class="type">PositionSubData</span>]]()</div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">  sqlConn = <span class="type">DriverManager</span>.getConnection(url, user, password)</div><div class="line">  <span class="keyword">val</span> pstmt: <span class="type">PreparedStatement</span> = sqlConn.prepareStatement(sqlStr)</div><div class="line">  <span class="keyword">val</span> rs: <span class="type">ResultSet</span> = pstmt.executeQuery()</div><div class="line">  <span class="comment">//    var positionSubDataList = ArrayBuffer[PositionSubData]</span></div><div class="line"></div><div class="line">  <span class="keyword">while</span> (rs.next()) &#123;</div><div class="line">    <span class="keyword">val</span> subId: <span class="type">Long</span> = rs.getLong(<span class="string">"sub_id"</span>)</div><div class="line">    <span class="keyword">val</span> spId: <span class="type">String</span> = rs.getString(<span class="string">"sp_id"</span>)</div><div class="line">    <span class="keyword">val</span> locationId: <span class="type">String</span> = rs.getString(<span class="string">"location_id"</span>)</div><div class="line">    <span class="keyword">val</span> provId: <span class="type">String</span> = rs.getString(<span class="string">"prov_id"</span>)</div><div class="line">    <span class="keyword">val</span> cityCode: <span class="type">String</span> = rs.getString(<span class="string">"city_code"</span>)</div><div class="line">    <span class="keyword">val</span> intervalTime: <span class="type">Int</span> = rs.getInt(<span class="string">"interv"</span>)</div><div class="line">    <span class="keyword">val</span> available: <span class="type">Int</span> = rs.getInt(<span class="string">"available"</span>)</div><div class="line">    <span class="keyword">val</span> startTime = rs.getLong(<span class="string">"start_time"</span>)</div><div class="line">    <span class="keyword">val</span> endTime = rs.getLong(<span class="string">"end_time"</span>)</div><div class="line">    <span class="keyword">val</span> centerLongitude: <span class="type">Double</span> = rs.getDouble(<span class="string">"center_longitude"</span>)</div><div class="line">    <span class="keyword">val</span> centerLatitude: <span class="type">Double</span> = rs.getDouble(<span class="string">"center_latitude"</span>)</div><div class="line">    <span class="keyword">val</span> radius: <span class="type">Int</span> = rs.getInt(<span class="string">"radius"</span>)</div><div class="line">    <span class="keyword">val</span> shape: <span class="type">String</span> = rs.getString(<span class="string">"shape"</span>)</div><div class="line"></div><div class="line">    <span class="comment">//      printLog.info("cityCode:" + cityCode)</span></div><div class="line"></div><div class="line">    <span class="keyword">val</span> positionSubData: <span class="type">PositionSubData</span> = <span class="type">PositionSubData</span>(subId, spId, locationId, provId, cityCode, intervalTime,</div><div class="line">      available, startTime, endTime, centerLongitude, centerLatitude, radius, shape)</div><div class="line">    <span class="comment">//      printLog.info("positionSubData: " + positionSubData)</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> ((startTime &lt; currentTime) &amp;&amp; (endTime &gt; currentTime)) &#123;</div><div class="line">      <span class="comment">//        positionSubDataList. += positionSubData</span></div><div class="line">      <span class="keyword">val</span> cityCodeArray: <span class="type">Array</span>[<span class="type">String</span>] = cityCode.split(<span class="string">"\\|"</span>)</div><div class="line">      <span class="keyword">for</span> (eachCityCode &lt;- cityCodeArray) &#123;</div><div class="line">        <span class="keyword">if</span> (positionSubDataMap.get(eachCityCode.toInt) == <span class="literal">null</span>) &#123;</div><div class="line">          <span class="comment">// 代表以该城市为key没有其它景区</span></div><div class="line">          <span class="keyword">val</span> positionSubDataList: util.<span class="type">LinkedList</span>[<span class="type">PositionSubData</span>] = <span class="keyword">new</span> util.<span class="type">LinkedList</span>[<span class="type">PositionSubData</span>]</div><div class="line">          positionSubDataList.add(positionSubData)</div><div class="line">          <span class="comment">//            printLog.info("1positionSubDataList：" + positionSubDataList)</span></div><div class="line">          positionSubDataMap.put(eachCityCode.toInt, positionSubDataList)</div><div class="line">          <span class="comment">//            printLog.info("1positionSubDataMap: " + positionSubDataMap)</span></div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          <span class="comment">// 代表以该城市为key有其它景区，并且已经记录在案</span></div><div class="line">          <span class="keyword">val</span> positionSubDataList: util.<span class="type">LinkedList</span>[<span class="type">PositionSubData</span>] = positionSubDataMap.get(eachCityCode.toInt)</div><div class="line">          positionSubDataList.add(positionSubData)</div><div class="line">          <span class="comment">//            printLog.info("2positionSubDataList：" + positionSubDataList)</span></div><div class="line">          <span class="comment">//            printLog.info("2eachCityCode.toInt:" + eachCityCode.toInt)</span></div><div class="line">          positionSubDataMap.put(eachCityCode.toInt, positionSubDataList)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (rs != <span class="literal">null</span>) &#123;</div><div class="line">    rs.close()</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (pstmt != <span class="literal">null</span>) &#123;</div><div class="line">    pstmt.close()</div><div class="line">  &#125;</div><div class="line">&#125; <span class="keyword">catch</span> &#123;</div><div class="line">  <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; &#123;</div><div class="line">    printLog.error(<span class="string">"数据库连接错误：e"</span> + e)</div><div class="line">  &#125;</div><div class="line">&#125; <span class="keyword">finally</span> &#123;</div><div class="line">  <span class="keyword">if</span> (sqlConn != <span class="literal">null</span>) &#123;</div><div class="line">    sqlConn.close()</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">positionSubDataMap</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>综上所述，Streaming程序中动态连接数据库要谨慎，要及时查看数据库的连接状态，看看数据库连接有没有被及时释放，它不会马上就报错，但随着连接数到达数据库的最高值的时候就会出错，检测不及时，等上了生产再出问题，就后悔莫及。</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p>在foreachRDD中建立连接池的例子<br><a href="http://www.cnblogs.com/xlturing/p/spark.html" target="_blank" rel="external">http://www.cnblogs.com/xlturing/p/spark.html</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[scala通过slick连接数据库]]></title>
      <url>http://flume.cn/2016/10/31/scala%E9%80%9A%E8%BF%87slick%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/</url>
      <content type="html"><![CDATA[<p>（持续更新）<br>由于Spark是由scala语言开发的，scala语言可以使用到所有java语言中的特性，所以spark连接数据库（比如Mysql）有很多种方法，这里记录两种我使用到的高级用法以及一些教训，分别是：</p>
<ol>
<li>使用Slick优雅地连接数据库；</li>
<li>如何使用SparkStreaming实时地获取数据库中的内容；</li>
<li>连接数据库过程中的踩坑集锦。</li>
</ol>
<h2 id="使用Slick优雅地连接数据库"><a href="#使用Slick优雅地连接数据库" class="headerlink" title="使用Slick优雅地连接数据库"></a>使用Slick优雅地连接数据库</h2><p>如果使用scala语言，当然可以想到的是，通过java连接数据库的方式连接数据库是没有问题的，但是scala语言有没有自己更加优雅地方法连接数据库呢？答案是肯定的，非常推荐使用：Slick</p>
<h3 id="Slick简介"><a href="#Slick简介" class="headerlink" title="Slick简介"></a>Slick简介</h3><p>Slick 是 TypeSafe 推出的 Scala 数据库访问库。开发者可以使用 Scala 语言风格来编写数据查询，而不是用 SQL 。 Slick 对于 Scala 来说，有如 LINQ 至于 C#，或者类似于其它平台上的 ORM 系统，它使用应用使用数据库有如使用 Scala 内置的集合类型（比如列表，集合等）一样方便。当然如有需要你还是可以直接使用 SQL 语句来查询数据库。<br>使用 Slick 而不直接使用 SQL 语句，可以使用编译器帮助发现一些类型错误，同时 Slick 可以为不同的后台数据库类型生成查询。它具有一些如下的特性：</p>
<ol>
<li>Scala </li>
</ol>
<p>所有查询，表格和字段映射，以及类型都采用普通的 Scala 语法。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Coffees</span>(<span class="params">tag: <span class="type">Tag</span></span>) <span class="keyword">extends</span> <span class="title">Table</span>[(<span class="type">String</span>, <span class="type">Double</span>)](<span class="params">tag, "<span class="type">COFFEES</span>"</span>) </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">name</span> </span>= column[<span class="type">String</span>](<span class="string">"COF_NAME"</span>, <span class="type">O</span>.<span class="type">PrimaryKey</span>)</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">price</span> </span>= column[<span class="type">Double</span>](<span class="string">"PRICE"</span>)</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">*</span> </span>= (name, price)</div><div class="line">&#125;</div><div class="line"><span class="keyword">val</span> coffees = <span class="type">TableQuery</span>[<span class="type">Coffees</span>]</div></pre></td></tr></table></figure></p>
<p>数据访问接口类型 Scala 的集合类型<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Query that only returns the "name" column</span></div><div class="line">coffees.map(_.name)</div><div class="line"></div><div class="line"><span class="comment">// Query that does a "where price &lt; 10.0"</span></div><div class="line">coffees.filter(_.price &lt; <span class="number">10.0</span>)</div></pre></td></tr></table></figure></p>
<ol>
<li>类型安全</li>
</ol>
<p>你使用的 IDE 可以帮助你写代码 在编译时而无需到运行时就可以发现一些错误<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// The result of "select PRICE from COFFEES" is a Seq of Double</span></div><div class="line"><span class="comment">// because of the type safe column definitions</span></div><div class="line"><span class="keyword">val</span> coffeeNames: <span class="type">Seq</span>[<span class="type">Double</span>] = coffees.map(_.price).list</div><div class="line"></div><div class="line"><span class="comment">// Query builders are type safe:</span></div><div class="line">coffees.filter(_.price &lt; <span class="number">10.0</span>)</div><div class="line"><span class="comment">// Using a string in the filter would result in a compilation error</span></div></pre></td></tr></table></figure></p>
<ol>
<li>可以组合</li>
</ol>
<p>查询接口为函数，这些函数可以多次组合和重用。可以使用函数式的方式来访问数据库</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Create a query for coffee names with a price less than 10, sorted by name</span></div><div class="line">coffees.filter(_.price &lt; <span class="number">10.0</span>).sortBy(_.name).map(_.name)</div><div class="line"><span class="comment">// The generated SQL is equivalent to:</span></div><div class="line"><span class="comment">// select name from COFFEES where PRICE &lt; 10.0 order by NAME</span></div></pre></td></tr></table></figure>
<ol>
<li><p>支持几乎所有常见的数据库</p>
<ul>
<li>DB2 (via slick-extensions)</li>
<li>Derby/JavaDB</li>
<li>H2</li>
<li>HSQLDB/HyperSQL</li>
<li>Microsoft Access</li>
<li>Microsoft SQL Server (via slick-extensions)</li>
<li>MySQL</li>
<li>Oracle (via slick-extensions)</li>
<li>PostgreSQL</li>
<li>SQLite</li>
</ul>
</li>
</ol>
<p>对于其它的一些数据库类型 Slick 也提供了有限的支持。</p>
<p>关于Slick的具体教程以及API，可以参阅<br><a href="http://slick.lightbend.com/" target="_blank" rel="external">Slick官网</a><br><a href="http://wiki.jikexueyuan.com/project/slick-guide/" target="_blank" rel="external">极客学院Slick中文教程</a><br>以及google</p>
<h3 id="如何使用到Spark项目中"><a href="#如何使用到Spark项目中" class="headerlink" title="如何使用到Spark项目中"></a>如何使用到Spark项目中</h3><p>以我自己摸索的方法为例，当然会有更多方法</p>
<h4 id="配置文件中配置数据库连接信息"><a href="#配置文件中配置数据库连接信息" class="headerlink" title="配置文件中配置数据库连接信息"></a>配置文件中配置数据库连接信息</h4><p>Slick默认会读取项目顶层的配置文件，当然配置文件的路径可以手动指定，默认在顶层的配置文件路径下，我的配置文件在 <code>resources/application.conf</code>中：</p>
<figure class="highlight plain"><figcaption><span>resources/application.conf</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">// todo: 使其变成可以从外部文件载入</div><div class="line">// 开发环境Mysql</div><div class="line">mysql = &#123;</div><div class="line">  url = &quot;jdbc:mysql://someIp:3306/someDb?useUnicode=true&amp;characterEncoding=utf-8&quot;</div><div class="line">  driver = &quot;com.mysql.jdbc.Driver&quot;</div><div class="line">  connectionPool = disabled</div><div class="line">  keepAliveConnection = true</div><div class="line">  databaseName = &quot;someDb&quot;</div><div class="line">  user = &quot;user&quot;</div><div class="line">  password = &quot;password&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="定义数据库表格对应的case-class"><a href="#定义数据库表格对应的case-class" class="headerlink" title="定义数据库表格对应的case class"></a>定义数据库表格对应的case class</h4><p>比如：该AppFrame是我定义的一个应用框架的样例类，与数据库中的字段有对应关系<br>PS：AppFrame的设置是为了将一切配置写到数据库中，这样可以实现项目的热切换，一套程序可以不需要重新编译而使用到不同的环境不同的策略中，亲测有效。</p>
<figure class="highlight scala"><figcaption><span>bean/AppFrame.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Author: wangxiaogang</div><div class="line">  * Date: 2016/10/1</div><div class="line">  * Email: wangxiaogang@chinatelecom.cn</div><div class="line">  * 应用的整体描述，包括 appId, app名称，输入类型，输入类型详细配置表，输出类型，输出类型详细配置表</div><div class="line">  */</div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">AppFrame</span> (<span class="params"></span></span></div><div class="line">               id: <span class="type">Int</span>,</div><div class="line">               name: <span class="type">String</span>,</div><div class="line">               inputStr: <span class="type">String</span>,</div><div class="line">               // 代表对应的输入在所在类型的输入中的id</div><div class="line">               inputId: <span class="type">Int</span>,</div><div class="line">               outputStr: <span class="type">String</span>,</div><div class="line">               outId: <span class="type">Int</span>,</div><div class="line">               redisStr: <span class="type">String</span>,</div><div class="line">               redisId: <span class="type">Int</span>,</div><div class="line">               sqlPoolStr: <span class="type">String</span>,</div><div class="line">               sqlPoolId:<span class="type">Int</span></div><div class="line">               )&#123;&#125;</div></pre></td></tr></table></figure>
<h4 id="使用Slick编写读取数据库的逻辑"><a href="#使用Slick编写读取数据库的逻辑" class="headerlink" title="使用Slick编写读取数据库的逻辑"></a>使用Slick编写读取数据库的逻辑</h4><p>这里就是slick的优势，非常简单</p>
<figure class="highlight scala"><figcaption><span>dao/MysqlDao.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Author: wangxiaogang</div><div class="line">  * Date: 2016/9/29</div><div class="line">  * Email: wangxiaogang@chinatelecom.cn</div><div class="line">  * 与Mysql数据库的交互类，使用了Slick方式</div><div class="line">  */</div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">MysqlDao</span> </span>&#123;</div><div class="line">  <span class="comment">/**</span></div><div class="line">    * 通过appId获取到应用的整体描述，包括 appId, 输入类型，输入类型详细配置表，输出类型，输出类型详细配置表</div><div class="line">    * 这里简单试验大数据实时处理框架的构思是否可行</div><div class="line">    *</div><div class="line">    * @param appId</div><div class="line">    * @return</div><div class="line">    * @todo : 目前只配置输入类型与输出类型的配置，以后尽量把所有数据处理方案都能以配置的形式写入数据库中</div><div class="line">    */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getAppFrame</span></span>(appId: <span class="type">Int</span>): <span class="type">AppFrame</span> = &#123;</div><div class="line">    <span class="keyword">implicit</span> <span class="keyword">val</span> getResult = <span class="type">GetResult</span>(r =&gt;</div><div class="line">      <span class="type">AppFrame</span>(r.nextInt(), r.nextString(), r.nextString(), r.nextInt(), r.nextString(), r.nextInt(), r.nextString(),</div><div class="line">        r.nextInt(),  r.nextString(), r.nextInt()))</div><div class="line">    <span class="keyword">val</span> q = <span class="string">sql""</span><span class="string">"SELECT * FROM appframe WHERE id = $appId"</span><span class="string">""</span>.as[<span class="type">AppFrame</span>]</div><div class="line"></div><div class="line">    <span class="keyword">val</span> db = <span class="type">Database</span>.forConfig(<span class="string">"mysql"</span>)</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="keyword">val</span> fu = db.run(q)</div><div class="line">      <span class="type">Await</span>.result(fu, <span class="number">10</span> seconds).head</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">      db.close()</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如上，就这么简单几行，就能连接数据库了，并且将其转化为对应的样例类，是不是超级好用<br>当然，我只是使用了一点皮毛，它还有很多有用的特性我没有使用到。</p>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p><a href="http://slick.lightbend.com/" target="_blank" rel="external">http://slick.lightbend.com/</a><br><a href="http://wiki.jikexueyuan.com/project/slick-guide/" target="_blank" rel="external">http://wiki.jikexueyuan.com/project/slick-guide/</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[记HashMap遇到的java.util.ConcurrentModificationException的bug]]></title>
      <url>http://flume.cn/2016/10/27/%E8%AE%B0HashMap%E9%81%87%E5%88%B0%E7%9A%84java-util-ConcurrentModificationException%E7%9A%84bug/</url>
      <content type="html"><![CDATA[<h4 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h4><p>spark Streaming 实时程序在联调期间稳定运行了两天，以为问题不大了，第二天早上的时候打开一看，竟然挂了，定位到代码，原来我的程序实时读取redis的数据为一个HashMap，直到挂的时候，Redis中数据一直在增大，共 6083条：</p>
<p>spark相关代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">// 1. 从L1中删除过期的号码，同时Redis中的对应该K-V也删除</span></div><div class="line"> ...</div><div class="line"><span class="keyword">val</span> sentinelPool = <span class="type">InternalRedisClient</span>.getSentinelPool</div><div class="line"><span class="keyword">var</span> phoneSet: util.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="keyword">new</span> util.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>]()</div><div class="line"><span class="comment">//            printLog.debug( "sentinelPool NumIdle0: " + sentinelPool.getNumIdle + " Active0: " + sentinelPool.getNumActive)</span></div><div class="line"></div><div class="line"><span class="keyword">var</span> jedis1: <span class="type">Jedis</span> = <span class="literal">null</span></div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">  jedis1 = sentinelPool.getResource</div><div class="line">  <span class="comment">//              printLog.debug( "sentinelPool NumIdle1: " + sentinelPool.getNumIdle + " Active1: " + sentinelPool.getNumActive)</span></div><div class="line"></div><div class="line">  phoneSet = jedis1.hgetAll(redisHashKey)</div><div class="line">  <span class="comment">//              printLog.debug( "sentinelPool NumIdle3: " + sentinelPool.getNumIdle + " Active3: " + sentinelPool.getNumActive)</span></div><div class="line"></div><div class="line">  printLog.debug(<span class="string">"phoneSet_1: "</span> + phoneSet)</div><div class="line">&#125; <span class="keyword">finally</span> &#123;</div><div class="line">  <span class="keyword">if</span> (jedis1 != <span class="literal">null</span>) &#123;</div><div class="line">    printLog.debug(<span class="string">"close jedis1"</span>)</div><div class="line">    jedis1.close()</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">if</span> (!phoneSet.isEmpty) &#123; </div><div class="line">  <span class="keyword">for</span> (eachPhoneKV: (<span class="type">String</span>, <span class="type">String</span>) &lt;- phoneSet) &#123; <span class="comment">// 就在这里挂掉了</span></div><div class="line">    <span class="keyword">val</span> expirationDate: <span class="type">Int</span> = eachPhoneKV._2.split(<span class="string">"\\|"</span>)(<span class="number">4</span>).toInt</div><div class="line">    <span class="keyword">val</span> today: <span class="type">Int</span> = getNowDate.toInt</div><div class="line">    <span class="keyword">if</span> (today &gt; expirationDate) &#123;</div><div class="line">      phoneSet.remove(eachPhoneKV._1)</div><div class="line">      <span class="keyword">var</span> jedis2: <span class="type">Jedis</span> = <span class="literal">null</span></div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        jedis2 = sentinelPool.getResource</div><div class="line">        jedis2.hdel(redisHashKey, eachPhoneKV._1)</div><div class="line">      &#125; <span class="keyword">finally</span> &#123;</div><div class="line">        <span class="keyword">if</span> (jedis2 != <span class="literal">null</span>) &#123;</div><div class="line">          printLog.debug(<span class="string">"close jedis2"</span>)</div><div class="line">          jedis2.close()</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  printLog.debug(<span class="string">"phoneSet_filtedByData: "</span> + phoneSet)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>具体异常粘信息如下：<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 0.0 failed 4 times, most recent failure: Lost task 4.3 in stage 0.0 (TID 170, NM-304-HW-XH628V3-BIGDATA-063): java.util.ConcurrentModificationException</div><div class="line">at java.util.HashMap$HashIterator.nextEntry(HashMap.java:922)</div><div class="line">at java.util.HashMap$EntryIterator.next(HashMap.java:962)</div><div class="line">at java.util.HashMap$EntryIterator.next(HashMap.java:960)</div><div class="line">at scala.collection.convert.Wrappers$JMapWrapperLike$$anon$2.next(Wrappers.scala:267)</div><div class="line">at scala.collection.convert.Wrappers$JMapWrapperLike$$anon$2.next(Wrappers.scala:264)</div><div class="line">at scala.collection.Iterator$class.foreach(Iterator.scala:727)</div><div class="line">at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)</div><div class="line">at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)</div><div class="line">at scala.collection.AbstractIterable.foreach(Iterable.scala:54)</div><div class="line">at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)</div><div class="line">at com.chinatelecom.bigdata.oidd2.Location$$anonfun$2$$anonfun$3.apply(Location.scala:871)</div><div class="line">at com.chinatelecom.bigdata.oidd2.Location$$anonfun$2$$anonfun$3.apply(Location.scala:677)</div></pre></td></tr></table></figure></p>
<h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p>项目太紧张，来不及详细分析java的源码了，根据经验redis中应该六千多条数据应该不是很大的，HashMap完全可以一次读取，从网上查到原因是因为remove操作导致的，在Iterator遍历过程中调用HashMap的remove方法会crash，有两个解决办法：</p>
<ol>
<li>一个解决办法是用一个ArrayList记录要删除的key,然后再遍历这个ArrayList,调用HashMap的remove方法以ArrayList的元素为key进行删除；这个方法需要额外的空间和时间，虽然也浪费的不多，但总感觉不够优雅；</li>
<li>创建一个Iterator<map.entry<integer, string="">&gt; iterator = map.entrySet().iterator();，然后用这个 iterator.remove()方法进行删除，这个是可以删除的；</map.entry<integer,></li>
</ol>
<p>我使用的是方法二，修改后的代码如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">var</span> phoneMap: util.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="keyword">new</span> util.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>]()</div><div class="line"><span class="comment">// printLog.debug( "sentinelPool NumIdle0: " + sentinelPool.getNumIdle + " Active0: " + sentinelPool.getNumActive)</span></div><div class="line"></div><div class="line"><span class="keyword">var</span> jedis1: <span class="type">Jedis</span> = <span class="literal">null</span></div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">  jedis1 = sentinelPool.getResource</div><div class="line">  <span class="comment">// printLog.debug( "sentinelPool NumIdle1: " + sentinelPool.getNumIdle + " Active1: " + sentinelPool.getNumActive)</span></div><div class="line"></div><div class="line">  phoneMap = jedis1.hgetAll(redisHashKey)</div><div class="line">  <span class="comment">// printLog.debug( "sentinelPool NumIdle3: " + sentinelPool.getNumIdle + " Active3: " + sentinelPool.getNumActive)</span></div><div class="line"></div><div class="line">  printLog.debug(<span class="string">"phoneSet_1: "</span> + phoneMap)</div><div class="line">&#125; <span class="keyword">finally</span> &#123;</div><div class="line">  <span class="keyword">if</span> (jedis1 != <span class="literal">null</span>) &#123;</div><div class="line">    printLog.debug(<span class="string">"close jedis1"</span>)</div><div class="line">    jedis1.close()</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 1. 从L1中删除过期的号码，同时Redis中的对应该K-V也删除</span></div><div class="line"><span class="keyword">if</span> (!phoneMap.isEmpty) &#123;</div><div class="line">  <span class="keyword">val</span> iterator: util.<span class="type">Iterator</span>[<span class="type">Entry</span>[<span class="type">String</span>, <span class="type">String</span>]] = phoneMap.entrySet().iterator()</div><div class="line">  <span class="keyword">while</span> (iterator.hasNext) &#123;</div><div class="line">    <span class="keyword">val</span> eachPhoneKV: <span class="type">Entry</span>[<span class="type">String</span>, <span class="type">String</span>] = iterator.next()</div><div class="line">    <span class="keyword">val</span> mdn = eachPhoneKV.getKey</div><div class="line">    <span class="keyword">val</span> redisValue = eachPhoneKV.getValue</div><div class="line"></div><div class="line">    <span class="keyword">var</span> expirationDate: <span class="type">Int</span> = <span class="number">0</span></div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      expirationDate = redisValue.split(<span class="string">"\\|"</span>)(<span class="number">4</span>).toInt</div><div class="line">      <span class="keyword">val</span> today: <span class="type">Int</span> = getNowDate.toInt</div><div class="line">      <span class="keyword">if</span> (today &gt; expirationDate) &#123;</div><div class="line">        printLog.info(<span class="string">"delete this data for expirationDate:"</span> + eachPhoneKV)</div><div class="line">        iterator.remove()</div><div class="line">        <span class="comment">// phoneMap.remove(mdn) // 这一句是错误的，因为无法据此删除</span></div><div class="line">        <span class="keyword">var</span> jedis2: <span class="type">Jedis</span> = <span class="literal">null</span></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">          jedis2 = sentinelPool.getResource</div><div class="line">          jedis2.hdel(redisHashKey, mdn)</div><div class="line">        &#125; <span class="keyword">finally</span> &#123;</div><div class="line">          <span class="keyword">if</span> (jedis2 != <span class="literal">null</span>) &#123;</div><div class="line">            printLog.debug(<span class="string">"close jedis2"</span>)</div><div class="line">            jedis2.close()</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="comment">// 如果解析发生异常，则redis中删掉这个key，并且在phoneMap中同时删除</span></div><div class="line">      <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; &#123;</div><div class="line">        printLog.error(<span class="string">"redis error data and del it: "</span> + eachPhoneKV + <span class="string">" error: "</span> + ex)</div><div class="line">        iterator.remove()</div><div class="line">        <span class="keyword">var</span> jedis4: <span class="type">Jedis</span> = <span class="literal">null</span></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">          jedis4 = sentinelPool.getResource</div><div class="line">          jedis4.hdel(redisHashKey, mdn)</div><div class="line">        &#125; <span class="keyword">finally</span> &#123;</div><div class="line">          <span class="keyword">if</span> (jedis4 != <span class="literal">null</span>) &#123;</div><div class="line">            printLog.debug(<span class="string">"close jedis4"</span>)</div><div class="line">            jedis4.close()</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>然后测试，打包，部署，OK，解决。</p>
<h4 id="原理说明"><a href="#原理说明" class="headerlink" title="原理说明"></a>原理说明</h4><p>遍历HashMap有三种方法，分别是:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span>(Map.Entry&lt;Integer, String&gt; entry : map.entrySet())&#123;&#125;  <span class="comment">// scala中为 &lt;-</span></div><div class="line"></div><div class="line"><span class="keyword">for</span>(Integer key : keySet)&#123;&#125;</div><div class="line"></div><div class="line">Iterator&lt;Map.Entry&lt;Integer, String&gt;&gt; it = map.entrySet().iterator();</div><div class="line">        <span class="keyword">while</span>(it.hasNext())&#123;&#125;</div></pre></td></tr></table></figure></p>
<p>其实上面的三种遍历方式从根本上讲都是使用的迭代器，之所以出现不同的结果是由于remove操作的实现不同决定的。</p>
<p>首先前两种方法都在调用nextEntry方法的同一个地方抛出了异常，虽然remove成功了，但是在迭代器遍历下一个元素的时候抛出异常：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">final</span> Entry&lt;K,V&gt; <span class="title">nextEntry</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (modCount != expectedModCount)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException();</div><div class="line">    Entry&lt;K,V&gt; e = next;</div><div class="line">    ...</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>这里modCount是表示map中的元素被修改了几次(在移除，新加元素时此值都会自增)，而expectedModCount是表示期望的修改次数，在迭代器构造的时候这两个值是相等，如果在遍历过程中这两个值出现了不同步就会抛出ConcurrentModificationException异常。</p>
<p>1、HashMap的remove方法实现<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> V <span class="title">remove</span><span class="params">(Object key)</span> </span>&#123;</div><div class="line">    Entry&lt;K,V&gt; e = removeEntryForKey(key);</div><div class="line">    <span class="keyword">return</span> (e == <span class="keyword">null</span> ? <span class="keyword">null</span> : e.value);</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>2、HashMap.KeySet的remove方法实现<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">remove</span><span class="params">(Object o)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> HashMap.<span class="keyword">this</span>.removeEntryForKey(o) != <span class="keyword">null</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>3、HashMap.HashIterator的remove方法实现<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">remove</span><span class="params">()</span> </span>&#123;</div><div class="line">   <span class="keyword">if</span> (current == <span class="keyword">null</span>)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException();</div><div class="line">   <span class="keyword">if</span> (modCount != expectedModCount)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException();</div><div class="line">   Object k = current.key;</div><div class="line">   current = <span class="keyword">null</span>;</div><div class="line">   HashMap.<span class="keyword">this</span>.removeEntryForKey(k);</div><div class="line">   expectedModCount = modCount;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>以上三种实现方式都通过调用HashMap.removeEntryForKey方法来实现删除key的操作。在removeEntryForKey方法内只要移除了key modCount就会执行一次自增操作，此时modCount就与expectedModCount不一致了，上面三种remove实现中，只有第三种iterator的remove方法在调用完removeEntryForKey方法后同步了expectedModCount值与modCount相同，所以在遍历下个元素调用nextEntry方法时，iterator方式不会抛异常。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">final</span> Entry&lt;K,V&gt; <span class="title">removeEntryForKey</span><span class="params">(Object key)</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> hash = (key == <span class="keyword">null</span>) ? <span class="number">0</span> : hash(key.hashCode());</div><div class="line">    <span class="keyword">int</span> i = indexFor(hash, table.length);</div><div class="line">    Entry&lt;K,V&gt; prev = table[i];</div><div class="line">    Entry&lt;K,V&gt; e = prev;</div><div class="line"></div><div class="line">    <span class="keyword">while</span> (e != <span class="keyword">null</span>) &#123;</div><div class="line">        Entry&lt;K,V&gt; next = e.next;</div><div class="line">        Object k;</div><div class="line">        <span class="keyword">if</span> (e.hash == hash &amp;&amp;</div><div class="line">            ((k = e.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k)))) &#123;</div><div class="line">            modCount++;</div><div class="line">            size--;</div><div class="line">            <span class="keyword">if</span> (prev == e)</div><div class="line">                table[i] = next;</div><div class="line">            <span class="keyword">else</span></div><div class="line">                prev.next = next;</div><div class="line">            e.recordRemoval(<span class="keyword">this</span>);</div><div class="line">            <span class="keyword">return</span> e;</div><div class="line">        &#125;</div><div class="line">        prev = e;</div><div class="line">        e = next;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">return</span> e;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="其它思考"><a href="#其它思考" class="headerlink" title="其它思考"></a>其它思考</h4><p>1、如果是遍历过程中增加或修改数据呢？<br>增加或修改数据只能通过Map的put方法实现，在遍历过程中修改数据可以，但如果增加新key就会在下次循环时抛异常，因为在添加新key时modCount也会自增。</p>
<p>2、有些集合类也有同样的遍历问题，如ArrayList，通过Iterator方式可正确遍历完成remove操作，直接调用list的remove方法就会抛异常。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//会抛ConcurrentModificationException异常</span></div><div class="line"><span class="keyword">for</span>(String str : list)&#123;</div><div class="line">	list.remove(str);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//正确遍历移除方式</span></div><div class="line">Iterator&lt;String&gt; it = list.iterator();</div><div class="line"><span class="keyword">while</span>(it.hasNext())&#123;</div><div class="line">	it.next();</div><div class="line">	it.remove();</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>3、jdk为什么这样设计，只允许通过iterator进行remove操作？</p>
<p>HashMap和keySet的remove方法都可以通过传递key参数删除任意的元素，而iterator只能删除当前元素(current);<br>对于通过HashMap的remove方法来说，一旦删除的元素是iterator对象中next所正在引用的，如果没有通过modCount与 expectedModCount的比较实现快速失败抛出异常，下次循环该元素将成为current指向，此时iterator就遍历了一个已移除的过期数据，所以一定要判断这两个值是否一致。</p>
<p>4、这是一个坑，如果IDE能提示就好了，下次注意</p>
<h4 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h4><p><a href="http://afredlyj.github.io/posts/hashmap-concurrentmodificationexception.html" target="_blank" rel="external">http://afredlyj.github.io/posts/hashmap-concurrentmodificationexception.html</a><br><a href="http://dumbee.net/archives/41" target="_blank" rel="external">http://dumbee.net/archives/41</a><br><a href="http://blog.csdn.net/wzy_1988/article/details/51423583" target="_blank" rel="external">http://blog.csdn.net/wzy_1988/article/details/51423583</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[scala中@的用法]]></title>
      <url>http://flume.cn/2016/09/23/scala%E4%B8%AD-%E7%9A%84%E7%94%A8%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>有些场景，比如模式匹配会遇到scala代码中有@符号，比如<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">case</span> x @ <span class="type">Some</span>(<span class="type">Nil</span>) =&gt; x</div></pre></td></tr></table></figure></p>
<p>现将网友的答案总结一下，并持续更新：</p>
<h3 id="绑定在模式匹配中，取出对应的原来输入值"><a href="#绑定在模式匹配中，取出对应的原来输入值" class="headerlink" title="绑定在模式匹配中，取出对应的原来输入值"></a>绑定在模式匹配中，取出对应的原来输入值</h3><p>比如：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> o: <span class="type">Option</span>[<span class="type">Int</span>] = <span class="type">Some</span>(<span class="number">5</span>)</div><div class="line"></div><div class="line"><span class="comment">// o: Option[Int] = Some(5)</span></div><div class="line"></div><div class="line">o <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> <span class="type">Some</span>(x) =&gt; println(x)</div><div class="line">  <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">&#125;</div><div class="line"><span class="comment">// 输出:  </span></div><div class="line"><span class="number">5</span></div><div class="line"></div><div class="line">o <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> x @ <span class="type">Some</span>(_) =&gt; println(x)</div><div class="line">  <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">&#125;</div><div class="line"><span class="comment">// 输出</span></div><div class="line"><span class="type">Some</span>(<span class="number">5</span>)</div></pre></td></tr></table></figure>
<p>如上案例，有些情况下，模式匹配后你并不想取出他的值，而是取出他本来的自己（Some(5)），这种情况下就用 @；并且@可以用于各个级别</p>
<h3 id="可以用来将名称和一个匹配的模式绑定，然后这个值作为匹配模式"><a href="#可以用来将名称和一个匹配的模式绑定，然后这个值作为匹配模式" class="headerlink" title="@可以用来将名称和一个匹配的模式绑定，然后这个值作为匹配模式"></a>@可以用来将名称和一个匹配的模式绑定，然后这个值作为匹配模式</h3><p>听起来很绕口，看如下代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> d@(c<span class="meta">@Some</span>(a), <span class="type">Some</span>(b)) = (<span class="type">Some</span>(<span class="number">1</span>), <span class="type">Some</span>(<span class="number">2</span>))</div></pre></td></tr></table></figure>
<p>结果竟然产生了四个值：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">d: (<span class="type">Some</span>[<span class="type">Int</span>], <span class="type">Some</span>[<span class="type">Int</span>]) = (<span class="type">Some</span>(<span class="number">1</span>),<span class="type">Some</span>(<span class="number">2</span>))</div><div class="line">c: <span class="type">Some</span>[<span class="type">Int</span>] = <span class="type">Some</span>(<span class="number">1</span>)</div><div class="line">a: <span class="type">Int</span> = <span class="number">1</span></div><div class="line">b: <span class="type">Int</span> = <span class="number">2</span></div></pre></td></tr></table></figure></p>
<p>如上所述，说明定义d和c是两个匹配模式，a和b是两个数字</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">(<span class="type">Some</span>(<span class="number">1</span>), <span class="type">Some</span>(<span class="number">2</span>)) <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> d@(c<span class="meta">@Some</span>(a), <span class="type">Some</span>(b)) =&gt; println(a, b, c, d)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>结果如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(<span class="number">1</span>,<span class="number">2</span>,<span class="type">Some</span>(<span class="number">1</span>),(<span class="type">Some</span>(<span class="number">1</span>),<span class="type">Some</span>(<span class="number">2</span>)))</div></pre></td></tr></table></figure></p>
<p>再跑一个例子<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">for</span> (t<span class="meta">@Some</span>(u) &lt;- <span class="type">Seq</span>(<span class="type">Some</span>(<span class="number">1</span>))) println(t, u)</div><div class="line">(<span class="type">Some</span>(<span class="number">1</span>),<span class="number">1</span>)</div></pre></td></tr></table></figure></p>
<p>再跑一个例子</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> <span class="type">List</span>(x, xs @ _*) = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</div><div class="line">x: <span class="type">Int</span> = <span class="number">1</span></div><div class="line">xs: <span class="type">Seq</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">2</span>, <span class="number">3</span>)</div></pre></td></tr></table></figure>
<h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><p><a href="http://stackoverflow.com/questions/2359014/scala-operator" target="_blank" rel="external">http://stackoverflow.com/questions/2359014/scala-operator</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark在Kerberos下连接使用Hbase的配置]]></title>
      <url>http://flume.cn/2016/09/18/Spark%E5%9C%A8Kerberos%E4%B8%8B%E8%BF%9E%E6%8E%A5%E4%BD%BF%E7%94%A8Hbase%E7%9A%84%E9%85%8D%E7%BD%AE/</url>
      <content type="html"><![CDATA[<p>复制HBase目录下的lib文件到spark目录/lib/hbase。spark依赖此lib，但直接指定到Hbase下的lib目录的话又会出错<br>清单如下：guava-12.0.1.jar htrace-core-3.1.0-incubating.jar protobuf-java-2.5.0.jar   这三个jar加上以hbase开头所有jar，其它就不必了，全部复制会引起报错。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> <span class="variable">$SPARK_HOME</span>/lib/hbase</div><div class="line">cp /usr/lib/hbase/lib/hbase-* ./</div><div class="line">cp /usr/lib/hbase/lib/guava-12.0.1.jar ./</div><div class="line">cp /usr/lib/hbase/lib/htrace-core-3.1.0-incubating.jar ./</div><div class="line">cp /usr/lib/hbase/lib/protobuf-java-2.5.0.jar ./</div></pre></td></tr></table></figure></p>
<p>然后在spark客户端配置如下：<br>也就是增加到classpath<br><figure class="highlight bash"><figcaption><span>spark-default.conf</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">spark.driver.extraClassPath /usr/lib/hadoop/lib/*:/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/lib/hbase/*</div></pre></td></tr></table></figure></p>
<p>就可以了</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark在yarn中的资源申请与分配调研]]></title>
      <url>http://flume.cn/2016/09/18/Spark%E5%9C%A8yarn%E4%B8%AD%E7%9A%84%E8%B5%84%E6%BA%90%E7%94%B3%E8%AF%B7%E4%B8%8E%E5%88%86%E9%85%8D/</url>
      <content type="html"><![CDATA[<p>本文解决遇到的以下问题：<br><em>spark作业提交到yarn的时候，如果用户(wzfw)所在队列本来有500个executor的权限，但是他跑一个简单的程序根本不需要这么多的资源，只需要200个核就足够了，那他如果申请了400个核的话，是否需要全部分配给他？</em></p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>目前我们所有的spark程序的分配都是靠参数设置固定的Executor数量进行资源预分配的，如果用户op在yarn的资源队列里可以申请到200个资源，那它就算跑占用资源很少的程序也能申请到200个核，这是不合理的</p>
<p>比如简单跑如下SparkPi程序，申请20个核：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --num-executors 20 lib/spark-examples-1.6.2-hadoop2.6.0.jar</div></pre></td></tr></table></figure></p>
<p>yarn中资源占用情况如下：<br><img src="/2016/09/18/Spark在yarn中的资源申请与分配/spark-yarn-allocation.png" alt="spark-yarn-allocation.png" title=""><br>可以看到，我就跑了一个SparkPi啊，竟然用了43G的内存，这样很不合理！</p>
<h2 id="总述"><a href="#总述" class="headerlink" title="总述"></a>总述</h2><p>Spark在yarn集群上运行的时候，一方面默认通过num-executors参数设置固定的Executor数量，每个application会独占所有预分配的资源直到整个生命周期的结束。Spark1.2后开始引入动态资源分配（Dynamic Resource Allocation）机制，支持资源弹性分配。</p>
<p>对于已知的业务负载，使用固定的集群资源配置是相对容易的；对于未知的业务负载，使用动态的集群资源分配方式可以满足负载的动态变化，这样集群的资源利用和业务负载的处理效率都会更加灵活。</p>
<p>动态资源分配测试在Spark1.2仅支持Yarn模式，从Spark1.6开始，支持standalone、Yarn、Mesos.这个特性默认是禁用的。<br><a id="more"></a></p>
<h2 id="动态资源分配的思想"><a href="#动态资源分配的思想" class="headerlink" title="动态资源分配的思想"></a>动态资源分配的思想</h2><p>简单来说，就是基于负载来动态调节Spark应用的资源占用，你的应用会在资源空闲的时候将其释放给集群，而后续用到的时候再重新申请。</p>
<h3 id="动态资源分配策略"><a href="#动态资源分配策略" class="headerlink" title="动态资源分配策略"></a>动态资源分配策略</h3><p>其实没有一个固定的方法可以预测一个executor后续是否马上会被分配去执行任务，或者一个新分配的执行器实际上是空闲的，所以我们需要一些试探性的方法，来决定是否申请或移除一个执行器。策略分为<strong>请求策略</strong>与<strong>移除策略</strong>：</p>
<h4 id="请求策略"><a href="#请求策略" class="headerlink" title="请求策略"></a>请求策略</h4><p>开启动态分配策略后，application会在task因没有足够资源被挂起的时候去动态申请资源，这种情况意味着该application现有的executor无法满足所有task并行运行。spark一轮一轮的申请资源，当有task挂起或等待spark.dynamicAllocation.schedulerBacklogTimeout(默认1s)时间的时候，会开始动态资源分配；之后会每隔spark.dynamicAllocation.sustainedSchedulerBacklogTimeout(默认1s)时间申请一次，直到申请到足够的资源。<strong>每次申请的资源量是指数增长的，即1,2,4,8等</strong>。<br>之所以采用指数增长，出于两方面考虑：其一，开始申请的少是考虑到可能application会马上得到满足；其次要成倍增加，是为了如果application需要很多资源，而该方式可以在很少次数的申请之后得到满足。<br>（这段指数增长的策略可以根据实际情况通过修改源码来修改）</p>
<h4 id="资源回收策略"><a href="#资源回收策略" class="headerlink" title="资源回收策略"></a>资源回收策略</h4><p>当application的executor空闲时间超过spark.dynamicAllocation.executorIdleTimeout（默认60s）后，就会被回收。</p>
<h2 id="配置思路"><a href="#配置思路" class="headerlink" title="配置思路"></a>配置思路</h2><h3 id="启动-external-shuffle-service"><a href="#启动-external-shuffle-service" class="headerlink" title="启动 external shuffle service"></a>启动 external shuffle service</h3><p>要使用这一特性有两个前提条件。首先，你的应用必须设置 spark.dynamicAllocation.enabled 为 true。其次，你必须在每个节点上启动一个外部混洗服务（external shuffle service），并在你的应用中将 spark.shuffle.service.enabled 设为true。外部混洗服务的目的就是为了在删除执行器的时候，能够保留其输出的混洗文件（本文后续有更详细的描述）。启用外部混洗的方式在各个集群管理器上各不相同：</p>
<p>在Spark独立部署的集群中，你只需要在worker启动前设置 spark.shuffle.server.enabled 为true即可。</p>
<p>在YARN模式下，混洗服务需要按以下步骤在各个NodeManager上启动：</p>
<ol>
<li>首先按照YARN profile 构建Spark。如果你已经有打好包的Spark，可以忽略这一步。</li>
<li>找到 spark-<version>-yarn-shuffle.jar。如果你是自定义编译，其位置应该在 ${SPARK_HOME}/network/yarn/target/scala-<version>，否则应该可以在 lib 目录下找到这个jar包。</version></version></li>
<li>将该jar包添加到NodeManager的classpath路径中。</li>
<li>配置各个节点上的yarn-site.xml，将 spark_shuffle 添加到 yarn.nodemanager.aux-services 中，然后将 yarn.nodemanager.aux-services.spark_shuffle.class 设为 org.apache.spark.network.yarn.YarnShuffleService，并将 spark.shuffle.service.enabled 设为 true。</li>
<li>最后重启各节点上的NodeManager。</li>
</ol>
<p>所有相关的配置都是可选的，并且都在 spark.dynamicAllocation.<em> 和 spark.shuffle.service.</em> 命名空间下。更详细请参考：<a href="http://spark.apache.org/docs/latest/configuration.html#dynamic-allocation" target="_blank" rel="external">configurations page</a>。</p>
<h3 id="外部混洗服务external-shuffle-service"><a href="#外部混洗服务external-shuffle-service" class="headerlink" title="外部混洗服务external shuffle service"></a>外部混洗服务external shuffle service</h3><p>非动态分配模式下，执行器可能的退出原因有执行失败或者相关Spark应用已经退出。不管是哪种原因，执行器的所有状态都已经不再需要，可以丢弃掉。但在动态分配的情形下，执行器有可能在Spark应用运行期间被移除。这时候，如果Spark应用尝试去访问该执行器存储的状态，就必须重算这一部分数据。因此，Spark需要一种机制，能够优雅地关闭执行器，同时还保留其状态数据。</p>
<p>这种需求对于混洗操作尤其重要。混洗过程中，Spark执行器首先将map输出写到本地磁盘，同时执行器本身又是一个文件服务器，这样其他执行器就能够通过该执行器获得对应的map结果数据。一旦有某些任务执行时间过长，动态分配有可能在混洗结束前移除任务异常的执行器，而这些被移除的执行器对应的数据将会被重新计算，但这些重算其实是不必要的。</p>
<p>要解决这一问题，就需要用到一个外部混洗服务（external shuffle service），该服务在Spark 1.2引入。该服务在每个节点上都会启动一个不依赖于任何Spark应用或执行器的独立进程。一旦该服务启用，Spark执行器不再从各个执行器上获取shuffle文件，转而从这个service获取。这意味着，任何执行器输出的混洗状态数据都可能存留时间比对应的执行器进程还长。</p>
<p>除了混洗文件之外，执行器也会在磁盘或者内存中缓存数。一旦执行器被移除，其缓存数据将无法访问。这个问题目前还没有解决。或许在未来的版本中，可能会采用外部混洗服务类似的方法，将缓存数据保存在堆外存储中以解决这一问题。</p>
<h2 id="配置说明"><a href="#配置说明" class="headerlink" title="配置说明"></a>配置说明</h2><p>配置文件：<br>$SPARK_HOME/conf/spark-defaults.conf<br>$HADOOP_HOME/conf/yarn-site.xml</p>
<h3 id="Spark配置说明"><a href="#Spark配置说明" class="headerlink" title="Spark配置说明"></a>Spark配置说明</h3><p>在spark-defaults.conf 中添加<br><figure class="highlight bash"><figcaption><span>spark-defaults.conf</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">spark.shuffle.service.enabled <span class="literal">true</span>   <span class="comment"># 开启外部shuffle服务，开启这个服务可以保护executor的shuffle文件，安全移除executor，在Yarn模式下这个shuffle服务以org.apache.spark.yarn.network.YarnShuffleService实现</span></div><div class="line">spark.shuffle.service.port 7337 <span class="comment"># Shuffle Service服务端口，必须和yarn-site中的一致</span></div><div class="line">spark.dynamicAllocation.enabled <span class="literal">true</span>  <span class="comment"># 开启动态资源分配</span></div><div class="line">spark.dynamicAllocation.minExecutors 1  <span class="comment"># 每个Application最小分配的executor数</span></div><div class="line">spark.dynamicAllocation.maxExecutors 30  <span class="comment"># 每个Application最大并发分配的executor数</span></div><div class="line">spark.dynamicAllocation.schedulerBacklogTimeout 1s <span class="comment"># 任务待时间（超时便申请新资源)默认60秒</span></div><div class="line">spark.dynamicAllocation.sustainedSchedulerBacklogTimeout 5s <span class="comment">#  再次请求等待时间，默认60秒</span></div><div class="line">spark.dynamicAllocation.executorIdleTimeout <span class="comment"># executor闲置时间（超过释放资源）默认600秒</span></div></pre></td></tr></table></figure></p>
<h3 id="yarn的配置"><a href="#yarn的配置" class="headerlink" title="yarn的配置"></a>yarn的配置</h3><h4 id="添加相应的jar包spark-yarn-shuffle-jar"><a href="#添加相应的jar包spark-yarn-shuffle-jar" class="headerlink" title="添加相应的jar包spark--yarn-shuffle.jar"></a>添加相应的jar包spark-<version>-yarn-shuffle.jar</version></h4><p>如果是自己编译的spark，可以在$SPARK_HOME/network/yarn/target/scala-<version>下面找到<br>是预编译的，直接在$SPARK_HOME/lib/下面找到<br>找到jar包后，将其添加到每个nodemanager的classpath下面(或者直接放到yarn的lib目录中,${HADOOP_HOME}/share/hadoop/yarn/lib/)</version></p>
<h4 id="配置yarn-site-xml文件"><a href="#配置yarn-site-xml文件" class="headerlink" title="配置yarn-site.xml文件"></a>配置yarn-site.xml文件</h4><p>在所有节点的yarn-site.xml中，为yarn.nodemanager.aux-services配置项新增spark_shuffle这个值（注意是新增，在原有value的基础上逗号分隔新增即可）<br><figure class="highlight xml"><figcaption><span>yarn-site.xml</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>spark.shuffle.service.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle,spark_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.spark_shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.spark.network.yarn.YarnShuffleService<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure></p>
<h4 id="重启所有的节点"><a href="#重启所有的节点" class="headerlink" title="重启所有的节点"></a>重启所有的节点</h4><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>当开启了动态资源分配（spark.dynamicAllocation.enabled），num-executor选项将不再兼容，如果设置了num-executor，那么动态资源分配将被关闭</p>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p><a href="http://spark.apache.org/docs/1.6.2/job-scheduling.html#dynamic-resource-allocation" target="_blank" rel="external">spark1.6.2作业调度官网</a><br><a href="http://ifeve.com/spark-schedule/" target="_blank" rel="external">spark1.6.2作业调度翻译版</a><br><a href="http://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/" target="_blank" rel="external">Apache Spark Resource Management and YARN App Models</a><br><a href="https://issues.apache.org/jira/browse/YARN-1197" target="_blank" rel="external">jira/browse/YARN-1197–Support changing resources of an allocated container</a><br><a href="http://hejunhao.me/archives/675" target="_blank" rel="external">Spark集群资源动态分配</a><br><a href="http://blog.sina.com.cn/s/blog_a29dec8d0102vfwx.html" target="_blank" rel="external">spark动态资源分配在yarn（hadoop）的配置</a><br><a href="https://www.linkedin.com/pulse/spark-executors%E5%9C%A8yarn%E4%B8%8A%E7%9A%84%E5%8A%A8%E6%80%81%E5%88%86%E9%85%8D-victor-wang" target="_blank" rel="external">Spark Executors在YARN上的动态分配</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[livy-server初探1——简介与提交脚本以及LivyServer类]]></title>
      <url>http://flume.cn/2016/09/13/livy-server%E5%88%9D%E6%8E%A21%E2%80%94%E2%80%94%E7%AE%80%E4%BB%8B%E4%B8%8E%E6%8F%90%E4%BA%A4%E8%84%9A%E6%9C%AC%E4%BB%A5%E5%8F%8ALivyServer%E7%B1%BB/</url>
      <content type="html"><![CDATA[<p><a href="http://livy.io/" target="_blank" rel="external">Livy server</a>是针对Spark的开源的REST接口，使得我们可以通过REST接口来实现与Spark交互,之前应该是Hue框架的一个功能模块，现在已经独立出来啦。具有如下功能：<br>1） 可以与scala、python、R shell客户端交互，执行一些代码片段<br>2） 可以提交整个Spark Job,支持scala、python、java编写的Spark job。</p>
<h2 id="Welcome-to-Livy"><a href="#Welcome-to-Livy" class="headerlink" title="Welcome to Livy"></a>Welcome to Livy</h2><p>下面是官网文档中我对 Welcome to Livy的翻译：</p>
<p>Livy通过提供REST服务来简化与Spark集群的交互。它可以通过job或者代码片段的方式来提交Spark任务，并同步或者异步地获得任务的结果，以及管理spark context，上述功能通过简单的REST接口或者RPC服务来实现。livy也可以简化Spark与一些应用程序之间的交互，使得Spark可以用于一些web应用(比如Hue)。更多的功能包括：</p>
<ul>
<li>拥有长期运行的Spark Contexts供多用户提交各种的Spark job；</li>
<li>不同的任务和用户可以共享cached RDD或者DataFrames；</li>
<li>多个SC可以按计划同时运行，为了使得SC具有更好的容错性和并发性，可以将SC运行在yarn/Mesos等集群中；</li>
<li>可以通过java/scala客户端的API来提交预编译好的jar包或代码片段</li>
<li>支持一定的安全机制</li>
<li>Apache-licensed 100%开源</li>
</ul>
<p>与ReadMe中的文档结合再补充几条：</p>
<ul>
<li>支持Scala，Python，R Shell的交互；</li>
<li>支持 Scala，Java，Python的批量提交；</li>
<li>不需要你对你自己的代码增加任何改变；</li>
</ul>
<p>官网和github逛了一整子后不禁感叹，新东西总是缺乏底层的文档的，所以要了解它就要阅读源码了。</p>
<h2 id="从-bin-livy-server进入"><a href="#从-bin-livy-server进入" class="headerlink" title="从./bin/livy-server进入"></a>从./bin/livy-server进入</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">usage=<span class="string">"Usage: livy-server (start|stop)"</span></div><div class="line"></div><div class="line"><span class="comment"># 指定LIVY_HOME与LIVY_CONF_DIR，上述 `export LIVY_HOME=$(cd $(dirname $0)/.. &amp;&amp; pwd)`这种写法值得学习，代表将LIVY_HOME环境变量设为本脚本的父目录，通过这种写法，增强了脚本的可移植性，另外注明一点，dirname这个命令在命令行里是不能用的，只有写在脚本中才能起作用。</span></div><div class="line"><span class="built_in">export</span> LIVY_HOME=$(<span class="built_in">cd</span> $(dirname <span class="variable">$0</span>)/.. &amp;&amp; <span class="built_in">pwd</span>)</div><div class="line">LIVY_CONF_DIR=<span class="variable">$&#123;LIVY_CONF_DIR:-"$LIVY_HOME/conf"&#125;</span></div><div class="line"></div><div class="line"><span class="comment"># 运行所有的livy-env.sh中的环境变量，并使用set -a 表示输出所有的环境变量的改变</span></div><div class="line"><span class="keyword">if</span> [ <span class="_">-f</span> <span class="string">"<span class="variable">$&#123;LIVY_CONF_DIR&#125;</span>/livy-env.sh"</span> ]; <span class="keyword">then</span></div><div class="line">  <span class="comment"># Promote all variable declarations to environment (exported) variables</span></div><div class="line">  <span class="built_in">set</span> <span class="_">-a</span></div><div class="line">  . <span class="string">"<span class="variable">$&#123;LIVY_CONF_DIR&#125;</span>/livy-env.sh"</span></div><div class="line">  <span class="built_in">set</span> +a</div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure>
<p>接下来可以看到调用了 start_livy_server，以及stop的代码(其实就是ps -p到livy的那个进程，然后kill掉，值得借鉴)：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">option=<span class="variable">$1</span></div><div class="line"></div><div class="line"><span class="keyword">case</span> <span class="variable">$option</span> <span class="keyword">in</span></div><div class="line"></div><div class="line">  (start)</div><div class="line">    start_livy_server <span class="string">"new"</span></div><div class="line">    ;;</div><div class="line"></div><div class="line">  (<span class="string">""</span>)</div><div class="line">    <span class="comment"># make it compatible with previous version of livy-server</span></div><div class="line">    start_livy_server <span class="string">"old"</span></div><div class="line">    ;;</div><div class="line"></div><div class="line">  (stop)</div><div class="line">    <span class="keyword">if</span> [ <span class="_">-f</span> <span class="variable">$pid</span> ]; <span class="keyword">then</span></div><div class="line">      TARGET_ID=<span class="string">"<span class="variable">$(cat "$pid")</span>"</span></div><div class="line">      <span class="keyword">if</span> [[ $(ps -p <span class="string">"<span class="variable">$TARGET_ID</span>"</span> -o comm=) =~ <span class="string">"java"</span> ]]; <span class="keyword">then</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"stopping livy_server"</span></div><div class="line">        <span class="built_in">kill</span> <span class="string">"<span class="variable">$TARGET_ID</span>"</span> &amp;&amp; rm <span class="_">-f</span> <span class="string">"<span class="variable">$pid</span>"</span></div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"no livy_server to stop"</span></div><div class="line">      <span class="keyword">fi</span></div><div class="line">    <span class="keyword">else</span></div><div class="line">      <span class="built_in">echo</span> <span class="string">"no livy_server to stop"</span></div><div class="line">    <span class="keyword">fi</span></div><div class="line">    ;;</div><div class="line"></div><div class="line">  (*)</div><div class="line">    <span class="built_in">echo</span> <span class="variable">$usage</span></div><div class="line">    <span class="built_in">exit</span> 1</div><div class="line">    ;;</div><div class="line"></div><div class="line"><span class="keyword">esac</span></div></pre></td></tr></table></figure>
<p>接下来就是start_livy_server函数了，它做了下面几件事情：</p>
<ol>
<li>找到livy的jar包；</li>
<li>设置LIVY_CLASSPATH并将SPARK与HADOOP以及YARN的CONF_DIR加入到classpath中；</li>
<li>如果是<code>./bin/livy-server</code>启动的程序，就直接运行 “$RUNNER $LIVY_SERVER_JAVA_OPTS -cp $LIVY_CLASSPATH:$CLASSPATH com.cloudera.livy.server.LivyServer”</li>
<li>如果是<code>./bin/livy-server start</code>启动的程序，则增加了日志记录，以方便查看，所以推荐新版本使用带start参数的方式</li>
</ol>
<h2 id="com-cloudera-livy-server-LivyServer"><a href="#com-cloudera-livy-server-LivyServer" class="headerlink" title="com.cloudera.livy.server.LivyServer"></a>com.cloudera.livy.server.LivyServer</h2><p>从后面进入：原来是创建了一个 LivyServer的server，然后start和join启动</p>
<figure class="highlight scala"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">LivyServer</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">val</span> server = <span class="keyword">new</span> <span class="type">LivyServer</span>()</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      server.start()</div><div class="line">      server.join()</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">      server.stop()</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="LiveServer的属性"><a href="#LiveServer的属性" class="headerlink" title="LiveServer的属性"></a>LiveServer的属性</h3><p>LivyServer的属性不多，（与spark源码相比）：</p>
<figure class="highlight scala"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> <span class="type">LivyConf</span>._</div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">var</span> server: <span class="type">WebServer</span> = _</div><div class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _serverUrl: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></div><div class="line">  <span class="comment">// make livyConf accessible for testing</span></div><div class="line">  <span class="keyword">private</span>[livy] <span class="keyword">var</span> livyConf: <span class="type">LivyConf</span> = _</div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">var</span> kinitFailCount: <span class="type">Int</span> = <span class="number">0</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">var</span> executor: <span class="type">ScheduledExecutorService</span> = _</div></pre></td></tr></table></figure>
<h3 id="start-函数"><a href="#start-函数" class="headerlink" title="start()函数"></a>start()函数</h3><p>然后是start()函数了<br>首先，从配置文件中读取配置信息（这一块内容自己写得时候可以借用）：</p>
<ul>
<li>从配置信息中得到host和port</li>
</ul>
<figure class="highlight scala"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">livyConf = <span class="keyword">new</span> <span class="type">LivyConf</span>().loadFromFile(<span class="string">"livy.conf"</span>)</div><div class="line"><span class="keyword">val</span> host = livyConf.get(<span class="type">SERVER_HOST</span>)</div><div class="line"><span class="keyword">val</span> port = livyConf.getInt(<span class="type">SERVER_PORT</span>)</div><div class="line"># 这个而没有看懂</div><div class="line"><span class="keyword">val</span> multipartConfig = <span class="type">MultipartConfig</span>(</div><div class="line">    maxFileSize = <span class="type">Some</span>(livyConf.getLong(<span class="type">LivyConf</span>.<span class="type">FILE_UPLOAD_MAX_SIZE</span>))</div><div class="line">  ).toMultipartConfigElement</div></pre></td></tr></table></figure>
<ul>
<li>测试SparkHome是否设置成功</li>
</ul>
<p>如下代码，这里使用了require方法对参数进行先决条件检测(值得借鉴)<br><figure class="highlight scala"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Make sure the `spark-submit` program exists, otherwise much of livy won't work.</span></div><div class="line">testSparkHome(livyConf)</div><div class="line">...</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">* Sets the spark-submit path if it's not configured in the LivyConf</div><div class="line">*/</div><div class="line"><span class="keyword">private</span>[server] <span class="function"><span class="keyword">def</span> <span class="title">testSparkHome</span></span>(livyConf: <span class="type">LivyConf</span>): <span class="type">Unit</span> = &#123;</div><div class="line"><span class="keyword">val</span> sparkHome = livyConf.sparkHome().getOrElse &#123;</div><div class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"Livy requires the SPARK_HOME environment variable"</span>)</div><div class="line">&#125;</div><div class="line"></div><div class="line">require(<span class="keyword">new</span> <span class="type">File</span>(sparkHome).isDirectory(), <span class="string">"SPARK_HOME path does not exist"</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<ul>
<li>测试spark-submit命令可用，否则livy无法工作(值得借鉴):</li>
</ul>
<p>这里的代码写得太精彩了！先是定义一个<code>$SPAKR_HOME/bin/spark-sumbit --version</code>的命令，使用java的ProcessBuilder，然后可以得到exitCode和重定向的标准输出结果，如果结果是”version …”的话，就代表执行成功，输出结果；然后对这个version进行正则匹配，如果是1.6到2.0版本之间，就返回true，否则就说明spark版本不支持；</p>
<figure class="highlight scala"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line">testSparkSubmit(livyConf)</div><div class="line">...</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">* Test that the configured `spark-submit` executable exists.</div><div class="line">*</div><div class="line">* @param livyConf</div><div class="line">*/</div><div class="line"><span class="keyword">private</span>[server] <span class="function"><span class="keyword">def</span> <span class="title">testSparkSubmit</span></span>(livyConf: <span class="type">LivyConf</span>): <span class="type">Unit</span> = &#123;</div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">  testSparkVersion(sparkSubmitVersion(livyConf))</div><div class="line">&#125; <span class="keyword">catch</span> &#123;</div><div class="line">  <span class="keyword">case</span> e: <span class="type">IOException</span> =&gt;</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">"Failed to run spark-submit executable"</span>, e)</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">...</div><div class="line"><span class="comment">/**</span></div><div class="line">* Return the version of the configured `spark-submit` version.</div><div class="line">*</div><div class="line">* @param livyConf</div><div class="line">* @return the version</div><div class="line">*/</div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">sparkSubmitVersion</span></span>(livyConf: <span class="type">LivyConf</span>): <span class="type">String</span> = &#123;</div><div class="line"><span class="keyword">val</span> sparkSubmit = livyConf.sparkSubmit()</div><div class="line"><span class="keyword">val</span> pb = <span class="keyword">new</span> <span class="type">ProcessBuilder</span>(sparkSubmit, <span class="string">"--version"</span>)</div><div class="line">pb.redirectErrorStream(<span class="literal">true</span>)</div><div class="line">pb.redirectInput(<span class="type">ProcessBuilder</span>.<span class="type">Redirect</span>.<span class="type">PIPE</span>)</div><div class="line"></div><div class="line"><span class="keyword">if</span> (<span class="type">LivyConf</span>.<span class="type">TEST_MODE</span>) &#123;</div><div class="line">  pb.environment().put(<span class="string">"LIVY_TEST_CLASSPATH"</span>, sys.props(<span class="string">"java.class.path"</span>))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">val</span> process = <span class="keyword">new</span> <span class="type">LineBufferedProcess</span>(pb.start())</div><div class="line"><span class="keyword">val</span> exitCode = process.waitFor()</div><div class="line"><span class="keyword">val</span> output = process.inputIterator.mkString(<span class="string">"\n"</span>)</div><div class="line"></div><div class="line"><span class="keyword">val</span> regex = <span class="string">""</span><span class="string">"version (.*)"</span><span class="string">""</span>.r.unanchored</div><div class="line"></div><div class="line">output <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> regex(version) =&gt; version</div><div class="line">  <span class="keyword">case</span> _ =&gt;</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">f"Unable to determine spark-submit version [<span class="subst">$exitCode</span>]:\n<span class="subst">$output</span>"</span>)</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">* Throw an exception if Spark version is not supported.</div><div class="line">* @param version Spark version</div><div class="line">*/</div><div class="line"><span class="keyword">private</span>[server] <span class="function"><span class="keyword">def</span> <span class="title">testSparkVersion</span></span>(version: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</div><div class="line"><span class="keyword">val</span> versionPattern = <span class="string">""</span><span class="string">"(\d)+\.(\d)+(?:\.\d*)?"</span><span class="string">""</span>.r</div><div class="line"><span class="comment">// This is exclusive. Version which equals to this will be rejected.</span></div><div class="line"><span class="keyword">val</span> maxVersion = (<span class="number">2</span>, <span class="number">0</span>)</div><div class="line"><span class="keyword">val</span> minVersion = (<span class="number">1</span>, <span class="number">6</span>)</div><div class="line"></div><div class="line"><span class="keyword">val</span> supportedVersion = version <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> versionPattern(major, minor) =&gt;</div><div class="line">    <span class="keyword">val</span> v = (major.toInt, minor.toInt)</div><div class="line">    v &gt;= minVersion &amp;&amp; v &lt; maxVersion</div><div class="line">  <span class="keyword">case</span> _ =&gt; <span class="literal">false</span></div><div class="line">&#125;</div><div class="line">require(supportedVersion, <span class="string">s"Unsupported Spark version <span class="subst">$version</span>."</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>通过livy.spark.master是否以yarn开头判断是否需要初始化YarnClient</li>
</ul>
<figure class="highlight scala"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Initialize YarnClient ASAP to save time.</span></div><div class="line"><span class="keyword">if</span> (livyConf.isRunningOnYarn()) &#123;</div><div class="line">  <span class="type">Future</span> &#123; <span class="type">SparkYarnApp</span>.yarnClient &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// SparkYarnApp.scala</span></div><div class="line"><span class="comment">// YarnClient is thread safe. Create once, share it across threads.</span></div><div class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> yarnClient = &#123;</div><div class="line">  <span class="keyword">val</span> c = <span class="type">YarnClient</span>.createYarnClient() <span class="comment">// 这里调用的是yarn提供的API</span></div><div class="line">  c.init(<span class="keyword">new</span> <span class="type">YarnConfiguration</span>())</div><div class="line">  c.start()</div><div class="line">  c</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>接下来启动WebServer</li>
</ul>
<p>这个webServer是Jetty的WebServer，通过设置是否使用ssl的配置来判断启动的是http server还是 https server，也会判断有没有Kerberos，设置IP，端口，日志等。（以后写得时候得查看Jetty的API和文档）</p>
<figure class="highlight scala"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">server = <span class="keyword">new</span> <span class="type">WebServer</span>(livyConf, host, port)</div><div class="line">server.context.setResourceBase(<span class="string">"src/main/com/cloudera/livy/server"</span>)</div><div class="line">server.context.addEventListener(</div><div class="line">  <span class="keyword">new</span> <span class="type">ServletContextListener</span>() <span class="keyword">with</span> <span class="type">MetricsBootstrap</span> <span class="keyword">with</span> <span class="type">ServletApiImplicits</span> &#123;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">mount</span></span>(sc: <span class="type">ServletContext</span>, servlet: <span class="type">Servlet</span>, mappings: <span class="type">String</span>*): <span class="type">Unit</span> = &#123;</div><div class="line">      <span class="keyword">val</span> registration = sc.addServlet(servlet.getClass().getName(), servlet)</div><div class="line">      registration.addMapping(mappings: _*)</div><div class="line">      registration.setMultipartConfig(multipartConfig)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">contextDestroyed</span></span>(sce: <span class="type">ServletContextEvent</span>): <span class="type">Unit</span> = &#123;</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">contextInitialized</span></span>(sce: <span class="type">ServletContextEvent</span>): <span class="type">Unit</span> = &#123;</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        <span class="keyword">val</span> context = sce.getServletContext()</div><div class="line">        context.initParameters(org.scalatra.<span class="type">EnvironmentKey</span>) = livyConf.get(<span class="type">ENVIRONMENT</span>)</div><div class="line">        mount(context, <span class="keyword">new</span> <span class="type">InteractiveSessionServlet</span>(livyConf), <span class="string">"/sessions/*"</span>)</div><div class="line">        mount(context, <span class="keyword">new</span> <span class="type">BatchSessionServlet</span>(livyConf), <span class="string">"/batches/*"</span>)</div><div class="line">        context.mountMetricsAdminServlet(<span class="string">"/"</span>)</div><div class="line">      &#125; <span class="keyword">catch</span> &#123;</div><div class="line">        <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">          error(<span class="string">"Exception thrown when initializing server"</span>, e)</div><div class="line">          sys.exit(<span class="number">1</span>)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">  &#125;)</div></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark在Kerberos环境下指定任意用户在yarn上提交任务]]></title>
      <url>http://flume.cn/2016/09/08/Spark%E5%9C%A8Kerberos%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%8C%87%E5%AE%9A%E4%BB%BB%E6%84%8F%E7%94%A8%E6%88%B7%E5%9C%A8yarn%E4%B8%8A%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1/</url>
      <content type="html"><![CDATA[<p>众所周知，Spark在Kerberos环境下提交任务有两种方式，分别是先kinit的方式和通过 –keytab的方式：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[op]$ spark-submit --keytab <span class="built_in">test</span>Jars/op.keytab --principal op --master <span class="built_in">local</span> --class SparkPi ./<span class="built_in">test</span>Jars/my.jar 4</div></pre></td></tr></table></figure>
<p>Spark在Kerberos环境下可以在提交任务时通过指定用户的keytab和principal来提交任务，比如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 事先进行kinit的方式</span></div><div class="line">[op]$ kinit -kt op.keytab op</div><div class="line">[op]$ spark-submit --master <span class="built_in">local</span> --class SparkPi ./<span class="built_in">test</span>Jars/my.jar 4</div><div class="line"></div><div class="line"><span class="comment"># 提交keytab的方式</span></div><div class="line">[op]$ spark-submit --keytab <span class="built_in">test</span>Jars/op.keytab --principal op --master <span class="built_in">local</span> --class SparkPi ./<span class="built_in">test</span>Jars/my.jar 4</div></pre></td></tr></table></figure>
<p>其实还可以模拟其它用户的方式提交任务，比如使用ts账户提交：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[op]$ spark-submit --keytab <span class="built_in">test</span>Jars/ts.keytab --principal ts@HADOOP.CHINATELECOM.CN --master <span class="built_in">local</span> --class SparkPi ./<span class="built_in">test</span>Jars/my.jar 4</div></pre></td></tr></table></figure>
<p>当然没有那么简单，如果想要使用ts账户执行程序，需要进行如下设置：</p>
<h4 id="模拟其它用户需要的条件"><a href="#模拟其它用户需要的条件" class="headerlink" title="模拟其它用户需要的条件"></a>模拟其它用户需要的条件</h4><ol>
<li>ts要在KDC下生成对应的keytab和principal；</li>
<li>要在hadoop集群的所有机器上创建ts账户：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo groupadd ts</div><div class="line">sudo useradd -g ts ts</div></pre></td></tr></table></figure>
<p>值得注意的是，如果要在yarn中模拟其它用户执行，需要在集群中所有机器上增加该用户。</p>
<p>后期有时间了详细说明原因。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark Web与history web配置与测试]]></title>
      <url>http://flume.cn/2016/09/07/Spark-Web%E4%B8%8Ehistory%E6%B5%8B%E8%AF%95/</url>
      <content type="html"><![CDATA[<h2 id="Spark-Web的查看"><a href="#Spark-Web的查看" class="headerlink" title="Spark Web的查看"></a>Spark Web的查看</h2><ol>
<li>运行任意一个yarn-client或者yarn-cluster模式的spark测试用例</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">cd</span> <span class="variable">$SPARK_HOME</span></div><div class="line">$ spark-submit --keytab <span class="built_in">test</span>Jars/op.keytab --principal op --master yarn-client --class SparkPi ./<span class="built_in">test</span>Jars/my.jar 4</div></pre></td></tr></table></figure>
<ol>
<li>打开<a href="http://yarn-host:8088/cluster页面，找到正在运行的Spark测试用例" target="_blank" rel="external">http://yarn-host:8088/cluster页面，找到正在运行的Spark测试用例</a></li>
</ol>
<img src="/2016/09/07/Spark-Web与history测试/spark-yarn-web1.png" alt="spark-yarn-web1.png" title="">
<p>点击上图所示的AM，就进入了Spark的Web界面：下图就是Spark程序的web界面，值得注意的是，这个web界面会随着spark程序的运行结束而消失<br><img src="/2016/09/07/Spark-Web与history测试/spark-yarn-web2.png" alt="spark-yarn-web2.png" title=""></p>
<h2 id="Spark-history-Web查看测试"><a href="#Spark-history-Web查看测试" class="headerlink" title="Spark history Web查看测试"></a>Spark history Web查看测试</h2><p>在Kerberos环境下要启动spark history配置，需要在 spark -env下面开启如下配置 SPARK_HISTORY_OPTS：</p>
<figure class="highlight bash"><figcaption><span>spark-env.sh</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># history需要的Kerberos配置</span></div><div class="line">SPARK_HISTORY_OPTS=<span class="string">"-Dspark.history.ui.port=8777 -Dspark.history.retainedApplications=10 -Dspark.history.fs.logDirectory=hdfs://ns/user/op/sparkHistoryServer -Dspark.history.kerberos.enabled=true -Dspark.history.kerberos.principal=op @HADOOP.CHINATELECOM.CN -Dspark.history.kerberos.keytab=/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/conf/op.keytab"</span></div></pre></td></tr></table></figure>
<p>然后通过 ./sbin/start-history-server.sh 命令启动history-server<br>然后登录 <a href="http://spark-client-ip:8777/" target="_blank" rel="external">http://spark-client-ip:8777/</a> 即可查看 spark-history-web</p>
<img src="/2016/09/07/Spark-Web与history测试/spark-yarn-web3.png" alt="spark-yarn-web3.png" title="">]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[kerberos下spark客户端的配置]]></title>
      <url>http://flume.cn/2016/09/06/kerberos%E4%B8%8Bspark%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%9A%84%E9%85%8D%E7%BD%AE/</url>
      <content type="html"><![CDATA[<p>Kerberos环境下spark的客户端配置并不是很多，主要需要配置的是spark-history与spark-sql</p>
<p>软件版本：spark-1.6.2</p>
<p>注：正式环境中，需要将spark客户端的路径放入其它短路经，比如 /etc/local/spark 等<br><figure class="highlight bash"><figcaption><span>spark-env.sh</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 由于此处的 hive-site.xml 需要做一定修改，所以需要将hive-site.xml core-site.xml hdfs-site.xml yarn-site.xml等导入conf文件夹下</span></div><div class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/conf</div><div class="line"></div><div class="line"><span class="built_in">export</span> JAVA_HOME=/usr/java/jdk1.7.0_75</div><div class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$LD_LIBRARY_PATH</span>:/usr/lib/hadoop/lib/native/</div><div class="line"><span class="built_in">export</span> SPARK_LIBRARY_PATH=/usr/lib/hadoop/lib/native/:<span class="variable">$SPARK_LIBRARY_PATH</span></div><div class="line"></div><div class="line"><span class="comment"># history需要的Kerberos配置</span></div><div class="line">SPARK_HISTORY_OPTS=<span class="string">"-Dspark.history.ui.port=8777 -Dspark.history.retainedApplications=10 -Dspark.history.fs.logDirectory=hdfs://ns/user/op/sparkHistoryServer -Dspark.history.kerberos.enabled=true -Dspark.history.kerberos.principal=op    @HADOOP.CHINATELECOM.CN -Dspark.history.kerberos.keytab=/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/conf/op.keytab"</span></div></pre></td></tr></table></figure></p>
<h4 id="从hive-keytab-hiveserver创建spark-thrift-server的keytab"><a href="#从hive-keytab-hiveserver创建spark-thrift-server的keytab" class="headerlink" title="从hive.keytab_hiveserver创建spark-thrift-server的keytab"></a>从hive.keytab_hiveserver创建spark-thrift-server的keytab</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">-rw------- 1 hive hive     424 8月  23 09:55 hive.keytab_hiveserver</div><div class="line">-rw------- 1 op   bigdata  424 9月   3 12:25 hive.keytab_sparkthrift</div></pre></td></tr></table></figure>
<h4 id="hive-site的配置"><a href="#hive-site的配置" class="headerlink" title="hive-site的配置"></a>hive-site的配置</h4><p>修改hive-site.xml：</p>
<ul>
<li>增加hive.server2.thrift.bind.host</li>
<li>修改hive.server2.thrift.port为10010</li>
<li>修改hive.server2.authentication.kerberos.keytab为如下</li>
</ul>
<figure class="highlight bash"><figcaption><span>hive-site.xml</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">145 &lt;!-- ZooKeeper conf--&gt;</div><div class="line">146 &lt;property&gt;</div><div class="line">147   &lt;name&gt;hive.server2.enable.doAs&lt;/name&gt;</div><div class="line">148   &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</div><div class="line">149   &lt;description&gt; Impersonate the connected user &lt;/description&gt;</div><div class="line">150 &lt;/property&gt;</div><div class="line">151 &lt;property&gt;</div><div class="line">152   &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</div><div class="line">153   &lt;value&gt;10010&lt;/value&gt;</div><div class="line">154   &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class="line">155 &lt;/property&gt;</div><div class="line">156 </div><div class="line">157 &lt;property&gt;</div><div class="line">158    &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</div><div class="line">159    &lt;value&gt;<span class="built_in">test</span>-bdd-076&lt;/value&gt;</div><div class="line">160    &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class="line">161  &lt;/property&gt;</div><div class="line">162 </div><div class="line">163 &lt;property&gt;</div><div class="line">164   &lt;name&gt;hive.metastore.execute.setugi&lt;/name&gt;</div><div class="line">165   &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</div><div class="line">166 &lt;/property&gt;</div><div class="line">...</div><div class="line">209 &lt;property&gt;</div><div class="line">210    &lt;name&gt;hive.server2.authentication.kerberos.principal&lt;/name&gt;</div><div class="line">211    &lt;value&gt;hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN&lt;/value&gt;</div><div class="line">212  &lt;/property&gt;</div><div class="line">213 &lt;property&gt;</div><div class="line">214   &lt;name&gt;hive.server2.authentication.kerberos.keytab&lt;/name&gt;</div><div class="line">215   &lt;value&gt;/etc/hive/conf/hive.keytab_sparkthrift&lt;/value&gt;</div><div class="line">216 &lt;/property&gt;</div></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[在Kerberos环境下配置hue通过spark-thrift-server访问SparkSql]]></title>
      <url>http://flume.cn/2016/09/05/%E5%9C%A8Kerberos%E7%8E%AF%E5%A2%83%E4%B8%8B%E9%85%8D%E7%BD%AEhue%E9%80%9A%E8%BF%87spark-thrift-server%E8%AE%BF%E9%97%AESparkSql/</url>
      <content type="html"><![CDATA[<p>hue-spark-thriftserver-kerberos</p>
<h3 id="背景说明"><a href="#背景说明" class="headerlink" title="背景说明"></a>背景说明</h3><p>Kerberos项目最后要对基于Hue的TODP平台进行安全测试，在搭建配置的过程中踩了一些坑，现在把其中的配置与步骤进行总结，以免以后忘记。</p>
<p>其中用到以下代号：<br>40机器：hue平台所在的机器<br>76机器：spark thrift服务端口10010，hive-thrift-server服务端口10000<br>74机器：spark thrift服务端口10010，hive-thrift-server服务端口10000<br>TEST-BDD-HIVESERVER机器：负载均衡所在的机器，负载均衡机器需要配合开启10000和10010端口</p>
<p>在kerberos认证下, sparksql的thriftserver连接hiveserver2变得相对复杂，主要是因为各种kerberos认证出现各种问题。后来由于hive使用了负载均衡，所以spark-sql也需加入负载均衡，否则不能使用，就是这个负载均衡服务器的加入使得kerberos认证变得更加复杂，使得不明原理的新手在配置kerberos的keytab与principal时各种不匹配。这里是通过Hue可视化界面调用后台的sparksql,然后sparksql通过JDBC连接Hive的hiveServer2服务。</p>
<h3 id="40机器hue端配置"><a href="#40机器hue端配置" class="headerlink" title="40机器hue端配置"></a>40机器hue端配置</h3><p>进入40机器hue所在的目录<br><figure class="highlight bash"><figcaption><span>hue.ini</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">cd</span> /usr/lib/hue/ </div><div class="line">$ vim desktop/conf/hue.ini</div></pre></td></tr></table></figure></p>
<p>修改hue的配置文件如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">1119 [spark]</div><div class="line">...</div><div class="line">1134   <span class="comment"># spark-sql config</span></div><div class="line">1135   spark_sql_server_host=TEST-BDD-HIVESERVER</div><div class="line">1136   <span class="comment">## spark_sql_server_port=10010</span></div></pre></td></tr></table></figure></p>
<p>由于此处使用了负载均衡，所以上述TEST-BDD-HIVESERVER指向的是负载均衡所在的ip，最终会转发给两个spark-thrift-server</p>
<h3 id="Kerberos服务器端配置"><a href="#Kerberos服务器端配置" class="headerlink" title="Kerberos服务器端配置"></a>Kerberos服务器端配置</h3><p>生成类似 hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN 的keytab，配置了负载均衡后，使用test-bdd-hiveserver</p>
<h3 id="76机器上的配置"><a href="#76机器上的配置" class="headerlink" title="76机器上的配置"></a>76机器上的配置</h3><p>76机器与74机器配置步骤一样，只是hive-site.xml需要改一处，将下面的 076改成 074即可<br><figure class="highlight bash"><figcaption><span>hive-site.xml</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">   &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</div><div class="line">   &lt;value&gt;<span class="built_in">test</span>-bdd-076&lt;/value&gt;</div><div class="line">   &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure></p>
<p>其它都一样，所以在这里只写076的配置步骤</p>
<h4 id="从hive-keytab创建spark的keytab"><a href="#从hive-keytab创建spark的keytab" class="headerlink" title="从hive.keytab创建spark的keytab"></a>从hive.keytab创建spark的keytab</h4><p>然后在/etc/hive/conf/下创建spark需要的keytab，在这里使用hiveserver的keytab，将已有的hive.keytab_hiveserver 拷贝成 hive.keytab_sparkthrift，然后修改权限如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">-rw------- 1 hive hive     424 8月  23 09:55 hive.keytab_hiveserver</div><div class="line">-rw------- 1 op   bigdata  424 9月   3 12:25 hive.keytab_sparkthrift</div></pre></td></tr></table></figure>
<p>修改好后用如下命令检查：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">$ sudo klist -k hive.keytab_sparkthrift </div><div class="line">Keytab name: FILE:hive.keytab_sparkthrift</div><div class="line">KVNO Principal</div><div class="line">---- --------------------------------------------------------------------------</div><div class="line">   1 hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div><div class="line">   1 hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div><div class="line">   1 hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div><div class="line">   1 hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div><div class="line">   1 hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div></pre></td></tr></table></figure>
<p>如果klist是如上结果，就对了</p>
<h4 id="配置spark需要的hive-site-xml"><a href="#配置spark需要的hive-site-xml" class="headerlink" title="配置spark需要的hive-site.xml"></a>配置spark需要的hive-site.xml</h4><p>由于需要修改hive的一些配置，进入76机器spark所在的目录，将<code>/etc/hive/conf/</code>下的<code>hive-site.xml</code>拷贝到spark的conf下，赋予权限并修改<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ sudo cp /etc/hive/conf/hive-site.xml <span class="variable">$SPARK_HOME</span>/conf/</div><div class="line">$ <span class="built_in">cd</span> <span class="variable">$SPARK_HOME</span></div><div class="line">$ sudo chmod op conf/hive-site.xml</div><div class="line">$ vim conf/hive-site.xml</div></pre></td></tr></table></figure></p>
<p>修改hive-site.xml,增加hive.server2.thrift.bind.host</p>
<figure class="highlight bash"><figcaption><span>hive-site.xml</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">&lt;!-- ZooKeeper conf--&gt;</div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;hive.server2.enable.doAs&lt;/name&gt;</div><div class="line">  &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</div><div class="line">  &lt;description&gt; Impersonate the connected user &lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</div><div class="line">  &lt;value&gt;10010&lt;/value&gt;</div><div class="line">  &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line">&lt;property&gt;</div><div class="line">   &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</div><div class="line">   &lt;value&gt;<span class="built_in">test</span>-bdd-076&lt;/value&gt;</div><div class="line">   &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class="line"> &lt;/property&gt;</div><div class="line"></div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;hive.metastore.execute.setugi&lt;/name&gt;</div><div class="line">  &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">   &lt;name&gt;hive.server2.authentication.kerberos.principal&lt;/name&gt;</div><div class="line">   &lt;value&gt;hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN&lt;/value&gt;</div><div class="line"> &lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;hive.server2.authentication.kerberos.keytab&lt;/name&gt;</div><div class="line">  &lt;value&gt;/etc/hive/conf/hive.keytab_sparkthrift&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line"><span class="comment">#### 启动Spark-thrift-server</span></div><div class="line">``` bash</div><div class="line">$ <span class="built_in">cd</span> <span class="variable">$SPARK_HOME</span></div><div class="line">$ ./sbin/start-thriftserver.sh</div></pre></td></tr></table></figure>
<p>可以通过如下日志查看是否启动成功：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ vim logs/spark-op-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-TEST-BDD-076.out</div></pre></td></tr></table></figure></p>
<p>启动成功会看到如下日志:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"> 96 16/09/05 13:41:25 INFO AbstractService: Service:HiveServer2 is started.</div><div class="line"> 97 16/09/05 13:41:25 INFO HiveThriftServer2: HiveThriftServer2 started</div><div class="line"> 98 16/09/05 13:41:25 INFO UserGroupInformation: Login successful <span class="keyword">for</span> user hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN using keytab file /etc/hive/conf/hive.keytab_sparkthrift</div><div class="line"> 99 16/09/05 13:41:25 INFO AbstractDelegationTokenSecretManager: Updating the current master key <span class="keyword">for</span> generating delegation tokens</div><div class="line">100 16/09/05 13:41:25 INFO TokenStoreDelegationTokenSecretManager: New master key with key id=0</div><div class="line">101 16/09/05 13:41:25 INFO TokenStoreDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)</div><div class="line">102 16/09/05 13:41:25 INFO AbstractDelegationTokenSecretManager: Updating the current master key <span class="keyword">for</span> generating delegation tokens</div><div class="line">103 16/09/05 13:41:25 INFO TokenStoreDelegationTokenSecretManager: New master key with key id=1</div><div class="line">104 16/09/05 13:41:25 INFO ThriftCLIService: Starting ThriftBinaryCLIService on port 10010 with 5...500 worker threads</div></pre></td></tr></table></figure></p>
<h3 id="负载均衡机器的查看"><a href="#负载均衡机器的查看" class="headerlink" title="负载均衡机器的查看"></a>负载均衡机器的查看</h3><p>进入 67.121机器<br>输入 命令 <code>sudo ipvsadm -ln</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">$ sudo ipvsadm -ln</div><div class="line">IP Virtual Server version 1.2.1 (size=4194304)</div><div class="line">Prot LocalAddress:Port Scheduler Flags</div><div class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</div><div class="line">TCP  10.142.67.123:10000 wlc persistent 7200 synproxy</div><div class="line">  -&gt; 10.142.78.74:10000           FullNat 50     3          0         </div><div class="line">  -&gt; 10.142.78.76:10000           FullNat 50     0          0         </div><div class="line">TCP  10.142.67.123:10010 wlc persistent 7200 synproxy</div><div class="line">  -&gt; 10.142.78.74:10010           FullNat 50     0          0         </div><div class="line">  -&gt; 10.142.78.76:10010           FullNat 50     2          0</div></pre></td></tr></table></figure>
<p>就可以看到负载均衡的情况了：</p>
<h3 id="踩坑说明以及解决方案"><a href="#踩坑说明以及解决方案" class="headerlink" title="踩坑说明以及解决方案"></a>踩坑说明以及解决方案</h3><h4 id="缺少配置kerberos认证错误"><a href="#缺少配置kerberos认证错误" class="headerlink" title="缺少配置kerberos认证错误"></a>缺少配置kerberos认证错误</h4><p>需要在hive-site.xml文件中添加kerberos认证相关配置</p>
<h4 id="kerberos认证失败"><a href="#kerberos认证失败" class="headerlink" title="kerberos认证失败"></a>kerberos认证失败</h4><p>1)  在hive-site.xml中配置好kerberos认证，但是op用户下无法读取hive.keytab的问题，出现unable to login …given keytab/principal 以及Unable to obtain password from user。因为hive.keytab 是hive用户创建的，op用户无法读取，导致看似kerberos已经配置好，<br>但是程序没有读取权限，依旧认为没有配置好，这是会有在日志文件中会有NULLPOINT类似的错误提示，说明是没有读取权限。解决方案是复制hive.keytab到op用户下。<br>2）在hue界面连接spark时可能会出现10010端口不能连接的问题，这是sparkthrift没有启动导致的；<br>3）spark thriftserver明明已经启动，但是hue界面仍旧不能连接，出现TTransportException的错误，原因是kerberos配置没有配置正确，即没有配置kerberos认证的keytab与principal。hive/test-bdd-hiveserver必须与hive.keytab_hiveserver配套使用，同理，test-bdd-074或者 test-bdd-076必须与hive/test-bdd-74或者hive/test-bdd-76配套使用，否则出现认证失败的问题。</p>
<h4 id="hue的配置问题。"><a href="#hue的配置问题。" class="headerlink" title="hue的配置问题。"></a>hue的配置问题。</h4><p>在hue的desktop/conf目录下hue.ini文件中，主要配置spark_sql_server_host，也就是spark thriftserver所在主机，这里可以是负载均衡服务器TEST-BDD-HIVESERVER,spark_sql_server_port 是spark thriftserver的服务端口。<br>需要注意的是，加上kerberos认证后，主机名不能是ip地址的形式，需要FQDN的形式。hive的配置需要注意的是hive_server_host，这里绝对不能是hiveserver2的服务器的地址，一定是负载均衡服务器的地址，不然在hue界面连接HIVE时出现<br>Unable to access databases, Query Server or Metastore may be down.的错误以及GSS initial failed的错误，无法访问hive数据库。</p>
<h4 id="metastore的问题"><a href="#metastore的问题" class="headerlink" title="metastore的问题"></a>metastore的问题</h4><p>连接metastore也需要principal的认证。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">20 &lt;property&gt;</div><div class="line">221   &lt;name&gt;hive.metastore.sasl.enabled&lt;/name&gt;</div><div class="line">222   &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</div><div class="line">223   &lt;description&gt;If <span class="literal">true</span>, the metastore thrift interface will be secured with SASL. Clients must authenticate with Kerberos.&lt;/description&gt;</div><div class="line">224 &lt;/property&gt;</div><div class="line">225 &lt;property&gt;</div><div class="line">226   &lt;name&gt;hive.metastore.kerberos.principal&lt;/name&gt;</div><div class="line">227   &lt;value&gt;hive/_HOST@HADOOP.CHINATELECOM.CN&lt;/value&gt;</div><div class="line">228   &lt;description&gt;The service principal <span class="keyword">for</span> the metastore thrift server. The special string _HOST will be replaced automatically with the correct host name.&lt;/description&gt;</div><div class="line">229 &lt;/property&gt;</div></pre></td></tr></table></figure></p>
<p>之所以问题多多，主要原因是对kerberos+Hive+lvs整体原理没有搞清楚，以至于在配置过程中出现各种错误。我们搭建的hive集群有74,76两台主机，spark thriftserver也有74,76两台主机，负载均衡服务器在test-bdd-hiveserver上。在配置时，需要将spark-sql-server-host配置成test-bdd-hiveserver,因为对spark而言，74与76上的hiveserver是一个整体，不能配置成单一的主机，不然lvs可能会将服务分到另外一台主机上，造成主机配置失败。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[spark支持snappy压缩踩坑总结]]></title>
      <url>http://flume.cn/2016/08/15/spark%E6%94%AF%E6%8C%81snappy%E5%8E%8B%E7%BC%A9%E8%B8%A9%E5%9D%91%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<h3 id="配置snappy压缩"><a href="#配置snappy压缩" class="headerlink" title="配置snappy压缩"></a>配置snappy压缩</h3><p>首先在/usr/lib/hadoop/lib/目录下配置lzo相关的包，<br>然后在spark客户端配置如下：</p>
<figure class="highlight bash"><figcaption><span>spark-default.conf</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">spark.driver.extraClassPath /usr/lib/hadoop/lib/*</div><div class="line">spark.driver.extraLibraryPath /usr/lib/hadoop/lib/native</div><div class="line">spark.executor.extraClassPath /usr/lib/hadoop/lib/*</div><div class="line">spark.executor.extraLibraryPath /usr/lib/hadoop/lib/native</div></pre></td></tr></table></figure>
<p>如上配置，即可，但是为了得到这么小小的一点配置，浪费了三天的时间啊，网上的资料都是转载，无法解决问题。最新的官网的配置文件中并没有关于spark.executor.extraClassPath的配置，查了源码才得知，作为教训。以后出现问题要冷静思考，不要简单的去网上搜索，先判断问题出现的原因，知其所以然，必要时要去源码中查询，否则会浪费很多时间，走很多弯路。</p>
<h3 id="踩坑集锦"><a href="#踩坑集锦" class="headerlink" title="踩坑集锦"></a>踩坑集锦</h3><p>首先，会遇到这个错误：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Compression codec com.hadoop.compression.lzo.LzoCodec not found</div></pre></td></tr></table></figure>
<p>原因是spark-env.sh的配置文件缺少关联hadoop的配置语句</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> SPARK_LIBRARY_PATH=<span class="variable">$SPARK_LIBRARY_PATH</span>:/usr/lib/hadoop/lib/native/:/usr/lib/hadoop/lib/*</div></pre></td></tr></table></figure>
<p>然后yarn-cluster模式下snappy压缩总会报错：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="number">16</span>/<span class="number">08</span>/<span class="number">08</span> <span class="number">19</span>:<span class="number">05</span>:<span class="number">03</span> DEBUG util.NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib</div><div class="line"><span class="number">16</span>/<span class="number">08</span>/<span class="number">08</span> <span class="number">19</span>:<span class="number">05</span>:<span class="number">03</span> WARN util.NativeCodeLoader: Unable to load <span class="keyword">native</span>-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</div><div class="line"><span class="number">1</span>）</div><div class="line"><span class="number">16</span>/<span class="number">08</span>/<span class="number">08</span> <span class="number">19</span>:<span class="number">05</span>:<span class="number">03</span> DEBUG util.PerformanceAdvisory: Both <span class="keyword">short</span>-circuit local reads and UNIX domain socket are disabled.</div><div class="line"><span class="number">16</span>/<span class="number">08</span>/<span class="number">08</span> <span class="number">19</span>:<span class="number">05</span>:<span class="number">03</span> DEBUG sasl.DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration <span class="keyword">for</span> dfs.data.transfer.protection</div><div class="line"><span class="number">16</span>/<span class="number">08</span>/<span class="number">08</span> <span class="number">19</span>:<span class="number">05</span>:<span class="number">03</span> ERROR lzo.GPLNativeCodeLoader: Could not load <span class="keyword">native</span> gpl library</div><div class="line">java.lang.UnsatisfiedLinkError: no gplcompression in java.library.path</div><div class="line"> at java.lang.ClassLoader.loadLibrary(ClassLoader.java:<span class="number">1886</span>)</div><div class="line"> at java.lang.Runtime.loadLibrary0(Runtime.java:<span class="number">849</span>)</div><div class="line"> at java.lang.System.loadLibrary(System.java:<span class="number">1088</span>)</div><div class="line"> at com.hadoop.compression.lzo.GPLNativeCodeLoader.&lt;clinit&gt;(GPLNativeCodeLoader.java:<span class="number">32</span>)</div><div class="line"> at com.hadoop.compression.lzo.LzoCodec.&lt;clinit&gt;(LzoCodec.java:<span class="number">71</span>)</div><div class="line"> at java.lang.Class.forName0(Native Method)</div><div class="line"> at java.lang.Class.forName(Class.java:<span class="number">274</span>)</div><div class="line"> at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:<span class="number">2013</span>)</div><div class="line"> at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:<span class="number">1978</span>)</div><div class="line"> at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:<span class="number">128</span>)</div><div class="line"> at org.apache.hadoop.io.compress.CompressionCodecFactory.&lt;init&gt;(CompressionCodecFactory.java:<span class="number">175</span>)</div><div class="line"> at org.apache.hadoop.mapred.TextInputFormat.configure(TextInputFormat.java:<span class="number">45</span>)</div><div class="line"> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</div></pre></td></tr></table></figure>
<p>这个究其原因就是程序运行的那个节点找不到lzo解压的包导致的，官网中只说明了 spark.driver.extraClassPath，但并没有说明配置spark.executor.extraClassPath 与 spark.executor.extraLibraryPath，导致不管怎么根据网上博客或者官网配置配，executor还是找不到lzo压缩相关的包，后来聪哥通过源码查看才发现有这么一个参数配置，只是各类文档中都没有，加上就ok了~</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Java中的“钩子”]]></title>
      <url>http://flume.cn/2016/07/18/Java%E4%B8%AD%E7%9A%84%E2%80%9C%E9%92%A9%E5%AD%90%E2%80%9D/</url>
      <content type="html"><![CDATA[<p>最近看银辉大哥写的对hdfs中小文件打包成大文件的程序的时候，发现他在代码中巧妙地运用了“钩子”，是用匿名内部类来实现的，感觉很酷，所以决定好好向大神学习一下使用匿名内部类实现钩子的用法：</p>
<h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>为了让我能够快速了解这个方法的使用，银辉大哥首先给我出个题：<br>比如上医院看病，一般会有　挂号，问诊，开药，付费，拿药　几个过程，但是不同的病科室不同，大夫不同，药方不同，付费方式不同，取药方式不同。写一个程序，打印不同的看病流程：如一个人感冒：挂呼吸科，看张大夫，开了砒霜，支付宝支付，快递拿药。<br>另外一个人胃痛，挂了内科，看了王大夫，开了鹤顶红，没有付钱，直接抢药。</p>
<h4 id="模板方法"><a href="#模板方法" class="headerlink" title="模板方法"></a>模板方法</h4><p>模板方法模式（Template Method）：定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。该模式使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。<br>使用场景：<br>1、一次性实现一个算法的不变的部分，并将可变的行为留给子类来实现。<br>2、各子类中公共的行为应被提取出来并集中到一个公共父类中以避免代码重复。即“重分解以一般化”，首先识别现有代码中的不同之处，并且将不同之处分离为新的操作。最后，用一个调用这些新的操作的模板方法来替换这些不同的代码。<br>3、控制子类扩展。模板方法只在特定点调用“Hook Method（钩子方法）”操作，这样就只允许在这些点进行扩展。</p>
<img src="/2016/07/18/Java中的“钩子”/hook.jpg" alt="hook.jpg" title="">
<h4 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h4><h5 id="interface-GoHospital-java"><a href="#interface-GoHospital-java" class="headerlink" title="interface GoHospital.java:"></a>interface GoHospital.java:</h5><pre><code>/**
 * Created by Adam on 2016/5/27.
 */
public interface GoHospital extends Runnable{
    /**
     * 挂号
     * @return 科室名
     */
    void onRegister(ActionHook register);

    /**
     * 问诊
     * @return 病名
     */
    void onInterview(ActionHook interview);

    /**
     * 开药
     * @return 药名
     */
    void onMedicine(ActionHook medicine);

    /**
     * 付费
     * @return 付了多少钱
     */
    void onPay(ActionHook pay);

    /**
     * 返回取药方式
     * @return 取药方式
     */
    void onGetMedicine(ActionHook getMedicine);

}
</code></pre><h6 id="AbsGoHospital-java"><a href="#AbsGoHospital-java" class="headerlink" title="AbsGoHospital.java"></a>AbsGoHospital.java</h6><pre><code>/**
 * Created by Adam on 2016/5/27.
 */
abstract class AbsGoHospital implements GoHospital{
    private ActionHook register;
    private ActionHook intterview;
    private ActionHook medicine;
    private ActionHook pay;
    private ActionHook getMedicine;

    public void run(){
        if (register != null) {
            boolean isRegisterOk = register.exec();
            if (isRegisterOk &amp;&amp; intterview != null) {
                boolean isIntterviewOk = intterview.exec(isRegisterOk);
                if (isIntterviewOk &amp;&amp; medicine != null) {
                    boolean isMedicineOK = medicine.exec();
                    if (isMedicineOK &amp;&amp; pay != null) {
                        boolean isPayOk = pay.exec();
                        if (isPayOk &amp;&amp; getMedicine != null) {
                            boolean isGetMedicineOk = getMedicine.exec();
                        }
                    }
                }
            }
        }

    }



    /**
     * 挂号
     * @return 科室名
     */
    public void onRegister(ActionHook register) {
        this.register = register;
    }

    /**
     * 问诊
     * @return 病名
     */
    public void onInterview(ActionHook interview) {
        this.intterview = interview;
    }

    /**
     * 开药
     * @return 药名
     */
    public void onMedicine(ActionHook medicine) {
        this.medicine = medicine;
    }

    /**
     * 付费
     * @return 付了多少钱
     */
    public void onPay(ActionHook pay) {
        this.pay = pay;
    }

    /**
     * 返回取药方式
     * @return 取药方式
     */
    public void onGetMedicine(ActionHook getMedicine) {
        this.getMedicine = getMedicine;
    }

}
</code></pre><h6 id="定义一个钩子"><a href="#定义一个钩子" class="headerlink" title="定义一个钩子"></a>定义一个钩子</h6><pre><code>/**
 * Created by Adam on 2016/5/27.
 */
public interface ActionHook {
    /**
     * 钩子逻辑
     * @param args 任意参数
     */
    boolean exec(Object ... args);
}
</code></pre><h6 id="定义GoHostpital的实现类"><a href="#定义GoHostpital的实现类" class="headerlink" title="定义GoHostpital的实现类"></a>定义GoHostpital的实现类</h6><pre><code>/**
 * Created by Adam on 2016/5/27.
 */
public class XiaogangGoHopital extends AbsGoHospital {
    // todo 这里的字段，方法都可以用enum扩展，由于本例为了学习，所以略过
    private String name;
    private String diease;
    private String paymentPre;
    private String getMedicineWay;

    public XiaogangGoHopital(String name, String diease, String paymentPre, String getMedicineWay) {
        this.name = name;
        this.diease = diease;
        this.paymentPre = paymentPre;
        this.getMedicineWay = getMedicineWay;
    }

    @Override
    public String toString() {
        return &quot;XiaogangGoHopital{&quot; +
                &quot;name=&apos;&quot; + name + &apos;\&apos;&apos; +
                &quot;, diease=&apos;&quot; + diease + &apos;\&apos;&apos; +
                &quot;, paymentPre=&apos;&quot; + paymentPre + &apos;\&apos;&apos; +
                &quot;, getMedicineWay=&apos;&quot; + getMedicineWay + &apos;\&apos;&apos; +
                &apos;}&apos;;
    }

    public String getDiease() {
        return diease;
    }

    public void setDiease(String diease) {
        this.diease = diease;
    }

    public String getPaymentPre() {
        return paymentPre;
    }

    public void setPaymentPre(String paymentPre) {
        this.paymentPre = paymentPre;
    }

    public String getGetMedicineWay() {
        return getMedicineWay;
    }

    public void setGetMedicineWay(String getMedicineWay) {
        this.getMedicineWay = getMedicineWay;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    @Override
    public void onRegister(final ActionHook register) {
        super.onRegister(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                register.exec(args);
                System.out.println(name + &quot;骑电瓶车去的&quot;);
                return true;
            }
        });
    }
}
</code></pre><h6 id="LiGoHopital-实现类"><a href="#LiGoHopital-实现类" class="headerlink" title="LiGoHopital 实现类"></a>LiGoHopital 实现类</h6><pre><code>/**
 * Created by Adam on 2016/5/27.
 */
public class LiGoHopital extends AbsGoHospital {
    // todo 这里的字段，方法都可以用enum扩展，由于本例为了学习，所以略过
    private String name;
    private String diease;
    private String paymentPre;
    private String getMedicineWay;

    public LiGoHopital(String name, String diease, String paymentPre, String getMedicineWay) {
        this.name = name;
        this.diease = diease;
        this.paymentPre = paymentPre;
        this.getMedicineWay = getMedicineWay;
    }

    @Override
    public String toString() {
        return &quot;XiaogangGoHopital{&quot; +
                &quot;name=&apos;&quot; + name + &apos;\&apos;&apos; +
                &quot;, diease=&apos;&quot; + diease + &apos;\&apos;&apos; +
                &quot;, paymentPre=&apos;&quot; + paymentPre + &apos;\&apos;&apos; +
                &quot;, getMedicineWay=&apos;&quot; + getMedicineWay + &apos;\&apos;&apos; +
                &apos;}&apos;;
    }

    public String getDiease() {
        return diease;
    }

    public void setDiease(String diease) {
        this.diease = diease;
    }

    public String getPaymentPre() {
        return paymentPre;
    }

    public void setPaymentPre(String paymentPre) {
        this.paymentPre = paymentPre;
    }

    public String getGetMedicineWay() {
        return getMedicineWay;
    }

    public void setGetMedicineWay(String getMedicineWay) {
        this.getMedicineWay = getMedicineWay;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    @Override
    public void onRegister(final ActionHook register) {
        super.onRegister(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                register.exec(args);
                System.out.println(name + &quot;ta开车去的&quot;);
                return true;
            }
        });
    }

    @Override
    public void onPay(final ActionHook pay) {
        super.onPay(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                pay.exec();
                System.out.println(&quot;他不喜欢付钱&quot;);
                return true;
            }
        });
    }
}
</code></pre><h6 id="mainz测试函数"><a href="#mainz测试函数" class="headerlink" title="mainz测试函数"></a>mainz测试函数</h6><pre><code>/**
 * Created by Adam on 2016/5/27.
 */
public class Main {
    public static void main(String[] args) {
        XiaogangGoHopital xiaogangGoHopital = new XiaogangGoHopital(&quot;Gang&quot;, &quot;jiba&quot;, &quot;zhifubao&quot;, &quot;shunfeng&quot;);
        xiaogangGoHopital.onRegister(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                System.out.println(&quot;=====\n外科&quot;);
                return true;
            }
        });
        xiaogangGoHopital.onInterview(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                System.out.println(args[0]);
                System.out.println(&quot;右臂肌肉损伤&quot;);
                return true;
            }
        });
        xiaogangGoHopital.onMedicine(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                System.out.println(&quot;钙片&quot;);
                return true;
            }
        });
        xiaogangGoHopital.onPay(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                System.out.println(&quot;支付宝付了 123 元&quot;);
                return true;
            }
        });
        xiaogangGoHopital.onGetMedicine(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                System.out.println(&quot;顺风快递&quot;);
                return true;
            }
        });

        Thread thread = new Thread(xiaogangGoHopital);
        thread.start();



        LiGoHopital liGoHopital = new LiGoHopital(&quot;Li&quot;, &quot;jiba&quot;, &quot;zhifubao&quot;, &quot;shunfeng&quot;);
        liGoHopital.onRegister(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                System.out.println(&quot;=====\n外科&quot;);
                return true;
            }
        });
        liGoHopital.onInterview(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                System.out.println(&quot;右臂肌肉损伤&quot;);
                return true;
            }
        });
        liGoHopital.onMedicine(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                System.out.println(&quot;钙片&quot;);
                return true;
            }
        });
        liGoHopital.onPay(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                System.out.println(&quot;支付宝付了 123 元&quot;);
                return true;
            }
        });
        liGoHopital.onGetMedicine(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                System.out.println(&quot;顺风快递&quot;);
                return true;
            }
        });

        Thread thread2 = new Thread(liGoHopital);
        thread2.start();

    }
}
</code></pre><h4 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a>测试结果</h4><p>结果如下：</p>
<pre><code>=====
外科
Gang骑电瓶车去的
true
右臂肌肉损伤
钙片
支付宝付了 123 元
顺风快递
=====
外科
Lita开车去的
右臂肌肉损伤
钙片
支付宝付了 123 元
他不喜欢付钱
顺风快递
</code></pre><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>由于工作任务蛮重，所以实现地很简单，详细代码可以参考hdfs小文件问题的归档程序；<br>理解设计模式，或者实现技巧，才是第一步，以后能够把它熟练运用才是最重要的！</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用scaladiagrams工具构建scala项目的UML图]]></title>
      <url>http://flume.cn/2016/06/13/%E4%BD%BF%E7%94%A8scaladiagrams%E5%B7%A5%E5%85%B7%E6%9E%84%E5%BB%BAscala%E9%A1%B9%E7%9B%AE%E7%9A%84UML%E5%9B%BE/</url>
      <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>阅读spark源码到storage这一块的时候，由于类的继承，调用之间的关系比较复杂，想要画一下UML图，idea自带的diagrams方法对java支持很好，但对scala的一些继承关系支持不佳，因此google了一下有没有可以画scala UML类图的工具，还真找到了：</p>
<p>我是在x64 windows10下面，使用gitbash工具作为shell命令行，亲测可用</p>
<h3 id="clone开源项目scaladiagrams并安装"><a href="#clone开源项目scaladiagrams并安装" class="headerlink" title="clone开源项目scaladiagrams并安装"></a>clone开源项目scaladiagrams并安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/mikeyhu/scaladiagrams.git</div><div class="line"><span class="built_in">cd</span> scaladiagrams</div><div class="line">./build</div></pre></td></tr></table></figure>
<h3 id="安装graphviz工具"><a href="#安装graphviz工具" class="headerlink" title="安装graphviz工具"></a>安装graphviz工具</h3><p>graphviz是一个开源的图形可视化软件，矢量图生成工具，与其他图形软件所不同，它的理念是“所想即所得”，通过dot语言来描述并绘制图形。<br><a href="http://www.graphviz.org/Download_windows.php" target="_blank" rel="external">http://www.graphviz.org/Download_windows.php</a><br>如上链接下载，然后安装即可，将安装路径加入path中，该工具的目的是通过scaladiagrams工具生成的依赖关系画图；</p>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><h4 id="生成依赖关系文件dotFile"><a href="#生成依赖关系文件dotFile" class="headerlink" title="生成依赖关系文件dotFile"></a>生成依赖关系文件dotFile</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./scaladiagrams --source <span class="string">"D:\spark-1.6.0\core\src\main\scala\org\apache\spark\storage"</span> &gt; dotFile</div></pre></td></tr></table></figure>
<p>dotFile文件就是依赖关系的文件：<br>官方命名为 dot语言，是一个表示图的语言，挺好玩的：</p>
<figure class="highlight scala"><figcaption><span>dot语言</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">digraph diagram &#123;</div><div class="line"><span class="string">"BlockException"</span> [style=filled, fillcolor=burlywood]</div><div class="line">  <span class="string">"BlockException"</span> -&gt; <span class="string">"Exception"</span>;</div><div class="line"></div><div class="line"><span class="string">"BlockFetchException"</span> [style=filled, fillcolor=burlywood]</div><div class="line">  <span class="string">"BlockFetchException"</span> -&gt; <span class="string">"SparkException"</span>;</div><div class="line"></div><div class="line"><span class="string">"BlockId"</span> [style=filled, fillcolor=darkorange]</div><div class="line">  </div><div class="line"></div><div class="line"><span class="string">"RDDBlockId"</span> [style=filled, fillcolor=burlywood]</div><div class="line">  <span class="string">"RDDBlockId"</span> -&gt; <span class="string">"BlockId"</span>;</div><div class="line"></div><div class="line">  。。。</div></pre></td></tr></table></figure>
<h4 id="使用graphviz工具画图"><a href="#使用graphviz工具画图" class="headerlink" title="使用graphviz工具画图"></a>使用graphviz工具画图</h4><p>生成svg文件，文件比较大的话建议用这个<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat dotFile | dot -Tsvg &gt; spark_storage.svg</div></pre></td></tr></table></figure></p>
<p>生成png文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat dotFile | dot -Tpng &gt; spark_storage.png</div></pre></td></tr></table></figure></p>
<p>画的效果部分截图如下（就是图有点扁平）：<br><img src="spark_storage_part.png" alt="spark_storage_part.png"></p>
<p>因为好玩，又画了一个spark_core的类图，太大了，不好看，为了部分解决这个问题，只要在dot文件的第一行加入<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rankdir=RL;</div></pre></td></tr></table></figure></p>
<p>即可使得图片稍微好看一点<br><img src="spark_storage_part2.png" alt="spark_storage_part2.png"></p>
<p>引用：<br><a href="http://stackoverflow.com/questions/7227952/generating-uml-diagram-from-scala-sources" target="_blank" rel="external">http://stackoverflow.com/questions/7227952/generating-uml-diagram-from-scala-sources</a><br><a href="https://github.com/mikeyhu/scaladiagrams" target="_blank" rel="external">https://github.com/mikeyhu/scaladiagrams</a><br><a href="http://www.graphviz.org/About.php" target="_blank" rel="external">http://www.graphviz.org/About.php</a><br><a href="http://www.graphviz.org/pdf/dotguide.pdf" target="_blank" rel="external">http://www.graphviz.org/pdf/dotguide.pdf</a><br><a href="http://www.tonyballantyne.com/graphs.html" target="_blank" rel="external">http://www.tonyballantyne.com/graphs.html</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Scala中_的用法]]></title>
      <url>http://flume.cn/2016/06/13/Scala%E4%B8%AD-%E7%9A%84%E7%94%A8%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>在看Spark源码的过程中，遇到了很多对 下划线_的运用，后来经过查阅资料总结如下（感谢万能的知乎,StackOverFlow）：</p>
<ul>
<li><p>作为“通配符”，类似Java中的*。如import scala.math._</p>
</li>
<li><p>:<em>*作为一个整体，告诉编译器你希望将某个参数当作参数序列处理！例如val s = sum(1 to 5:</em><em>)就是将1 to 5当作参数序列处理。向函数或方法传入可变参数时不能直接传入Range或集合或数组对象，需要使用:_</em>转换才可传入</p>
</li>
<li><p>指代一个集合中的每个元素。例如我们要在一个Array a中筛出偶数，并乘以2，可以用以下办法：<br>a.filter(<em>%2==0).map(2*</em>)。<br>又如要对缓冲数组ArrayBuffer b排序，可以这样：<br>val bSorted = b.sorted(_</p>
</li>
<li><p>在元组中，可以用方法_1, _2, _3访问组员。如a._2。其中句点可以用空格替代。</p>
</li>
<li><p>使用模式匹配可以用来获取元组的组员，例如]</p>
</li>
</ul>
<p>val (first, second, third) = t<br>但如果不是所有的部件都需要，那么可以在不需要的部件位置上使用<em>。比如上一例中val (first, second, </em>) = t</p>
<ul>
<li><p>还有一点，下划线_代表的是某一类型的默认值。<br>对于Int来说，它是0。<br>对于Double来说，它是0.0<br>对于引用类型，它是null。</p>
</li>
<li><p>访问tuple变量的某个元素时通过索引_n来取得第n个元素</p>
</li>
<li><p>类的setter方法，比如类A中定义了var f，则相当于定义了setter方法f<em>=，当然你可以自己定义f</em>=方法来完成更多的事情，比如设置前作一些判断或预处理之类的操作</p>
</li>
<li><p>用于将方法转换成函数，比如val f=sqrt _，以后直接调用f(250)就能求平方根了</p>
</li>
<li><p>Spark源码中，私有变量约定俗成以 _开头，比如： </p>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> _conf: <span class="type">SparkConf</span> = _</div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> _eventLogDir: <span class="type">Option</span>[<span class="type">URI</span>] = <span class="type">None</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> _eventLogCodec: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> _env: <span class="type">SparkEnv</span> = _</div></pre></td></tr></table></figure>
<p>引用：</p>
<p><a href="https://www.zhihu.com/question/21622725/" target="_blank" rel="external">https://www.zhihu.com/question/21622725/</a><br><a href="http://stackoverflow.com/questions/8000903/what-are-all-the-uses-of-an-underscore-in-scala" target="_blank" rel="external">http://stackoverflow.com/questions/8000903/what-are-all-the-uses-of-an-underscore-in-scala</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Scala学习资料总结]]></title>
      <url>http://flume.cn/2016/05/20/Scala%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<p>还在路上，持续更新</p>
<p>以前对函数式编程很陌生，初学scala，感觉scala很难学，学习途中走了许多弯路，虽然目前本人只是连scala入门都算不上，希望能够分享一些资料和读后感给和我一样初学scala的同学，少一些搜集资料的时间，也希望能够抛砖引玉，也请更多高手们多多指教，随便修改本帖，有好资料随便贴上。</p>
<hr>
<h4 id="书籍推荐"><a href="#书籍推荐" class="headerlink" title="书籍推荐"></a>书籍推荐</h4><ul>
<li>1.1 快学Scala(Scala for the Impatient)</li>
</ul>
<p>只推荐这一本书，因为比较薄，看得时候也能看懂，但只是大概懂一点点<br>这本纸质书是李总买的公共书籍，也是本人学习scala的第一本书籍，因为读了一遍后并没有直接使用，故刚读完就把里面的知识给忘光了。直到最近需要模块地方知识翻第二遍的时候，才发现这本书写得蛮好的，从简单到困难都有提及，对我来说可以当成工具书。推荐给喜欢纸质书的同学。</p>
<p>本人这里有我从网上收集的每一章的课后答案，供大家下载使用：密码：7ry4</p>
<p><a href="http://pan.baidu.com/s/1cynlgU" target="_blank" rel="external">快学Scala 课后习题答案集合.rar</a></p>
<ul>
<li>1.2 Programming Scala 2nd edition (2015年出版）</li>
</ul>
<p>很有名，没看过，对我来说看起来太厚太慢了</p>
<h4 id="网站推荐"><a href="#网站推荐" class="headerlink" title="网站推荐"></a>网站推荐</h4><p>个人觉得scala的好的学习资料都在网上，只分享个人觉得好的，将我所知分享给各位：</p>
<ul>
<li>2.1 <a href="http://twitter.github.io/scala_school/" target="_blank" rel="external">Scala School</a></li>
</ul>
<p>吐血推荐，twitter的scala教学网站，言简意赅，真的非常非常好，我几经周转看到这个资料，有种相见恨晚的感觉。重要的是有中文版。</p>
<ul>
<li>2.2 <a href="http://www.scala-tour.com/#/welcome" target="_blank" rel="external">scala-tour</a></li>
</ul>
<p>网友做的scala学习网站，之前中文版有bug，现在中文版也很好用。是一个交互式的学习网站，右边是概念，左边直接是代码示例，可以直接修改并运行，很清晰，适合假期在家里学习。</p>
<ul>
<li>2.3 <a href="http://www.scala-lang.org/api/current/" target="_blank" rel="external">scala-api</a></li>
</ul>
<p>scala 官方API不解释，作为浏览器书签</p>
<ul>
<li>2.4 <a href="http://docs.scala-lang.org/cheatsheets/" target="_blank" rel="external">scala快查</a></li>
</ul>
<p>scala官方出的让你快速查阅的网页，就一页涵盖了基本所有的操作</p>
<ul>
<li>2.5 <a href="http://twitter.github.io/effectivescala/index-cn.html" target="_blank" rel="external">effective scala</a></li>
</ul>
<p>twitter的scala资深玩家讲述了一些scala编程需要注意的问题，在纠结用哪一种编程方式或者数据结构好的时候，这是一个很好的参考，有中文版</p>
<ul>
<li>2.6 <a href="http://hongjiang.info/scala/" target="_blank" rel="external">scala 说点什么</a></li>
</ul>
<p>hongjiang大神对scala的一些理解，写得很深入，之前很多scala的英文文档都是hongjiang大神翻译的</p>
<ul>
<li>2.7 <a href="http://www.tuicool.com/articles/2QFRZfE" target="_blank" rel="external">scala编程规范</a></li>
</ul>
<p>国人总结的scala编程规范，写得很全很详细，希望大家能够从中受到启发，写出优质的代码</p>
<ul>
<li>2.8 <a href="http://www.ibm.com/developerworks/cn/java/j-scala01228.html" target="_blank" rel="external">面向 Java 开发人员的 Scala 指南</a><br>面向 Java 开发人员的 Scala 指南</li>
</ul>
<h4 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h4><ul>
<li>3.1 Intellij Idea</li>
</ul>
<p>强烈推荐Intellij Idea（缺点是比较占内存），其对scala的支持是极好的，可以自动检测出你的代码中的一些问题，帮你看想看的源码，极好的单元测试的支持，遵循scala规范的自动格式化等功能</p>
<pre><code>IntelliJ IDEA https://www.jetbrains.com/idea/  
(请支持正版，下载后24小时内删除) http://idea.lanyus.com
</code></pre><p>Idea有一个scala学习的插件，左边是代码，右边是命令结果，学习效果比在shell中好很多。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[flume的http监控参数说明]]></title>
      <url>http://flume.cn/2016/05/18/flume%E7%9A%84http%E7%9B%91%E6%8E%A7%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E/</url>
      <content type="html"><![CDATA[<p>如果要启用http监控，需要在启动flume的时候添加如下命令：</p>
<pre><code>bin/flume-ng agent --conf conf --conf-file conf/avroToHdfs2.conf --name a1 -Dflume.monitoring.type=http -Dflume.monitoring.port=8533 -Dflume.root.logger=INFO,console
</code></pre><p>这样，在flume所在IP的8533端口，就可以接收到如下json串，可以在chrome浏览器上安装 JSONView这个插件，使得阅读这个json串更加方便。</p>
<p>对于一个source，一个channel和一个sink的agent监控如下：</p>
<p>其中 channel的ChannelFillPercentage值是比较具有明显特征的属性，代表channel满时的百分比，重点就在这个值，一般情况下，这个值小于0.01就代表很通畅，sink的速度比source的速度快，如果这个值超过了5，就代表肯定是sink的速度不够快，需要对sink进行调优，或者需要控制source的速率。如果这个值缓慢增加，增加到一定程度，就会出现数据丢失的情况。</p>
<p>结果: 其中src-1是子自定义的source名称</p>
<pre><code>{
&quot;SOURCE.src-1&quot;:{
    &quot;OpenConnectionCount&quot;:&quot;0&quot;,        //目前与客户端或sink保持连接的总数量(目前只有avro source展现该度量)
    &quot;Type&quot;:&quot;SOURCE&quot;,                    
    &quot;AppendBatchAcceptedCount&quot;:&quot;1355&quot;,    //成功提交到channel的批次的总数量
    &quot;AppendBatchReceivedCount&quot;:&quot;1355&quot;,    //接收到事件批次的总数量
    &quot;EventAcceptedCount&quot;:&quot;28286&quot;,    //成功写出到channel的事件总数量，且source返回success给创建事件的sink或RPC客户端系统
    &quot;AppendReceivedCount&quot;:&quot;0&quot;,        //每批只有一个事件的事件总数量(与RPC调用中的一个append调用相等)
    &quot;StopTime&quot;:&quot;0&quot;,            //source停止时自Epoch以来的毫秒值时间
    &quot;StartTime&quot;:&quot;1442566410435&quot;,    //source启动时自Epoch以来的毫秒值时间
    &quot;EventReceivedCount&quot;:&quot;28286&quot;,    //目前为止source已经接收到的事件总数量
    &quot;AppendAcceptedCount&quot;:&quot;0&quot;        //单独传入的事件到Channel且成功返回的事件总数量
},
&quot;CHANNEL.ch-1&quot;:{
    &quot;EventPutSuccessCount&quot;:&quot;28286&quot;,    //成功写入channel且提交的事件总数量
    &quot;ChannelFillPercentage&quot;:&quot;0.0&quot;,    //channel满时的百分比，重点就在这个值，一般情况下，这个值小于0.01就代表很通畅，sink的速度比source的速度快，如果这个值超过了5，就代表肯定是sink的速度不够快，需要对sink进行调优，或者需要控制source的速率。
    &quot;Type&quot;:&quot;CHANNEL&quot;,
    &quot;StopTime&quot;:&quot;0&quot;,            //channel停止时自Epoch以来的毫秒值时间
    &quot;EventPutAttemptCount&quot;:&quot;28286&quot;,    //Source尝试写入Channe的事件总数量
    &quot;ChannelSize&quot;:&quot;0&quot;,            //目前channel中事件的总数量
    &quot;StartTime&quot;:&quot;1442566410326&quot;,    //channel启动时自Epoch以来的毫秒值时间
    &quot;EventTakeSuccessCount&quot;:&quot;28286&quot;,    //sink成功读取的事件的总数量
    &quot;ChannelCapacity&quot;:&quot;1000000&quot;,       //channel的容量
    &quot;EventTakeAttemptCount&quot;:&quot;313734329512&quot; //sink尝试从channel拉取事件的总数量。这不意味着每次事件都被返回，因为sink拉取的时候channel可能没有任何数据
},
&quot;SINK.sink-1&quot;:{
    &quot;Type&quot;:&quot;SINK&quot;,
    &quot;ConnectionClosedCount&quot;:&quot;0&quot;,    //下一阶段或存储系统关闭的连接数量(如在HDFS中关闭一个文件)
    &quot;EventDrainSuccessCount&quot;:&quot;28286&quot;,    //sink成功写出到存储的事件总数量
    &quot;KafkaEventSendTimer&quot;:&quot;482493&quot;,    
    &quot;BatchCompleteCount&quot;:&quot;0&quot;,        //与最大批量尺寸相等的批量的数量
    &quot;ConnectionFailedCount&quot;:&quot;0&quot;,    //下一阶段或存储系统由于错误关闭的连接数量（如HDFS上一个新创建的文件因为超时而关闭）
    &quot;EventDrainAttemptCount&quot;:&quot;0&quot;,    //sink尝试写出到存储的事件总数量
    &quot;ConnectionCreatedCount&quot;:&quot;0&quot;,    //下一个阶段或存储系统创建的连接数量（如HDFS创建一个新文件）
    &quot;BatchEmptyCount&quot;:&quot;0&quot;,        //空的批量的数量，如果数量很大表示souce写数据比sink清理数据慢速度慢很多
    &quot;StopTime&quot;:&quot;0&quot;,            
    &quot;RollbackCount&quot;:&quot;9&quot;,            //
    &quot;StartTime&quot;:&quot;1442566411897&quot;,
    &quot;BatchUnderflowCount&quot;:&quot;0&quot;        //比sink配置使用的最大批量尺寸更小的批量的数量，如果该值很高也表示sink比souce更快
}
}
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[桂花真香]]></title>
      <url>http://flume.cn/2009/09/29/%E6%A1%82%E8%8A%B1%E7%9C%9F%E9%A6%99/</url>
      <content type="html"><![CDATA[<p>夏已消逝，秋意更浓，人们穿上了久违的长袖，大雁不时成群在天上飞过，桂花香也飘满了校园。</p>
<p>没来过南方的人永远想像不到桂花的香味，家在南方的人也不会感觉到桂花香的震撼，他们已经习惯了，因为很多东西当人们拥有时都不会去珍惜它。从北方来求学的我，站在秋天桂花香的风中，当多愁善感的人碰上多愁善感的事，不免得要多愁善感一番。</p>
<p>虽然家乡那边也有花，我也故作浪漫地闻过，但说实话我真没闻到过花香。当我来到南方，闻到桂花香时，才真正明白了花香的含义。桂花在暮夏时节就有了，那时只是一抹抹，一缕缕的，花香像调皮的小天使，随风飘荡，不经意间飞进鼻孔，当你使劲闻的时候，它却调皮地飞走了。而现在，大批的桂花跟随着秋姑娘的脚步轻轻走来，微风轻轻一吹，就会闻到浓浓的清香，”满城尽飘桂花香”，使劲闻吧！不必害怕把它惊走，多浪漫啊！我的心都醉了。</p>
<p>在桂花香的风中奔跑，路人用奇怪的眼光看我，我不在乎，应为我已沉浸在桂花香风中。真想化成风，一股微蓝的风，让别人看不见我，自由自在地在空中飞翔，空中陶醉，然后睡在桂花中。看！多小的桂花呀，一个个黄色的小花，像一个个黄色的小铃铛，在风的吹佛下，奏出悦耳的浪漫诗!</p>
<p>真想化成风，载着桂花香，飞到天上，飞到桂花树下私语的情侣旁，多浪漫的爱情啊!我为他们送去香甜；</p>
<p>真想化成风，飞到通渭，将桂花香带到家里，我轻轻触摸爸爸的脸，闻着妈妈洗完衣服的手上的肥皂香味，看着可爱的弟弟学习时的傻样,轻轻抚摸他柔软的头发；</p>
<p>真想化成风，飞到通渭一中，飞到教室里,为那些埋头苦读的兄弟们送去桂花香，真心祝福你们；</p>
<p>真想化成风，载着桂花香，飞到北京，西安，长春……飞到大学里和我一样离家的朋友旁，让他们也闻到桂花香，为他们驱散孤独。</p>
<p>如果可以，我会化成风，飞到一中，飞到200天以前的高三（九）班，看着那个坐在第二排戴眼镜的可爱男生，飞到他耳边，轻轻告诉他：</p>
<pre><code>&quot;勇敢点，不要让美好的高中留下遗憾。&quot;
&quot;乖点，再不要让父母生气了。&quot;
&quot;大度点，不要再与弟弟争了&quot;
......
</code></pre><p>真想化成风，一股微蓝的风，是秋天里浪漫的风，畅游在花香的海洋，载着花香，含着泪，在无人的时候，一个人独自飘。</p>
<p>——2009年秋</p>
<p>——安大新区</p>
]]></content>
    </entry>
    
  
  
    
    <entry>
      <title></title>
      <url>http://flume.cn/about/index.html</url>
      <content type="html"><![CDATA[<p>hi，你好</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[timeline]]></title>
      <url>http://flume.cn/timeline/index.html</url>
      <content type="html"></content>
    </entry>
    
  
</search>
