<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[livy-server初探1——总体介绍与提交脚本]]></title>
      <url>http://flume.cn/2016/09/13/livy-server%E5%88%9D%E6%8E%A2/</url>
      <content type="html"><![CDATA[<p><a href="http://livy.io/" target="_blank" rel="external">Livy server</a>是针对Spark的开源的REST接口，使得我们可以通过REST接口来实现与Spark交互,之前应该是Hue框架的一个功能模块，现在已经独立出来啦。具有如下功能：<br>1） 可以与scala、python、R shell客户端交互，执行一些代码片段<br>2） 可以提交整个Spark Job,支持scala、python、java编写的Spark job。</p>
<h2 id="Welcome-to-Livy"><a href="#Welcome-to-Livy" class="headerlink" title="Welcome to Livy"></a>Welcome to Livy</h2><p>下面是官网文档中我对 Welcome to Livy的翻译：</p>
<p>Livy通过提供REST服务来简化与Spark集群的交互。它可以通过job或者代码片段的方式来提交Spark任务，并同步或者异步地获得任务的结果，以及管理spark context，上述功能通过简单的REST接口或者RPC服务来实现。livy也可以简化Spark与一些应用程序之间的交互，使得Spark可以用于一些web应用(比如Hue)。更多的功能包括：</p>
<ul>
<li>拥有长期运行的Spark Contexts供多用户提交各种的Spark job；</li>
<li>不同的任务和用户可以共享cached RDD或者DataFrames；</li>
<li>多个SC可以按计划同时运行，为了使得SC具有更好的容错性和并发性，可以将SC运行在yarn/Mesos等集群中；</li>
<li>可以通过java/scala客户端的API来提交预编译好的jar包或代码片段</li>
<li>支持一定的安全机制</li>
<li>Apache-licensed 100%开源</li>
</ul>
<p>与ReadMe中的文档结合再补充几条：</p>
<ul>
<li>支持Scala，Python，R Shell的交互；</li>
<li>支持 Scala，Java，Python的批量提交；</li>
<li>不需要你对你自己的代码增加任何改变；</li>
</ul>
<p>官网和github逛了一整子后不禁感叹，新东西总是缺乏底层的文档的，所以要了解它就要阅读源码了。</p>
<h2 id="从-bin-livy-server进入"><a href="#从-bin-livy-server进入" class="headerlink" title="从./bin/livy-server进入"></a>从./bin/livy-server进入</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">usage=<span class="string">"Usage: livy-server (start|stop)"</span></div><div class="line"></div><div class="line"><span class="comment"># 指定LIVY_HOME与LIVY_CONF_DIR，上述 `export LIVY_HOME=$(cd $(dirname $0)/.. &amp;&amp; pwd)`这种写法值得学习，代表将LIVY_HOME环境变量设为本脚本的父目录，通过这种写法，增强了脚本的可移植性，另外注明一点，dirname这个命令在命令行里是不能用的，只有写在脚本中才能起作用。</span></div><div class="line"><span class="built_in">export</span> LIVY_HOME=$(<span class="built_in">cd</span> $(dirname <span class="variable">$0</span>)/.. &amp;&amp; <span class="built_in">pwd</span>)</div><div class="line">LIVY_CONF_DIR=<span class="variable">$&#123;LIVY_CONF_DIR:-"$LIVY_HOME/conf"&#125;</span></div><div class="line"></div><div class="line"><span class="comment"># 运行所有的livy-env.sh中的环境变量，并使用set -a 表示输出所有的环境变量的改变</span></div><div class="line"><span class="keyword">if</span> [ <span class="_">-f</span> <span class="string">"<span class="variable">$&#123;LIVY_CONF_DIR&#125;</span>/livy-env.sh"</span> ]; <span class="keyword">then</span></div><div class="line">  <span class="comment"># Promote all variable declarations to environment (exported) variables</span></div><div class="line">  <span class="built_in">set</span> <span class="_">-a</span></div><div class="line">  . <span class="string">"<span class="variable">$&#123;LIVY_CONF_DIR&#125;</span>/livy-env.sh"</span></div><div class="line">  <span class="built_in">set</span> +a</div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure>
<p>接下来可以看到调用了 start_livy_server，以及stop的代码(其实就是ps -p到livy的那个进程，然后kill掉，值得借鉴)：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">option=<span class="variable">$1</span></div><div class="line"></div><div class="line"><span class="keyword">case</span> <span class="variable">$option</span> <span class="keyword">in</span></div><div class="line"></div><div class="line">  (start)</div><div class="line">    start_livy_server <span class="string">"new"</span></div><div class="line">    ;;</div><div class="line"></div><div class="line">  (<span class="string">""</span>)</div><div class="line">    <span class="comment"># make it compatible with previous version of livy-server</span></div><div class="line">    start_livy_server <span class="string">"old"</span></div><div class="line">    ;;</div><div class="line"></div><div class="line">  (stop)</div><div class="line">    <span class="keyword">if</span> [ <span class="_">-f</span> <span class="variable">$pid</span> ]; <span class="keyword">then</span></div><div class="line">      TARGET_ID=<span class="string">"<span class="variable">$(cat "$pid")</span>"</span></div><div class="line">      <span class="keyword">if</span> [[ $(ps -p <span class="string">"<span class="variable">$TARGET_ID</span>"</span> -o comm=) =~ <span class="string">"java"</span> ]]; <span class="keyword">then</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"stopping livy_server"</span></div><div class="line">        <span class="built_in">kill</span> <span class="string">"<span class="variable">$TARGET_ID</span>"</span> &amp;&amp; rm <span class="_">-f</span> <span class="string">"<span class="variable">$pid</span>"</span></div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"no livy_server to stop"</span></div><div class="line">      <span class="keyword">fi</span></div><div class="line">    <span class="keyword">else</span></div><div class="line">      <span class="built_in">echo</span> <span class="string">"no livy_server to stop"</span></div><div class="line">    <span class="keyword">fi</span></div><div class="line">    ;;</div><div class="line"></div><div class="line">  (*)</div><div class="line">    <span class="built_in">echo</span> <span class="variable">$usage</span></div><div class="line">    <span class="built_in">exit</span> 1</div><div class="line">    ;;</div><div class="line"></div><div class="line"><span class="keyword">esac</span></div></pre></td></tr></table></figure>
<p>接下来就是start_livy_server函数了，它做了下面几件事情：</p>
<ol>
<li>找到livy的jar包；</li>
<li>设置LIVY_CLASSPATH并将SPARK与HADOOP以及YARN的CONF_DIR加入到classpath中；</li>
<li>如果是<code>./bin/livy-server</code>启动的程序，就直接运行 “$RUNNER $LIVY_SERVER_JAVA_OPTS -cp $LIVY_CLASSPATH:$CLASSPATH com.cloudera.livy.server.LivyServer”</li>
<li>如果是<code>./bin/livy-server start</code>启动的程序，则增加了日志记录，以方便查看，所以推荐新版本使用带start参数的方式</li>
</ol>
<h2 id="com-cloudera-livy-server-LivyServer"><a href="#com-cloudera-livy-server-LivyServer" class="headerlink" title="com.cloudera.livy.server.LivyServer"></a>com.cloudera.livy.server.LivyServer</h2><p>从后面进入：原来是创建了一个 LivyServer的server，然后start和join启动</p>
<figure class="highlight scala"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">LivyServer</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">val</span> server = <span class="keyword">new</span> <span class="type">LivyServer</span>()</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      server.start()</div><div class="line">      server.join()</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">      server.stop()</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="LiveServer的属性"><a href="#LiveServer的属性" class="headerlink" title="LiveServer的属性"></a>LiveServer的属性</h3><p>LivyServer的属性不多，（与spark源码相比）：</p>
<figure class="highlight scala"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> <span class="type">LivyConf</span>._</div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">var</span> server: <span class="type">WebServer</span> = _</div><div class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _serverUrl: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></div><div class="line">  <span class="comment">// make livyConf accessible for testing</span></div><div class="line">  <span class="keyword">private</span>[livy] <span class="keyword">var</span> livyConf: <span class="type">LivyConf</span> = _</div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">var</span> kinitFailCount: <span class="type">Int</span> = <span class="number">0</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">var</span> executor: <span class="type">ScheduledExecutorService</span> = _</div></pre></td></tr></table></figure>
<h3 id="start-函数"><a href="#start-函数" class="headerlink" title="start()函数"></a>start()函数</h3><p>然后是start()函数了<br>首先，从配置文件中读取配置信息（这一块内容自己写得时候可以借用）：</p>
<ul>
<li>从配置信息中得到 host和port</li>
</ul>
<figure class="highlight scala"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">livyConf = <span class="keyword">new</span> <span class="type">LivyConf</span>().loadFromFile(<span class="string">"livy.conf"</span>)</div><div class="line"><span class="keyword">val</span> host = livyConf.get(<span class="type">SERVER_HOST</span>)</div><div class="line"><span class="keyword">val</span> port = livyConf.getInt(<span class="type">SERVER_PORT</span>)</div><div class="line"># 这个而没有看懂</div><div class="line"><span class="keyword">val</span> multipartConfig = <span class="type">MultipartConfig</span>(</div><div class="line">    maxFileSize = <span class="type">Some</span>(livyConf.getLong(<span class="type">LivyConf</span>.<span class="type">FILE_UPLOAD_MAX_SIZE</span>))</div><div class="line">  ).toMultipartConfigElement</div></pre></td></tr></table></figure>
<ul>
<li>测试SparkHome是否设置成功</li>
</ul>
<p>如下代码，这里使用了require方法对参数进行先决条件检测(值得借鉴)<br><figure class="highlight scala"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Make sure the `spark-submit` program exists, otherwise much of livy won't work.</span></div><div class="line">testSparkHome(livyConf)</div><div class="line">...</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">* Sets the spark-submit path if it's not configured in the LivyConf</div><div class="line">*/</div><div class="line"><span class="keyword">private</span>[server] <span class="function"><span class="keyword">def</span> <span class="title">testSparkHome</span></span>(livyConf: <span class="type">LivyConf</span>): <span class="type">Unit</span> = &#123;</div><div class="line"><span class="keyword">val</span> sparkHome = livyConf.sparkHome().getOrElse &#123;</div><div class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"Livy requires the SPARK_HOME environment variable"</span>)</div><div class="line">&#125;</div><div class="line"></div><div class="line">require(<span class="keyword">new</span> <span class="type">File</span>(sparkHome).isDirectory(), <span class="string">"SPARK_HOME path does not exist"</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<ul>
<li>测试spark-submit命令可用，否则livy无法工作(值得借鉴):</li>
</ul>
<p>这里的代码写得太精彩了！先是定义一个<code>$SPAKR_HOME/bin/spark-sumbit --version</code>的命令，使用java的ProcessBuilder，然后可以得到exitCode和重定向的标准输出结果，如果结果是”version …”的话，就代表执行成功，输出结果；然后对这个version进行正则匹配，如果是1.6到2.0版本之间，就返回true，否则就说明spark版本不支持；</p>
<figure class="highlight scala"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line">testSparkSubmit(livyConf)</div><div class="line">...</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">* Test that the configured `spark-submit` executable exists.</div><div class="line">*</div><div class="line">* @param livyConf</div><div class="line">*/</div><div class="line"><span class="keyword">private</span>[server] <span class="function"><span class="keyword">def</span> <span class="title">testSparkSubmit</span></span>(livyConf: <span class="type">LivyConf</span>): <span class="type">Unit</span> = &#123;</div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">  testSparkVersion(sparkSubmitVersion(livyConf))</div><div class="line">&#125; <span class="keyword">catch</span> &#123;</div><div class="line">  <span class="keyword">case</span> e: <span class="type">IOException</span> =&gt;</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">"Failed to run spark-submit executable"</span>, e)</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">...</div><div class="line"><span class="comment">/**</span></div><div class="line">* Return the version of the configured `spark-submit` version.</div><div class="line">*</div><div class="line">* @param livyConf</div><div class="line">* @return the version</div><div class="line">*/</div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">sparkSubmitVersion</span></span>(livyConf: <span class="type">LivyConf</span>): <span class="type">String</span> = &#123;</div><div class="line"><span class="keyword">val</span> sparkSubmit = livyConf.sparkSubmit()</div><div class="line"><span class="keyword">val</span> pb = <span class="keyword">new</span> <span class="type">ProcessBuilder</span>(sparkSubmit, <span class="string">"--version"</span>)</div><div class="line">pb.redirectErrorStream(<span class="literal">true</span>)</div><div class="line">pb.redirectInput(<span class="type">ProcessBuilder</span>.<span class="type">Redirect</span>.<span class="type">PIPE</span>)</div><div class="line"></div><div class="line"><span class="keyword">if</span> (<span class="type">LivyConf</span>.<span class="type">TEST_MODE</span>) &#123;</div><div class="line">  pb.environment().put(<span class="string">"LIVY_TEST_CLASSPATH"</span>, sys.props(<span class="string">"java.class.path"</span>))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">val</span> process = <span class="keyword">new</span> <span class="type">LineBufferedProcess</span>(pb.start())</div><div class="line"><span class="keyword">val</span> exitCode = process.waitFor()</div><div class="line"><span class="keyword">val</span> output = process.inputIterator.mkString(<span class="string">"\n"</span>)</div><div class="line"></div><div class="line"><span class="keyword">val</span> regex = <span class="string">""</span><span class="string">"version (.*)"</span><span class="string">""</span>.r.unanchored</div><div class="line"></div><div class="line">output <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> regex(version) =&gt; version</div><div class="line">  <span class="keyword">case</span> _ =&gt;</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">f"Unable to determine spark-submit version [<span class="subst">$exitCode</span>]:\n<span class="subst">$output</span>"</span>)</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">* Throw an exception if Spark version is not supported.</div><div class="line">* @param version Spark version</div><div class="line">*/</div><div class="line"><span class="keyword">private</span>[server] <span class="function"><span class="keyword">def</span> <span class="title">testSparkVersion</span></span>(version: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</div><div class="line"><span class="keyword">val</span> versionPattern = <span class="string">""</span><span class="string">"(\d)+\.(\d)+(?:\.\d*)?"</span><span class="string">""</span>.r</div><div class="line"><span class="comment">// This is exclusive. Version which equals to this will be rejected.</span></div><div class="line"><span class="keyword">val</span> maxVersion = (<span class="number">2</span>, <span class="number">0</span>)</div><div class="line"><span class="keyword">val</span> minVersion = (<span class="number">1</span>, <span class="number">6</span>)</div><div class="line"></div><div class="line"><span class="keyword">val</span> supportedVersion = version <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> versionPattern(major, minor) =&gt;</div><div class="line">    <span class="keyword">val</span> v = (major.toInt, minor.toInt)</div><div class="line">    v &gt;= minVersion &amp;&amp; v &lt; maxVersion</div><div class="line">  <span class="keyword">case</span> _ =&gt; <span class="literal">false</span></div><div class="line">&#125;</div><div class="line">require(supportedVersion, <span class="string">s"Unsupported Spark version <span class="subst">$version</span>."</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark在Kerberos环境下指定任意用户在yarn上提交任务]]></title>
      <url>http://flume.cn/2016/09/08/Spark%E5%9C%A8Kerberos%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%8C%87%E5%AE%9A%E4%BB%BB%E6%84%8F%E7%94%A8%E6%88%B7%E5%9C%A8yarn%E4%B8%8A%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1/</url>
      <content type="html"><![CDATA[<p>众所周知，Spark在Kerberos环境下提交任务有两种方式，分别是先kinit的方式和通过 –keytab的方式：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[op]$ spark-submit --keytab <span class="built_in">test</span>Jars/op.keytab --principal op --master <span class="built_in">local</span> --class SparkPi ./<span class="built_in">test</span>Jars/my.jar 4</div></pre></td></tr></table></figure>
<p>Spark在Kerberos环境下可以在提交任务时通过指定用户的keytab和principal来提交任务，比如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 事先进行kinit的方式</span></div><div class="line">[op]$ kinit -kt op.keytab op</div><div class="line">[op]$ spark-submit --master <span class="built_in">local</span> --class SparkPi ./<span class="built_in">test</span>Jars/my.jar 4</div><div class="line"></div><div class="line"><span class="comment"># 提交keytab的方式</span></div><div class="line">[op]$ spark-submit --keytab <span class="built_in">test</span>Jars/op.keytab --principal op --master <span class="built_in">local</span> --class SparkPi ./<span class="built_in">test</span>Jars/my.jar 4</div></pre></td></tr></table></figure>
<p>其实还可以模拟其它用户的方式提交任务，比如使用ts账户提交：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[op]$ spark-submit --keytab <span class="built_in">test</span>Jars/ts.keytab --principal ts@HADOOP.CHINATELECOM.CN --master <span class="built_in">local</span> --class SparkPi ./<span class="built_in">test</span>Jars/my.jar 4</div></pre></td></tr></table></figure>
<p>当然没有那么简单，如果想要使用ts账户执行程序，需要进行如下设置：</p>
<h4 id="模拟其它用户需要的条件"><a href="#模拟其它用户需要的条件" class="headerlink" title="模拟其它用户需要的条件"></a>模拟其它用户需要的条件</h4><ol>
<li>ts要在KDC下生成对应的keytab和principal；</li>
<li>要在hadoop集群的所有机器上创建ts账户：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo groupadd ts</div><div class="line">sudo useradd -g ts ts</div></pre></td></tr></table></figure>
<p>值得注意的是，如果要在yarn中模拟其它用户执行，需要在集群中所有机器上增加该用户。</p>
<p>后期有时间了详细说明原因。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark Web与history web配置与测试]]></title>
      <url>http://flume.cn/2016/09/07/Spark-Web%E4%B8%8Ehistory%E6%B5%8B%E8%AF%95/</url>
      <content type="html"><![CDATA[<h2 id="Spark-Web的查看"><a href="#Spark-Web的查看" class="headerlink" title="Spark Web的查看"></a>Spark Web的查看</h2><ol>
<li>运行任意一个yarn-client或者yarn-cluster模式的spark测试用例</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">cd</span> <span class="variable">$SPARK_HOME</span></div><div class="line">$ spark-submit --keytab <span class="built_in">test</span>Jars/op.keytab --principal op --master yarn-client --class SparkPi ./<span class="built_in">test</span>Jars/my.jar 4</div></pre></td></tr></table></figure>
<ol>
<li>打开<a href="http://yarn-host:8088/cluster页面，找到正在运行的Spark测试用例" target="_blank" rel="external">http://yarn-host:8088/cluster页面，找到正在运行的Spark测试用例</a></li>
</ol>
<img src="/2016/09/07/Spark-Web与history测试/spark-yarn-web1.png" alt="spark-yarn-web1.png" title="">
<p>点击上图所示的AM，就进入了Spark的Web界面：下图就是Spark程序的web界面，值得注意的是，这个web界面会随着spark程序的运行结束而消失<br><img src="/2016/09/07/Spark-Web与history测试/spark-yarn-web2.png" alt="spark-yarn-web2.png" title=""></p>
<h2 id="Spark-history-Web查看测试"><a href="#Spark-history-Web查看测试" class="headerlink" title="Spark history Web查看测试"></a>Spark history Web查看测试</h2><p>在Kerberos环境下要启动spark history配置，需要在 spark -env下面开启如下配置 SPARK_HISTORY_OPTS：</p>
<figure class="highlight bash"><figcaption><span>spark-env.sh</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># history需要的Kerberos配置</span></div><div class="line">SPARK_HISTORY_OPTS=<span class="string">"-Dspark.history.ui.port=8777 -Dspark.history.retainedApplications=10 -Dspark.history.fs.logDirectory=hdfs://ns/user/op/sparkHistoryServer -Dspark.history.kerberos.enabled=true -Dspark.history.kerberos.principal=op @HADOOP.CHINATELECOM.CN -Dspark.history.kerberos.keytab=/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/conf/op.keytab"</span></div></pre></td></tr></table></figure>
<p>然后通过 ./sbin/start-history-server.sh 命令启动history-server<br>然后登录 <a href="http://spark-client-ip:8777/" target="_blank" rel="external">http://spark-client-ip:8777/</a> 即可查看 spark-history-web</p>
<img src="/2016/09/07/Spark-Web与history测试/spark-yarn-web3.png" alt="spark-yarn-web3.png" title="">]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[kerberos下spark客户端的配置]]></title>
      <url>http://flume.cn/2016/09/06/kerberos%E4%B8%8B%E6%94%AF%E6%8C%81snappy%E5%8E%8B%E7%BC%A9%E7%9A%84spark%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%9A%84%E9%85%8D%E7%BD%AE/</url>
      <content type="html"><![CDATA[<p>Kerberos环境下spark的客户端配置并不是很多，主要需要配置的是spark-history与spark-sql</p>
<p>软件版本：spark-1.6.2</p>
<p>注：正式环境中，需要将spark客户端的路径放入其它短路经，比如 /etc/local/spark 等<br><figure class="highlight bash"><figcaption><span>spark-env.sh</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 由于此处的 hive-site.xml 需要做一定修改，所以需要将hive-site.xml core-site.xml hdfs-site.xml yarn-site.xml等导入conf文件夹下</span></div><div class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/conf</div><div class="line"></div><div class="line"><span class="built_in">export</span> JAVA_HOME=/usr/java/jdk1.7.0_75</div><div class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$LD_LIBRARY_PATH</span>:/usr/lib/hadoop/lib/native/</div><div class="line"><span class="built_in">export</span> SPARK_LIBRARY_PATH=/usr/lib/hadoop/lib/native/:<span class="variable">$SPARK_LIBRARY_PATH</span></div><div class="line"></div><div class="line"><span class="comment"># history需要的Kerberos配置</span></div><div class="line">SPARK_HISTORY_OPTS=<span class="string">"-Dspark.history.ui.port=8777 -Dspark.history.retainedApplications=10 -Dspark.history.fs.logDirectory=hdfs://ns/user/op/sparkHistoryServer -Dspark.history.kerberos.enabled=true -Dspark.history.kerberos.principal=op    @HADOOP.CHINATELECOM.CN -Dspark.history.kerberos.keytab=/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/conf/op.keytab"</span></div></pre></td></tr></table></figure></p>
<h4 id="从hive-keytab-hiveserver创建spark-thrift-server的keytab"><a href="#从hive-keytab-hiveserver创建spark-thrift-server的keytab" class="headerlink" title="从hive.keytab_hiveserver创建spark-thrift-server的keytab"></a>从hive.keytab_hiveserver创建spark-thrift-server的keytab</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">-rw------- 1 hive hive     424 8月  23 09:55 hive.keytab_hiveserver</div><div class="line">-rw------- 1 op   bigdata  424 9月   3 12:25 hive.keytab_sparkthrift</div></pre></td></tr></table></figure>
<h4 id="hive-site的配置"><a href="#hive-site的配置" class="headerlink" title="hive-site的配置"></a>hive-site的配置</h4><p>修改hive-site.xml：</p>
<ul>
<li>增加hive.server2.thrift.bind.host</li>
<li>修改hive.server2.thrift.port为10010</li>
<li>修改hive.server2.authentication.kerberos.keytab为如下</li>
</ul>
<figure class="highlight bash"><figcaption><span>hive-site.xml</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">145 &lt;!-- ZooKeeper conf--&gt;</div><div class="line">146 &lt;property&gt;</div><div class="line">147   &lt;name&gt;hive.server2.enable.doAs&lt;/name&gt;</div><div class="line">148   &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</div><div class="line">149   &lt;description&gt; Impersonate the connected user &lt;/description&gt;</div><div class="line">150 &lt;/property&gt;</div><div class="line">151 &lt;property&gt;</div><div class="line">152   &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</div><div class="line">153   &lt;value&gt;10010&lt;/value&gt;</div><div class="line">154   &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class="line">155 &lt;/property&gt;</div><div class="line">156 </div><div class="line">157 &lt;property&gt;</div><div class="line">158    &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</div><div class="line">159    &lt;value&gt;<span class="built_in">test</span>-bdd-076&lt;/value&gt;</div><div class="line">160    &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class="line">161  &lt;/property&gt;</div><div class="line">162 </div><div class="line">163 &lt;property&gt;</div><div class="line">164   &lt;name&gt;hive.metastore.execute.setugi&lt;/name&gt;</div><div class="line">165   &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</div><div class="line">166 &lt;/property&gt;</div><div class="line">...</div><div class="line">209 &lt;property&gt;</div><div class="line">210    &lt;name&gt;hive.server2.authentication.kerberos.principal&lt;/name&gt;</div><div class="line">211    &lt;value&gt;hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN&lt;/value&gt;</div><div class="line">212  &lt;/property&gt;</div><div class="line">213 &lt;property&gt;</div><div class="line">214   &lt;name&gt;hive.server2.authentication.kerberos.keytab&lt;/name&gt;</div><div class="line">215   &lt;value&gt;/etc/hive/conf/hive.keytab_sparkthrift&lt;/value&gt;</div><div class="line">216 &lt;/property&gt;</div></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[在Kerberos环境下配置hue通过spark-thrift-server访问SparkSql]]></title>
      <url>http://flume.cn/2016/09/05/%E5%9C%A8Kerberos%E7%8E%AF%E5%A2%83%E4%B8%8B%E9%85%8D%E7%BD%AEhue%E9%80%9A%E8%BF%87spark-thrift-server%E8%AE%BF%E9%97%AESparkSql/</url>
      <content type="html"><![CDATA[<p>hue-spark-thriftserver-kerberos</p>
<h3 id="背景说明"><a href="#背景说明" class="headerlink" title="背景说明"></a>背景说明</h3><p>Kerberos项目最后要对基于Hue的TODP平台进行安全测试，在搭建配置的过程中踩了一些坑，现在把其中的配置与步骤进行总结，以免以后忘记。</p>
<p>其中用到以下代号：<br>40机器：hue平台所在的机器<br>76机器：spark thrift服务端口10010，hive-thrift-server服务端口10000<br>74机器：spark thrift服务端口10010，hive-thrift-server服务端口10000<br>TEST-BDD-HIVESERVER机器：负载均衡所在的机器，负载均衡机器需要配合开启10000和10010端口</p>
<p>在kerberos认证下, sparksql的thriftserver连接hiveserver2变得相对复杂，主要是因为各种kerberos认证出现各种问题。后来由于hive使用了负载均衡，所以spark-sql也需加入负载均衡，否则不能使用，就是这个负载均衡服务器的加入使得kerberos认证变得更加复杂，使得不明原理的新手在配置kerberos的keytab与principal时各种不匹配。这里是通过Hue可视化界面调用后台的sparksql,然后sparksql通过JDBC连接Hive的hiveServer2服务。</p>
<h3 id="40机器hue端配置"><a href="#40机器hue端配置" class="headerlink" title="40机器hue端配置"></a>40机器hue端配置</h3><p>进入40机器hue所在的目录<br><figure class="highlight bash"><figcaption><span>hue.ini</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">cd</span> /usr/lib/hue/ </div><div class="line">$ vim desktop/conf/hue.ini</div></pre></td></tr></table></figure></p>
<p>修改hue的配置文件如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">1119 [spark]</div><div class="line">...</div><div class="line">1134   <span class="comment"># spark-sql config</span></div><div class="line">1135   spark_sql_server_host=TEST-BDD-HIVESERVER</div><div class="line">1136   <span class="comment">## spark_sql_server_port=10010</span></div></pre></td></tr></table></figure></p>
<p>由于此处使用了负载均衡，所以上述TEST-BDD-HIVESERVER指向的是负载均衡所在的ip，最终会转发给两个spark-thrift-server</p>
<h3 id="Kerberos服务器端配置"><a href="#Kerberos服务器端配置" class="headerlink" title="Kerberos服务器端配置"></a>Kerberos服务器端配置</h3><p>生成类似 hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN 的keytab，配置了负载均衡后，使用test-bdd-hiveserver</p>
<h3 id="76机器上的配置"><a href="#76机器上的配置" class="headerlink" title="76机器上的配置"></a>76机器上的配置</h3><p>76机器与74机器配置步骤一样，只是hive-site.xml需要改一处，将下面的 076改成 074即可<br><figure class="highlight bash"><figcaption><span>hive-site.xml</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">   &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</div><div class="line">   &lt;value&gt;<span class="built_in">test</span>-bdd-076&lt;/value&gt;</div><div class="line">   &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure></p>
<p>其它都一样，所以在这里只写076的配置步骤</p>
<h4 id="从hive-keytab创建spark的keytab"><a href="#从hive-keytab创建spark的keytab" class="headerlink" title="从hive.keytab创建spark的keytab"></a>从hive.keytab创建spark的keytab</h4><p>然后在/etc/hive/conf/下创建spark需要的keytab，在这里使用hiveserver的keytab，将已有的hive.keytab_hiveserver 拷贝成 hive.keytab_sparkthrift，然后修改权限如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">-rw------- 1 hive hive     424 8月  23 09:55 hive.keytab_hiveserver</div><div class="line">-rw------- 1 op   bigdata  424 9月   3 12:25 hive.keytab_sparkthrift</div></pre></td></tr></table></figure>
<p>修改好后用如下命令检查：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">$ sudo klist -k hive.keytab_sparkthrift </div><div class="line">Keytab name: FILE:hive.keytab_sparkthrift</div><div class="line">KVNO Principal</div><div class="line">---- --------------------------------------------------------------------------</div><div class="line">   1 hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div><div class="line">   1 hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div><div class="line">   1 hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div><div class="line">   1 hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div><div class="line">   1 hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div></pre></td></tr></table></figure>
<p>如果klist是如上结果，就对了</p>
<h4 id="配置spark需要的hive-site-xml"><a href="#配置spark需要的hive-site-xml" class="headerlink" title="配置spark需要的hive-site.xml"></a>配置spark需要的hive-site.xml</h4><p>由于需要修改hive的一些配置，进入76机器spark所在的目录，将<code>/etc/hive/conf/</code>下的<code>hive-site.xml</code>拷贝到spark的conf下，赋予权限并修改<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ sudo cp /etc/hive/conf/hive-site.xml <span class="variable">$SPARK_HOME</span>/conf/</div><div class="line">$ <span class="built_in">cd</span> <span class="variable">$SPARK_HOME</span></div><div class="line">$ sudo chmod op conf/hive-site.xml</div><div class="line">$ vim conf/hive-site.xml</div></pre></td></tr></table></figure></p>
<p>修改hive-site.xml,增加hive.server2.thrift.bind.host</p>
<figure class="highlight bash"><figcaption><span>hive-site.xml</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">&lt;!-- ZooKeeper conf--&gt;</div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;hive.server2.enable.doAs&lt;/name&gt;</div><div class="line">  &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</div><div class="line">  &lt;description&gt; Impersonate the connected user &lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</div><div class="line">  &lt;value&gt;10010&lt;/value&gt;</div><div class="line">  &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line">&lt;property&gt;</div><div class="line">   &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</div><div class="line">   &lt;value&gt;<span class="built_in">test</span>-bdd-076&lt;/value&gt;</div><div class="line">   &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class="line"> &lt;/property&gt;</div><div class="line"></div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;hive.metastore.execute.setugi&lt;/name&gt;</div><div class="line">  &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">   &lt;name&gt;hive.server2.authentication.kerberos.principal&lt;/name&gt;</div><div class="line">   &lt;value&gt;hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN&lt;/value&gt;</div><div class="line"> &lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;hive.server2.authentication.kerberos.keytab&lt;/name&gt;</div><div class="line">  &lt;value&gt;/etc/hive/conf/hive.keytab_sparkthrift&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line"><span class="comment">#### 启动Spark-thrift-server</span></div><div class="line">``` bash</div><div class="line">$ <span class="built_in">cd</span> <span class="variable">$SPARK_HOME</span></div><div class="line">$ ./sbin/start-thriftserver.sh</div></pre></td></tr></table></figure>
<p>可以通过如下日志查看是否启动成功：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ vim logs/spark-op-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-TEST-BDD-076.out</div></pre></td></tr></table></figure></p>
<p>启动成功会看到如下日志:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"> 96 16/09/05 13:41:25 INFO AbstractService: Service:HiveServer2 is started.</div><div class="line"> 97 16/09/05 13:41:25 INFO HiveThriftServer2: HiveThriftServer2 started</div><div class="line"> 98 16/09/05 13:41:25 INFO UserGroupInformation: Login successful <span class="keyword">for</span> user hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN using keytab file /etc/hive/conf/hive.keytab_sparkthrift</div><div class="line"> 99 16/09/05 13:41:25 INFO AbstractDelegationTokenSecretManager: Updating the current master key <span class="keyword">for</span> generating delegation tokens</div><div class="line">100 16/09/05 13:41:25 INFO TokenStoreDelegationTokenSecretManager: New master key with key id=0</div><div class="line">101 16/09/05 13:41:25 INFO TokenStoreDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)</div><div class="line">102 16/09/05 13:41:25 INFO AbstractDelegationTokenSecretManager: Updating the current master key <span class="keyword">for</span> generating delegation tokens</div><div class="line">103 16/09/05 13:41:25 INFO TokenStoreDelegationTokenSecretManager: New master key with key id=1</div><div class="line">104 16/09/05 13:41:25 INFO ThriftCLIService: Starting ThriftBinaryCLIService on port 10010 with 5...500 worker threads</div></pre></td></tr></table></figure></p>
<h3 id="负载均衡机器的查看"><a href="#负载均衡机器的查看" class="headerlink" title="负载均衡机器的查看"></a>负载均衡机器的查看</h3><p>进入 67.121机器<br>输入 命令 <code>sudo ipvsadm -ln</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">$ sudo ipvsadm -ln</div><div class="line">IP Virtual Server version 1.2.1 (size=4194304)</div><div class="line">Prot LocalAddress:Port Scheduler Flags</div><div class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</div><div class="line">TCP  10.142.67.123:10000 wlc persistent 7200 synproxy</div><div class="line">  -&gt; 10.142.78.74:10000           FullNat 50     3          0         </div><div class="line">  -&gt; 10.142.78.76:10000           FullNat 50     0          0         </div><div class="line">TCP  10.142.67.123:10010 wlc persistent 7200 synproxy</div><div class="line">  -&gt; 10.142.78.74:10010           FullNat 50     0          0         </div><div class="line">  -&gt; 10.142.78.76:10010           FullNat 50     2          0</div></pre></td></tr></table></figure>
<p>就可以看到负载均衡的情况了：</p>
<h3 id="踩坑说明以及解决方案"><a href="#踩坑说明以及解决方案" class="headerlink" title="踩坑说明以及解决方案"></a>踩坑说明以及解决方案</h3><h4 id="缺少配置kerberos认证错误"><a href="#缺少配置kerberos认证错误" class="headerlink" title="缺少配置kerberos认证错误"></a>缺少配置kerberos认证错误</h4><p>需要在hive-site.xml文件中添加kerberos认证相关配置</p>
<h4 id="kerberos认证失败"><a href="#kerberos认证失败" class="headerlink" title="kerberos认证失败"></a>kerberos认证失败</h4><p>1)  在hive-site.xml中配置好kerberos认证，但是op用户下无法读取hive.keytab的问题，出现unable to login …given keytab/principal 以及Unable to obtain password from user。因为hive.keytab 是hive用户创建的，op用户无法读取，导致看似kerberos已经配置好，<br>但是程序没有读取权限，依旧认为没有配置好，这是会有在日志文件中会有NULLPOINT类似的错误提示，说明是没有读取权限。解决方案是复制hive.keytab到op用户下。<br>2）在hue界面连接spark时可能会出现10010端口不能连接的问题，这是sparkthrift没有启动导致的；<br>3）spark thriftserver明明已经启动，但是hue界面仍旧不能连接，出现TTransportException的错误，原因是kerberos配置没有配置正确，即没有配置kerberos认证的keytab与principal。hive/test-bdd-hiveserver必须与hive.keytab_hiveserver配套使用，同理，test-bdd-074或者 test-bdd-076必须与hive/test-bdd-74或者hive/test-bdd-76配套使用，否则出现认证失败的问题。</p>
<h4 id="hue的配置问题。"><a href="#hue的配置问题。" class="headerlink" title="hue的配置问题。"></a>hue的配置问题。</h4><p>在hue的desktop/conf目录下hue.ini文件中，主要配置spark_sql_server_host，也就是spark thriftserver所在主机，这里可以是负载均衡服务器TEST-BDD-HIVESERVER,spark_sql_server_port 是spark thriftserver的服务端口。<br>需要注意的是，加上kerberos认证后，主机名不能是ip地址的形式，需要FQDN的形式。hive的配置需要注意的是hive_server_host，这里绝对不能是hiveserver2的服务器的地址，一定是负载均衡服务器的地址，不然在hue界面连接HIVE时出现<br>Unable to access databases, Query Server or Metastore may be down.的错误以及GSS initial failed的错误，无法访问hive数据库。</p>
<h4 id="metastore的问题"><a href="#metastore的问题" class="headerlink" title="metastore的问题"></a>metastore的问题</h4><p>连接metastore也需要principal的认证。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">20 &lt;property&gt;</div><div class="line">221   &lt;name&gt;hive.metastore.sasl.enabled&lt;/name&gt;</div><div class="line">222   &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</div><div class="line">223   &lt;description&gt;If <span class="literal">true</span>, the metastore thrift interface will be secured with SASL. Clients must authenticate with Kerberos.&lt;/description&gt;</div><div class="line">224 &lt;/property&gt;</div><div class="line">225 &lt;property&gt;</div><div class="line">226   &lt;name&gt;hive.metastore.kerberos.principal&lt;/name&gt;</div><div class="line">227   &lt;value&gt;hive/_HOST@HADOOP.CHINATELECOM.CN&lt;/value&gt;</div><div class="line">228   &lt;description&gt;The service principal <span class="keyword">for</span> the metastore thrift server. The special string _HOST will be replaced automatically with the correct host name.&lt;/description&gt;</div><div class="line">229 &lt;/property&gt;</div></pre></td></tr></table></figure></p>
<p>之所以问题多多，主要原因是对kerberos+Hive+lvs整体原理没有搞清楚，以至于在配置过程中出现各种错误。我们搭建的hive集群有74,76两台主机，spark thriftserver也有74,76两台主机，负载均衡服务器在test-bdd-hiveserver上。在配置时，需要将spark-sql-server-host配置成test-bdd-hiveserver,因为对spark而言，74与76上的hiveserver是一个整体，不能配置成单一的主机，不然lvs可能会将服务分到另外一台主机上，造成主机配置失败。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[spark支持snappy压缩踩坑总结]]></title>
      <url>http://flume.cn/2016/08/15/spark%E6%94%AF%E6%8C%81snappy%E5%8E%8B%E7%BC%A9%E8%B8%A9%E5%9D%91%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<h3 id="配置snappy压缩"><a href="#配置snappy压缩" class="headerlink" title="配置snappy压缩"></a>配置snappy压缩</h3><figure class="highlight bash"><figcaption><span>spark-default.conf</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">spark.driver.extraClassPath /usr/lib/hadoop/lib/*:/usr/op/hadoop-lzo-cdh4-0.4.15-gplextras.jar </div><div class="line">spark.driver.extraClassPath /usr/lib/hadoop/lib/*</div><div class="line">spark.driver.extraLibraryPath /usr/lib/hadoop/lib/native</div><div class="line">spark.executor.extraClassPath /usr/lib/hadoop/lib/*</div><div class="line">spark.executor.extraLibraryPath /usr/lib/hadoop/lib/native</div></pre></td></tr></table></figure>
<p>如上配置，即可，但是为了得到这么小小的一点配置，浪费了三天的时间啊，网上的资料都是转载，无法解决问题。被官网坑了，官网的配置文件中并没有关于spark.executor.extraClassPath的配置，查了源码才得知，作为教训。以后出现问题要冷静思考，不要简单的去网上搜索，先判断问题出现的原因，知其所以然，必要时要去源码中查询，否则会浪费很多时间，走很多弯路。</p>
<h3 id="踩坑集锦"><a href="#踩坑集锦" class="headerlink" title="踩坑集锦"></a>踩坑集锦</h3><p>首先，会遇到这个错误：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Compression codec com.hadoop.compression.lzo.LzoCodec not found</div></pre></td></tr></table></figure>
<p>原因是spark-env.sh的配置文件缺少关联hadoop的配置语句</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> SPARK_LIBRARY_PATH=<span class="variable">$SPARK_LIBRARY_PATH</span>:/usr/lib/hadoop/lib/native/:/usr/lib/hadoop/lib/*</div></pre></td></tr></table></figure>
<p>然后yarn-cluster模式下snappy压缩总会报错：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="number">16</span>/<span class="number">08</span>/<span class="number">08</span> <span class="number">19</span>:<span class="number">05</span>:<span class="number">03</span> DEBUG util.NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib</div><div class="line"><span class="number">16</span>/<span class="number">08</span>/<span class="number">08</span> <span class="number">19</span>:<span class="number">05</span>:<span class="number">03</span> WARN util.NativeCodeLoader: Unable to load <span class="keyword">native</span>-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</div><div class="line"><span class="number">1</span>）</div><div class="line"><span class="number">16</span>/<span class="number">08</span>/<span class="number">08</span> <span class="number">19</span>:<span class="number">05</span>:<span class="number">03</span> DEBUG util.PerformanceAdvisory: Both <span class="keyword">short</span>-circuit local reads and UNIX domain socket are disabled.</div><div class="line"><span class="number">16</span>/<span class="number">08</span>/<span class="number">08</span> <span class="number">19</span>:<span class="number">05</span>:<span class="number">03</span> DEBUG sasl.DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration <span class="keyword">for</span> dfs.data.transfer.protection</div><div class="line"><span class="number">16</span>/<span class="number">08</span>/<span class="number">08</span> <span class="number">19</span>:<span class="number">05</span>:<span class="number">03</span> ERROR lzo.GPLNativeCodeLoader: Could not load <span class="keyword">native</span> gpl library</div><div class="line">java.lang.UnsatisfiedLinkError: no gplcompression in java.library.path</div><div class="line"> at java.lang.ClassLoader.loadLibrary(ClassLoader.java:<span class="number">1886</span>)</div><div class="line"> at java.lang.Runtime.loadLibrary0(Runtime.java:<span class="number">849</span>)</div><div class="line"> at java.lang.System.loadLibrary(System.java:<span class="number">1088</span>)</div><div class="line"> at com.hadoop.compression.lzo.GPLNativeCodeLoader.&lt;clinit&gt;(GPLNativeCodeLoader.java:<span class="number">32</span>)</div><div class="line"> at com.hadoop.compression.lzo.LzoCodec.&lt;clinit&gt;(LzoCodec.java:<span class="number">71</span>)</div><div class="line"> at java.lang.Class.forName0(Native Method)</div><div class="line"> at java.lang.Class.forName(Class.java:<span class="number">274</span>)</div><div class="line"> at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:<span class="number">2013</span>)</div><div class="line"> at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:<span class="number">1978</span>)</div><div class="line"> at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:<span class="number">128</span>)</div><div class="line"> at org.apache.hadoop.io.compress.CompressionCodecFactory.&lt;init&gt;(CompressionCodecFactory.java:<span class="number">175</span>)</div><div class="line"> at org.apache.hadoop.mapred.TextInputFormat.configure(TextInputFormat.java:<span class="number">45</span>)</div><div class="line"> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</div></pre></td></tr></table></figure>
<p>这个究其原因就是程序运行的那个节点找不到lzo解压的包导致的，官网中只说明了 spark.driver.extraClassPath，但并没有说明配置spark.executor.extraClassPath 与 spark.executor.extraLibraryPath，导致不管怎么根据网上博客或者官网配置配，executor还是找不到lzo压缩相关的包，后来聪哥通过源码查看才发现有这么一个参数配置，只是各类文档中都没有，加上就ok了~</p>
]]></content>
    </entry>
    
  
  
    
    <entry>
      <title><![CDATA[timeline]]></title>
      <url>http://flume.cn/timeline/index.html</url>
      <content type="html"></content>
    </entry>
    
    <entry>
      <title></title>
      <url>http://flume.cn/about/index.html</url>
      <content type="html"><![CDATA[<p>hi，你好</p>
]]></content>
    </entry>
    
  
</search>
