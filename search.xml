<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[Spark通过slick连接数据库]]></title>
      <url>http://flume.cn/2016/10/31/Spark%E9%80%9A%E8%BF%87slick%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/</url>
      <content type="html"><![CDATA[<p>（持续更新）<br>由于Spark是由scala语言开发的，scala语言可以使用到所有java语言中的特性，所以spark连接数据库（比如Mysql）有很多种方法，这里记录两种我使用到的高级用法以及一些教训，分别是：</p>
<ol>
<li>使用Slick优雅地连接数据库；</li>
<li>如何使用SparkStreaming实时地获取数据库中的内容；</li>
<li>连接数据库过程中的踩坑集锦。</li>
</ol>
<h2 id="使用Slick优雅地连接数据库"><a href="#使用Slick优雅地连接数据库" class="headerlink" title="使用Slick优雅地连接数据库"></a>使用Slick优雅地连接数据库</h2><p>如果使用scala语言，当然可以想到的是，通过java连接数据库的方式连接数据库是没有问题的，但是scala语言有没有自己更加优雅地方法连接数据库呢？答案是肯定的，非常推荐使用：Slick</p>
<h3 id="Slick简介"><a href="#Slick简介" class="headerlink" title="Slick简介"></a>Slick简介</h3><p>Slick 是 TypeSafe 推出的 Scala 数据库访问库。开发者可以使用 Scala 语言风格来编写数据查询，而不是用 SQL 。 Slick 对于 Scala 来说，有如 LINQ 至于 C#，或者类似于其它平台上的 ORM 系统，它使用应用使用数据库有如使用 Scala 内置的集合类型（比如列表，集合等）一样方便。当然如有需要你还是可以直接使用 SQL 语句来查询数据库。<br>使用 Slick 而不直接使用 SQL 语句，可以使用编译器帮助发现一些类型错误，同时 Slick 可以为不同的后台数据库类型生成查询。它具有一些如下的特性：</p>
<ol>
<li>Scala </li>
</ol>
<p>所有查询，表格和字段映射，以及类型都采用普通的 Scala 语法。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Coffees</span>(<span class="params">tag: <span class="type">Tag</span></span>) <span class="keyword">extends</span> <span class="title">Table</span>[(<span class="type">String</span>, <span class="type">Double</span>)](<span class="params">tag, "<span class="type">COFFEES</span>"</span>) </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">name</span> </span>= column[<span class="type">String</span>](<span class="string">"COF_NAME"</span>, <span class="type">O</span>.<span class="type">PrimaryKey</span>)</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">price</span> </span>= column[<span class="type">Double</span>](<span class="string">"PRICE"</span>)</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">*</span> </span>= (name, price)</div><div class="line">&#125;</div><div class="line"><span class="keyword">val</span> coffees = <span class="type">TableQuery</span>[<span class="type">Coffees</span>]</div></pre></td></tr></table></figure></p>
<p>数据访问接口类型 Scala 的集合类型<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Query that only returns the "name" column</span></div><div class="line">coffees.map(_.name)</div><div class="line"></div><div class="line"><span class="comment">// Query that does a "where price &lt; 10.0"</span></div><div class="line">coffees.filter(_.price &lt; <span class="number">10.0</span>)</div></pre></td></tr></table></figure></p>
<ol>
<li>类型安全</li>
</ol>
<p>你使用的 IDE 可以帮助你写代码 在编译时而无需到运行时就可以发现一些错误<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// The result of "select PRICE from COFFEES" is a Seq of Double</span></div><div class="line"><span class="comment">// because of the type safe column definitions</span></div><div class="line"><span class="keyword">val</span> coffeeNames: <span class="type">Seq</span>[<span class="type">Double</span>] = coffees.map(_.price).list</div><div class="line"></div><div class="line"><span class="comment">// Query builders are type safe:</span></div><div class="line">coffees.filter(_.price &lt; <span class="number">10.0</span>)</div><div class="line"><span class="comment">// Using a string in the filter would result in a compilation error</span></div></pre></td></tr></table></figure></p>
<ol>
<li>可以组合</li>
</ol>
<p>查询接口为函数，这些函数可以多次组合和重用。可以使用函数式的方式来访问数据库</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Create a query for coffee names with a price less than 10, sorted by name</span></div><div class="line">coffees.filter(_.price &lt; <span class="number">10.0</span>).sortBy(_.name).map(_.name)</div><div class="line"><span class="comment">// The generated SQL is equivalent to:</span></div><div class="line"><span class="comment">// select name from COFFEES where PRICE &lt; 10.0 order by NAME</span></div></pre></td></tr></table></figure>
<ol>
<li><p>支持几乎所有常见的数据库</p>
<ul>
<li>DB2 (via slick-extensions)</li>
<li>Derby/JavaDB</li>
<li>H2</li>
<li>HSQLDB/HyperSQL</li>
<li>Microsoft Access</li>
<li>Microsoft SQL Server (via slick-extensions)</li>
<li>MySQL</li>
<li>Oracle (via slick-extensions)</li>
<li>PostgreSQL</li>
<li>SQLite</li>
</ul>
</li>
</ol>
<p>对于其它的一些数据库类型 Slick 也提供了有限的支持。</p>
<p>关于Slick的具体教程以及API，可以参阅<br><a href="http://slick.lightbend.com/" target="_blank" rel="external">Slick官网</a><br><a href="http://wiki.jikexueyuan.com/project/slick-guide/" target="_blank" rel="external">极客学院Slick中文教程</a><br>以及google</p>
<h3 id="如何使用到Spark项目中"><a href="#如何使用到Spark项目中" class="headerlink" title="如何使用到Spark项目中"></a>如何使用到Spark项目中</h3><p>以我自己摸索的方法为例，当然会有更多方法</p>
<h4 id="配置文件中配置数据库连接信息"><a href="#配置文件中配置数据库连接信息" class="headerlink" title="配置文件中配置数据库连接信息"></a>配置文件中配置数据库连接信息</h4><p>Slick默认会读取项目顶层的配置文件，当然配置文件的路径可以手动指定，默认在顶层的配置文件路径下，我的配置文件在 <code>resources/application.conf</code>中：</p>
<figure class="highlight plain"><figcaption><span>resources/application.conf</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">// todo: 使其变成可以从外部文件载入</div><div class="line">// 开发环境Mysql</div><div class="line">mysql = &#123;</div><div class="line">  url = &quot;jdbc:mysql://someIp:3306/someDb?useUnicode=true&amp;characterEncoding=utf-8&quot;</div><div class="line">  driver = &quot;com.mysql.jdbc.Driver&quot;</div><div class="line">  connectionPool = disabled</div><div class="line">  keepAliveConnection = true</div><div class="line">  databaseName = &quot;someDb&quot;</div><div class="line">  user = &quot;user&quot;</div><div class="line">  password = &quot;password&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="定义数据库表格对应的case-class"><a href="#定义数据库表格对应的case-class" class="headerlink" title="定义数据库表格对应的case class"></a>定义数据库表格对应的case class</h4><p>比如：该AppFrame是我定义的一个应用框架的样例类，与数据库中的字段有对应关系<br>PS：AppFrame的设置是为了将一切配置写到数据库中，这样可以实现项目的热切换，一套程序可以不需要重新编译而使用到不同的环境不同的策略中，亲测有效。</p>
<figure class="highlight scala"><figcaption><span>bean/AppFrame.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Author: wangxiaogang</div><div class="line">  * Date: 2016/10/1</div><div class="line">  * Email: wangxiaogang@chinatelecom.cn</div><div class="line">  * 应用的整体描述，包括 appId, app名称，输入类型，输入类型详细配置表，输出类型，输出类型详细配置表</div><div class="line">  */</div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">AppFrame</span> (<span class="params"></span></span></div><div class="line">               id: <span class="type">Int</span>,</div><div class="line">               name: <span class="type">String</span>,</div><div class="line">               inputStr: <span class="type">String</span>,</div><div class="line">               // 代表对应的输入在所在类型的输入中的id</div><div class="line">               inputId: <span class="type">Int</span>,</div><div class="line">               outputStr: <span class="type">String</span>,</div><div class="line">               outId: <span class="type">Int</span>,</div><div class="line">               redisStr: <span class="type">String</span>,</div><div class="line">               redisId: <span class="type">Int</span>,</div><div class="line">               sqlPoolStr: <span class="type">String</span>,</div><div class="line">               sqlPoolId:<span class="type">Int</span></div><div class="line">               )&#123;&#125;</div></pre></td></tr></table></figure>
<h4 id="使用Slick编写读取数据库的逻辑"><a href="#使用Slick编写读取数据库的逻辑" class="headerlink" title="使用Slick编写读取数据库的逻辑"></a>使用Slick编写读取数据库的逻辑</h4><p>这里就是slick的优势，非常简单</p>
<figure class="highlight scala"><figcaption><span>dao/MysqlDao.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Author: wangxiaogang</div><div class="line">  * Date: 2016/9/29</div><div class="line">  * Email: wangxiaogang@chinatelecom.cn</div><div class="line">  * 与Mysql数据库的交互类，使用了Slick方式</div><div class="line">  */</div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">MysqlDao</span> </span>&#123;</div><div class="line">  <span class="comment">/**</span></div><div class="line">    * 通过appId获取到应用的整体描述，包括 appId, 输入类型，输入类型详细配置表，输出类型，输出类型详细配置表</div><div class="line">    * 这里简单试验大数据实时处理框架的构思是否可行</div><div class="line">    *</div><div class="line">    * @param appId</div><div class="line">    * @return</div><div class="line">    * @todo : 目前只配置输入类型与输出类型的配置，以后尽量把所有数据处理方案都能以配置的形式写入数据库中</div><div class="line">    */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getAppFrame</span></span>(appId: <span class="type">Int</span>): <span class="type">AppFrame</span> = &#123;</div><div class="line">    <span class="keyword">implicit</span> <span class="keyword">val</span> getResult = <span class="type">GetResult</span>(r =&gt;</div><div class="line">      <span class="type">AppFrame</span>(r.nextInt(), r.nextString(), r.nextString(), r.nextInt(), r.nextString(), r.nextInt(), r.nextString(),</div><div class="line">        r.nextInt(),  r.nextString(), r.nextInt()))</div><div class="line">    <span class="keyword">val</span> q = <span class="string">sql""</span><span class="string">"SELECT * FROM appframe WHERE id = $appId"</span><span class="string">""</span>.as[<span class="type">AppFrame</span>]</div><div class="line"></div><div class="line">    <span class="keyword">val</span> db = <span class="type">Database</span>.forConfig(<span class="string">"mysql"</span>)</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="keyword">val</span> fu = db.run(q)</div><div class="line">      <span class="type">Await</span>.result(fu, <span class="number">10</span> seconds).head</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">      db.close()</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如上，就这么简单几行，就能连接数据库了，并且将其转化为对应的样例类，是不是超级好用<br>当然，我只是使用了一点皮毛，它还有很多有用的特性我没有使用到。</p>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p><a href="http://slick.lightbend.com/" target="_blank" rel="external">http://slick.lightbend.com/</a><br><a href="http://wiki.jikexueyuan.com/project/slick-guide/" target="_blank" rel="external">http://wiki.jikexueyuan.com/project/slick-guide/</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[记HashMap遇到的java.util.ConcurrentModificationException的bug]]></title>
      <url>http://flume.cn/2016/10/27/%E8%AE%B0HashMap%E9%81%87%E5%88%B0%E7%9A%84java-util-ConcurrentModificationException%E7%9A%84bug/</url>
      <content type="html"><![CDATA[<h4 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h4><p>spark Streaming 实时程序在联调期间稳定运行了两天，以为问题不大了，第二天早上的时候打开一看，竟然挂了，定位到代码，原来我的程序实时读取redis的数据为一个HashMap，直到挂的时候，Redis中数据一直在增大，共 6083条：</p>
<p>spark相关代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">// 1. 从L1中删除过期的号码，同时Redis中的对应该K-V也删除</span></div><div class="line"> ...</div><div class="line"><span class="keyword">val</span> sentinelPool = <span class="type">InternalRedisClient</span>.getSentinelPool</div><div class="line"><span class="keyword">var</span> phoneSet: util.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="keyword">new</span> util.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>]()</div><div class="line"><span class="comment">//            printLog.debug( "sentinelPool NumIdle0: " + sentinelPool.getNumIdle + " Active0: " + sentinelPool.getNumActive)</span></div><div class="line"></div><div class="line"><span class="keyword">var</span> jedis1: <span class="type">Jedis</span> = <span class="literal">null</span></div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">  jedis1 = sentinelPool.getResource</div><div class="line">  <span class="comment">//              printLog.debug( "sentinelPool NumIdle1: " + sentinelPool.getNumIdle + " Active1: " + sentinelPool.getNumActive)</span></div><div class="line"></div><div class="line">  phoneSet = jedis1.hgetAll(redisHashKey)</div><div class="line">  <span class="comment">//              printLog.debug( "sentinelPool NumIdle3: " + sentinelPool.getNumIdle + " Active3: " + sentinelPool.getNumActive)</span></div><div class="line"></div><div class="line">  printLog.debug(<span class="string">"phoneSet_1: "</span> + phoneSet)</div><div class="line">&#125; <span class="keyword">finally</span> &#123;</div><div class="line">  <span class="keyword">if</span> (jedis1 != <span class="literal">null</span>) &#123;</div><div class="line">    printLog.debug(<span class="string">"close jedis1"</span>)</div><div class="line">    jedis1.close()</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">if</span> (!phoneSet.isEmpty) &#123; </div><div class="line">  <span class="keyword">for</span> (eachPhoneKV: (<span class="type">String</span>, <span class="type">String</span>) &lt;- phoneSet) &#123; <span class="comment">// 就在这里挂掉了</span></div><div class="line">    <span class="keyword">val</span> expirationDate: <span class="type">Int</span> = eachPhoneKV._2.split(<span class="string">"\\|"</span>)(<span class="number">4</span>).toInt</div><div class="line">    <span class="keyword">val</span> today: <span class="type">Int</span> = getNowDate.toInt</div><div class="line">    <span class="keyword">if</span> (today &gt; expirationDate) &#123;</div><div class="line">      phoneSet.remove(eachPhoneKV._1)</div><div class="line">      <span class="keyword">var</span> jedis2: <span class="type">Jedis</span> = <span class="literal">null</span></div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        jedis2 = sentinelPool.getResource</div><div class="line">        jedis2.hdel(redisHashKey, eachPhoneKV._1)</div><div class="line">      &#125; <span class="keyword">finally</span> &#123;</div><div class="line">        <span class="keyword">if</span> (jedis2 != <span class="literal">null</span>) &#123;</div><div class="line">          printLog.debug(<span class="string">"close jedis2"</span>)</div><div class="line">          jedis2.close()</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  printLog.debug(<span class="string">"phoneSet_filtedByData: "</span> + phoneSet)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>具体异常粘信息如下：<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 0.0 failed 4 times, most recent failure: Lost task 4.3 in stage 0.0 (TID 170, NM-304-HW-XH628V3-BIGDATA-063): java.util.ConcurrentModificationException</div><div class="line">at java.util.HashMap$HashIterator.nextEntry(HashMap.java:922)</div><div class="line">at java.util.HashMap$EntryIterator.next(HashMap.java:962)</div><div class="line">at java.util.HashMap$EntryIterator.next(HashMap.java:960)</div><div class="line">at scala.collection.convert.Wrappers$JMapWrapperLike$$anon$2.next(Wrappers.scala:267)</div><div class="line">at scala.collection.convert.Wrappers$JMapWrapperLike$$anon$2.next(Wrappers.scala:264)</div><div class="line">at scala.collection.Iterator$class.foreach(Iterator.scala:727)</div><div class="line">at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)</div><div class="line">at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)</div><div class="line">at scala.collection.AbstractIterable.foreach(Iterable.scala:54)</div><div class="line">at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)</div><div class="line">at com.chinatelecom.bigdata.oidd2.Location$$anonfun$2$$anonfun$3.apply(Location.scala:871)</div><div class="line">at com.chinatelecom.bigdata.oidd2.Location$$anonfun$2$$anonfun$3.apply(Location.scala:677)</div></pre></td></tr></table></figure></p>
<h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p>项目太紧张，来不及详细分析java的源码了，根据经验redis中应该六千多条数据应该不是很大的，HashMap完全可以一次读取，从网上查到原因是因为remove操作导致的，在Iterator遍历过程中调用HashMap的remove方法会crash，有两个解决办法：</p>
<ol>
<li>一个解决办法是用一个ArrayList记录要删除的key,然后再遍历这个ArrayList,调用HashMap的remove方法以ArrayList的元素为key进行删除；这个方法需要额外的空间和时间，虽然也浪费的不多，但总感觉不够优雅；</li>
<li>创建一个Iterator<map.entry<integer, string="">&gt; iterator = map.entrySet().iterator();，然后用这个 iterator.remove()方法进行删除，这个是可以删除的；</map.entry<integer,></li>
</ol>
<p>我使用的是方法二，修改后的代码如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">var</span> phoneMap: util.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="keyword">new</span> util.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>]()</div><div class="line"><span class="comment">// printLog.debug( "sentinelPool NumIdle0: " + sentinelPool.getNumIdle + " Active0: " + sentinelPool.getNumActive)</span></div><div class="line"></div><div class="line"><span class="keyword">var</span> jedis1: <span class="type">Jedis</span> = <span class="literal">null</span></div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">  jedis1 = sentinelPool.getResource</div><div class="line">  <span class="comment">// printLog.debug( "sentinelPool NumIdle1: " + sentinelPool.getNumIdle + " Active1: " + sentinelPool.getNumActive)</span></div><div class="line"></div><div class="line">  phoneMap = jedis1.hgetAll(redisHashKey)</div><div class="line">  <span class="comment">// printLog.debug( "sentinelPool NumIdle3: " + sentinelPool.getNumIdle + " Active3: " + sentinelPool.getNumActive)</span></div><div class="line"></div><div class="line">  printLog.debug(<span class="string">"phoneSet_1: "</span> + phoneMap)</div><div class="line">&#125; <span class="keyword">finally</span> &#123;</div><div class="line">  <span class="keyword">if</span> (jedis1 != <span class="literal">null</span>) &#123;</div><div class="line">    printLog.debug(<span class="string">"close jedis1"</span>)</div><div class="line">    jedis1.close()</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 1. 从L1中删除过期的号码，同时Redis中的对应该K-V也删除</span></div><div class="line"><span class="keyword">if</span> (!phoneMap.isEmpty) &#123;</div><div class="line">  <span class="keyword">val</span> iterator: util.<span class="type">Iterator</span>[<span class="type">Entry</span>[<span class="type">String</span>, <span class="type">String</span>]] = phoneMap.entrySet().iterator()</div><div class="line">  <span class="keyword">while</span> (iterator.hasNext) &#123;</div><div class="line">    <span class="keyword">val</span> eachPhoneKV: <span class="type">Entry</span>[<span class="type">String</span>, <span class="type">String</span>] = iterator.next()</div><div class="line">    <span class="keyword">val</span> mdn = eachPhoneKV.getKey</div><div class="line">    <span class="keyword">val</span> redisValue = eachPhoneKV.getValue</div><div class="line"></div><div class="line">    <span class="keyword">var</span> expirationDate: <span class="type">Int</span> = <span class="number">0</span></div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      expirationDate = redisValue.split(<span class="string">"\\|"</span>)(<span class="number">4</span>).toInt</div><div class="line">      <span class="keyword">val</span> today: <span class="type">Int</span> = getNowDate.toInt</div><div class="line">      <span class="keyword">if</span> (today &gt; expirationDate) &#123;</div><div class="line">        printLog.info(<span class="string">"delete this data for expirationDate:"</span> + eachPhoneKV)</div><div class="line">        iterator.remove()</div><div class="line">        <span class="comment">// phoneMap.remove(mdn) // 这一句是错误的，因为无法据此删除</span></div><div class="line">        <span class="keyword">var</span> jedis2: <span class="type">Jedis</span> = <span class="literal">null</span></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">          jedis2 = sentinelPool.getResource</div><div class="line">          jedis2.hdel(redisHashKey, mdn)</div><div class="line">        &#125; <span class="keyword">finally</span> &#123;</div><div class="line">          <span class="keyword">if</span> (jedis2 != <span class="literal">null</span>) &#123;</div><div class="line">            printLog.debug(<span class="string">"close jedis2"</span>)</div><div class="line">            jedis2.close()</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="comment">// 如果解析发生异常，则redis中删掉这个key，并且在phoneMap中同时删除</span></div><div class="line">      <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; &#123;</div><div class="line">        printLog.error(<span class="string">"redis error data and del it: "</span> + eachPhoneKV + <span class="string">" error: "</span> + ex)</div><div class="line">        iterator.remove()</div><div class="line">        <span class="keyword">var</span> jedis4: <span class="type">Jedis</span> = <span class="literal">null</span></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">          jedis4 = sentinelPool.getResource</div><div class="line">          jedis4.hdel(redisHashKey, mdn)</div><div class="line">        &#125; <span class="keyword">finally</span> &#123;</div><div class="line">          <span class="keyword">if</span> (jedis4 != <span class="literal">null</span>) &#123;</div><div class="line">            printLog.debug(<span class="string">"close jedis4"</span>)</div><div class="line">            jedis4.close()</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>然后测试，打包，部署，OK，解决。</p>
<h4 id="原理说明"><a href="#原理说明" class="headerlink" title="原理说明"></a>原理说明</h4><p>遍历HashMap有三种方法，分别是:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span>(Map.Entry&lt;Integer, String&gt; entry : map.entrySet())&#123;&#125;  <span class="comment">// scala中为 &lt;-</span></div><div class="line"></div><div class="line"><span class="keyword">for</span>(Integer key : keySet)&#123;&#125;</div><div class="line"></div><div class="line">Iterator&lt;Map.Entry&lt;Integer, String&gt;&gt; it = map.entrySet().iterator();</div><div class="line">        <span class="keyword">while</span>(it.hasNext())&#123;&#125;</div></pre></td></tr></table></figure></p>
<p>其实上面的三种遍历方式从根本上讲都是使用的迭代器，之所以出现不同的结果是由于remove操作的实现不同决定的。</p>
<p>首先前两种方法都在调用nextEntry方法的同一个地方抛出了异常，虽然remove成功了，但是在迭代器遍历下一个元素的时候抛出异常：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">final</span> Entry&lt;K,V&gt; <span class="title">nextEntry</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (modCount != expectedModCount)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException();</div><div class="line">    Entry&lt;K,V&gt; e = next;</div><div class="line">    ...</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>这里modCount是表示map中的元素被修改了几次(在移除，新加元素时此值都会自增)，而expectedModCount是表示期望的修改次数，在迭代器构造的时候这两个值是相等，如果在遍历过程中这两个值出现了不同步就会抛出ConcurrentModificationException异常。</p>
<p>1、HashMap的remove方法实现<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> V <span class="title">remove</span><span class="params">(Object key)</span> </span>&#123;</div><div class="line">    Entry&lt;K,V&gt; e = removeEntryForKey(key);</div><div class="line">    <span class="keyword">return</span> (e == <span class="keyword">null</span> ? <span class="keyword">null</span> : e.value);</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>2、HashMap.KeySet的remove方法实现<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">remove</span><span class="params">(Object o)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> HashMap.<span class="keyword">this</span>.removeEntryForKey(o) != <span class="keyword">null</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>3、HashMap.HashIterator的remove方法实现<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">remove</span><span class="params">()</span> </span>&#123;</div><div class="line">   <span class="keyword">if</span> (current == <span class="keyword">null</span>)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException();</div><div class="line">   <span class="keyword">if</span> (modCount != expectedModCount)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException();</div><div class="line">   Object k = current.key;</div><div class="line">   current = <span class="keyword">null</span>;</div><div class="line">   HashMap.<span class="keyword">this</span>.removeEntryForKey(k);</div><div class="line">   expectedModCount = modCount;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>以上三种实现方式都通过调用HashMap.removeEntryForKey方法来实现删除key的操作。在removeEntryForKey方法内只要移除了key modCount就会执行一次自增操作，此时modCount就与expectedModCount不一致了，上面三种remove实现中，只有第三种iterator的remove方法在调用完removeEntryForKey方法后同步了expectedModCount值与modCount相同，所以在遍历下个元素调用nextEntry方法时，iterator方式不会抛异常。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">final</span> Entry&lt;K,V&gt; <span class="title">removeEntryForKey</span><span class="params">(Object key)</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> hash = (key == <span class="keyword">null</span>) ? <span class="number">0</span> : hash(key.hashCode());</div><div class="line">    <span class="keyword">int</span> i = indexFor(hash, table.length);</div><div class="line">    Entry&lt;K,V&gt; prev = table[i];</div><div class="line">    Entry&lt;K,V&gt; e = prev;</div><div class="line"></div><div class="line">    <span class="keyword">while</span> (e != <span class="keyword">null</span>) &#123;</div><div class="line">        Entry&lt;K,V&gt; next = e.next;</div><div class="line">        Object k;</div><div class="line">        <span class="keyword">if</span> (e.hash == hash &amp;&amp;</div><div class="line">            ((k = e.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k)))) &#123;</div><div class="line">            modCount++;</div><div class="line">            size--;</div><div class="line">            <span class="keyword">if</span> (prev == e)</div><div class="line">                table[i] = next;</div><div class="line">            <span class="keyword">else</span></div><div class="line">                prev.next = next;</div><div class="line">            e.recordRemoval(<span class="keyword">this</span>);</div><div class="line">            <span class="keyword">return</span> e;</div><div class="line">        &#125;</div><div class="line">        prev = e;</div><div class="line">        e = next;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">return</span> e;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="其它思考"><a href="#其它思考" class="headerlink" title="其它思考"></a>其它思考</h4><p>1、如果是遍历过程中增加或修改数据呢？<br>增加或修改数据只能通过Map的put方法实现，在遍历过程中修改数据可以，但如果增加新key就会在下次循环时抛异常，因为在添加新key时modCount也会自增。</p>
<p>2、有些集合类也有同样的遍历问题，如ArrayList，通过Iterator方式可正确遍历完成remove操作，直接调用list的remove方法就会抛异常。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//会抛ConcurrentModificationException异常</span></div><div class="line"><span class="keyword">for</span>(String str : list)&#123;</div><div class="line">	list.remove(str);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//正确遍历移除方式</span></div><div class="line">Iterator&lt;String&gt; it = list.iterator();</div><div class="line"><span class="keyword">while</span>(it.hasNext())&#123;</div><div class="line">	it.next();</div><div class="line">	it.remove();</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>3、jdk为什么这样设计，只允许通过iterator进行remove操作？</p>
<p>HashMap和keySet的remove方法都可以通过传递key参数删除任意的元素，而iterator只能删除当前元素(current);<br>对于通过HashMap的remove方法来说，一旦删除的元素是iterator对象中next所正在引用的，如果没有通过modCount与 expectedModCount的比较实现快速失败抛出异常，下次循环该元素将成为current指向，此时iterator就遍历了一个已移除的过期数据，所以一定要判断这两个值是否一致。</p>
<p>4、这是一个坑，如果IDE能提示就好了，下次注意</p>
<h4 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h4><p><a href="http://afredlyj.github.io/posts/hashmap-concurrentmodificationexception.html" target="_blank" rel="external">http://afredlyj.github.io/posts/hashmap-concurrentmodificationexception.html</a><br><a href="http://dumbee.net/archives/41" target="_blank" rel="external">http://dumbee.net/archives/41</a><br><a href="http://blog.csdn.net/wzy_1988/article/details/51423583" target="_blank" rel="external">http://blog.csdn.net/wzy_1988/article/details/51423583</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[景区位置服务项目说明文档]]></title>
      <url>http://flume.cn/2016/10/18/%E6%99%AF%E5%8C%BA%E4%BD%8D%E7%BD%AE%E6%9C%8D%E5%8A%A1%E9%A1%B9%E7%9B%AE%E8%AF%B4%E6%98%8E%E6%96%87%E6%A1%A3/</url>
      <content type="html"><![CDATA[<h2 id="总体架构"><a href="#总体架构" class="headerlink" title="总体架构"></a>总体架构</h2><p>总体架构图如下：<br><img src="/2016/10/18/景区位置服务项目说明文档/1.png" alt="1.png" title=""></p>
<p>如上图：主要分为三大部分：</p>
<h4 id="上游数据"><a href="#上游数据" class="headerlink" title="上游数据"></a>上游数据</h4><p>由东方国信提供各个省份的Oidd数据，发送至kafka集群</p>
<h4 id="处理逻辑"><a href="#处理逻辑" class="headerlink" title="处理逻辑"></a>处理逻辑</h4><ol>
<li>Web微服务端：web微服务接受位置数据订阅，并将订阅的景区信息存储至Mysql中；</li>
<li>景区用户识别端：由Spark Streaming程序根据订阅信息实时对上游数据进行处理，识别出到达景区的最新电话号码列表，发送至下游；</li>
</ol>
<h4 id="下游系统"><a href="#下游系统" class="headerlink" title="下游系统"></a>下游系统</h4><p>处理逻辑将处理结果通过FTP共享接口发送至下游大数据营销系统，由大数据营销系统实现推荐等。</p>
<h2 id="处理逻辑-1"><a href="#处理逻辑-1" class="headerlink" title="处理逻辑"></a>处理逻辑</h2><h3 id="景区用户识别端详解"><a href="#景区用户识别端详解" class="headerlink" title="景区用户识别端详解"></a>景区用户识别端详解</h3><p>景区用户识别端详解逻辑由Spark Streaming程序实现，运行部署在yarn集群中，是一个实时的容错的处理逻辑<br>分别实现如下功能：</p>
<h4 id="配置信息读取"><a href="#配置信息读取" class="headerlink" title="配置信息读取"></a>配置信息读取</h4><p>kafka，flume，redis，mysql等配置信息配置在数据库中，系统载入试进行自动化预读取</p>
<h4 id="接收Kakfa消息流"><a href="#接收Kakfa消息流" class="headerlink" title="接收Kakfa消息流"></a>接收Kakfa消息流</h4><p>接收由国信发送至kafka的消息流，并将其分发到不同的机器，设置并发度；</p>
<h4 id="对所有的号码根据mdn生成key"><a href="#对所有的号码根据mdn生成key" class="headerlink" title="对所有的号码根据mdn生成key"></a>对所有的号码根据mdn生成key</h4><h4 id="对mdn重复数据进行去重"><a href="#对mdn重复数据进行去重" class="headerlink" title="对mdn重复数据进行去重"></a>对mdn重复数据进行去重</h4><p>注意这里是在每台不同的机器下进行的去重，而不是整体的去重</p>
<h4 id="建立Redis连接"><a href="#建立Redis连接" class="headerlink" title="建立Redis连接"></a>建立Redis连接</h4><p>在每台机器上建立Redis连接池，</p>
<h4 id="动态读取Mysql订阅信息"><a href="#动态读取Mysql订阅信息" class="headerlink" title="动态读取Mysql订阅信息"></a>动态读取Mysql订阅信息</h4><p>对每一批数据，动态从mysql数据库中载入订阅的景区信息L0，并将其转换成 cityCode -&gt; PositionSubData的 HashMap util.HashMap[Int, util.LinkedList[PositionSubData]]</p>
<h4 id="对源数据进行过滤，不在景区所属城市的数据丢掉"><a href="#对源数据进行过滤，不在景区所属城市的数据丢掉" class="headerlink" title="对源数据进行过滤，不在景区所属城市的数据丢掉"></a>对源数据进行过滤，不在景区所属城市的数据丢掉</h4><h4 id="读取Redis中已经保存的景区用户信息列表"><a href="#读取Redis中已经保存的景区用户信息列表" class="headerlink" title="读取Redis中已经保存的景区用户信息列表"></a>读取Redis中已经保存的景区用户信息列表</h4><p>从Redis中获取之前保存的号码列表L1，并且对过期的数据进行双向删除</p>
<h4 id="判断数据是否需要推送"><a href="#判断数据是否需要推送" class="headerlink" title="判断数据是否需要推送"></a>判断数据是否需要推送</h4><p>数据是否需要推送需要满足：</p>
<ol>
<li>不在Redis中：与Redis中数据进行对比，在Redis中的数据删除</li>
<li>在景区里：通过判断数据是否在景区里，如果在景区，则返回它对应的locationId_intervalTime_spId，否则返回””;</li>
<li>更新Redis </li>
</ol>
<h4 id="数据统一发送"><a href="#数据统一发送" class="headerlink" title="数据统一发送"></a>数据统一发送</h4><p>将处理好的数据按照一定的规则统一批量发送至flume</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark与Mysql互操作探索总结]]></title>
      <url>http://flume.cn/2016/10/10/Spark%E4%B8%8EMysql%E4%BA%92%E6%93%8D%E4%BD%9C%E6%8E%A2%E7%B4%A2%E6%80%BB%E7%BB%93/</url>
      <content type="html"></content>
    </entry>
    
    <entry>
      <title><![CDATA[scala中@的用法]]></title>
      <url>http://flume.cn/2016/09/23/scala%E4%B8%AD-%E7%9A%84%E7%94%A8%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>有些场景，比如模式匹配会遇到scala代码中有@符号，比如<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">case</span> x @ <span class="type">Some</span>(<span class="type">Nil</span>) =&gt; x</div></pre></td></tr></table></figure></p>
<p>现将网友的答案总结一下，并持续更新：</p>
<h3 id="绑定在模式匹配中，取出对应的原来输入值"><a href="#绑定在模式匹配中，取出对应的原来输入值" class="headerlink" title="绑定在模式匹配中，取出对应的原来输入值"></a>绑定在模式匹配中，取出对应的原来输入值</h3><p>比如：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> o: <span class="type">Option</span>[<span class="type">Int</span>] = <span class="type">Some</span>(<span class="number">5</span>)</div><div class="line"></div><div class="line"><span class="comment">// o: Option[Int] = Some(5)</span></div><div class="line"></div><div class="line">o <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> <span class="type">Some</span>(x) =&gt; println(x)</div><div class="line">  <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">&#125;</div><div class="line"><span class="comment">// 输出:  </span></div><div class="line"><span class="number">5</span></div><div class="line"></div><div class="line">o <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> x @ <span class="type">Some</span>(_) =&gt; println(x)</div><div class="line">  <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">&#125;</div><div class="line"><span class="comment">// 输出</span></div><div class="line"><span class="type">Some</span>(<span class="number">5</span>)</div></pre></td></tr></table></figure>
<p>如上案例，有些情况下，模式匹配后你并不想取出他的值，而是取出他本来的自己（Some(5)），这种情况下就用 @；并且@可以用于各个级别</p>
<h3 id="可以用来将名称和一个匹配的模式绑定，然后这个值作为匹配模式"><a href="#可以用来将名称和一个匹配的模式绑定，然后这个值作为匹配模式" class="headerlink" title="@可以用来将名称和一个匹配的模式绑定，然后这个值作为匹配模式"></a>@可以用来将名称和一个匹配的模式绑定，然后这个值作为匹配模式</h3><p>听起来很绕口，看如下代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> d@(c<span class="meta">@Some</span>(a), <span class="type">Some</span>(b)) = (<span class="type">Some</span>(<span class="number">1</span>), <span class="type">Some</span>(<span class="number">2</span>))</div></pre></td></tr></table></figure>
<p>结果竟然产生了四个值：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">d: (<span class="type">Some</span>[<span class="type">Int</span>], <span class="type">Some</span>[<span class="type">Int</span>]) = (<span class="type">Some</span>(<span class="number">1</span>),<span class="type">Some</span>(<span class="number">2</span>))</div><div class="line">c: <span class="type">Some</span>[<span class="type">Int</span>] = <span class="type">Some</span>(<span class="number">1</span>)</div><div class="line">a: <span class="type">Int</span> = <span class="number">1</span></div><div class="line">b: <span class="type">Int</span> = <span class="number">2</span></div></pre></td></tr></table></figure></p>
<p>如上所述，说明定义d和c是两个匹配模式，a和b是两个数字</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">(<span class="type">Some</span>(<span class="number">1</span>), <span class="type">Some</span>(<span class="number">2</span>)) <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> d@(c<span class="meta">@Some</span>(a), <span class="type">Some</span>(b)) =&gt; println(a, b, c, d)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>结果如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(<span class="number">1</span>,<span class="number">2</span>,<span class="type">Some</span>(<span class="number">1</span>),(<span class="type">Some</span>(<span class="number">1</span>),<span class="type">Some</span>(<span class="number">2</span>)))</div></pre></td></tr></table></figure></p>
<p>再跑一个例子<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">for</span> (t<span class="meta">@Some</span>(u) &lt;- <span class="type">Seq</span>(<span class="type">Some</span>(<span class="number">1</span>))) println(t, u)</div><div class="line">(<span class="type">Some</span>(<span class="number">1</span>),<span class="number">1</span>)</div></pre></td></tr></table></figure></p>
<p>再跑一个例子</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> <span class="type">List</span>(x, xs @ _*) = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</div><div class="line">x: <span class="type">Int</span> = <span class="number">1</span></div><div class="line">xs: <span class="type">Seq</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">2</span>, <span class="number">3</span>)</div></pre></td></tr></table></figure>
<h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><p><a href="http://stackoverflow.com/questions/2359014/scala-operator" target="_blank" rel="external">http://stackoverflow.com/questions/2359014/scala-operator</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark在Kerberos下连接使用Hbase的配置]]></title>
      <url>http://flume.cn/2016/09/18/Spark%E5%9C%A8Kerberos%E4%B8%8B%E8%BF%9E%E6%8E%A5%E4%BD%BF%E7%94%A8Hbase%E7%9A%84%E9%85%8D%E7%BD%AE/</url>
      <content type="html"><![CDATA[<p>复制HBase目录下的lib文件到spark目录/lib/hbase。spark依赖此lib，但直接指定到Hbase下的lib目录的话又会出错<br>清单如下：guava-12.0.1.jar htrace-core-3.1.0-incubating.jar protobuf-java-2.5.0.jar   这三个jar加上以hbase开头所有jar，其它就不必了，全部复制会引起报错。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> <span class="variable">$SPARK_HOME</span>/lib/hbase</div><div class="line">cp /usr/lib/hbase/lib/hbase-* ./</div><div class="line">cp /usr/lib/hbase/lib/guava-12.0.1.jar ./</div><div class="line">cp /usr/lib/hbase/lib/htrace-core-3.1.0-incubating.jar ./</div><div class="line">cp /usr/lib/hbase/lib/protobuf-java-2.5.0.jar ./</div></pre></td></tr></table></figure></p>
<p>然后在spark客户端配置如下：<br>也就是增加到classpath<br><figure class="highlight bash"><figcaption><span>spark-default.conf</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">spark.driver.extraClassPath /usr/lib/hadoop/lib/*:/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/lib/hbase/*</div></pre></td></tr></table></figure></p>
<p>就可以了</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark在yarn中的资源申请与分配调研]]></title>
      <url>http://flume.cn/2016/09/18/Spark%E5%9C%A8yarn%E4%B8%AD%E7%9A%84%E8%B5%84%E6%BA%90%E7%94%B3%E8%AF%B7%E4%B8%8E%E5%88%86%E9%85%8D/</url>
      <content type="html"><![CDATA[<p>本文解决遇到的以下问题：<br><em>spark作业提交到yarn的时候，如果用户(wzfw)所在队列本来有500个executor的权限，但是他跑一个简单的程序根本不需要这么多的资源，只需要200个核就足够了，那他如果申请了400个核的话，是否需要全部分配给他？</em></p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>目前我们所有的spark程序的分配都是靠参数设置固定的Executor数量进行资源预分配的，如果用户op在yarn的资源队列里可以申请到200个资源，那它就算跑占用资源很少的程序也能申请到200个核，这是不合理的</p>
<p>比如简单跑如下SparkPi程序，申请20个核：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --num-executors 20 lib/spark-examples-1.6.2-hadoop2.6.0.jar</div></pre></td></tr></table></figure></p>
<p>yarn中资源占用情况如下：<br><img src="/2016/09/18/Spark在yarn中的资源申请与分配/spark-yarn-allocation.png" alt="spark-yarn-allocation.png" title=""><br>可以看到，我就跑了一个SparkPi啊，竟然用了43G的内存，这样很不合理！</p>
<h2 id="总述"><a href="#总述" class="headerlink" title="总述"></a>总述</h2><p>Spark在yarn集群上运行的时候，一方面默认通过num-executors参数设置固定的Executor数量，每个application会独占所有预分配的资源直到整个生命周期的结束。Spark1.2后开始引入动态资源分配（Dynamic Resource Allocation）机制，支持资源弹性分配。</p>
<p>对于已知的业务负载，使用固定的集群资源配置是相对容易的；对于未知的业务负载，使用动态的集群资源分配方式可以满足负载的动态变化，这样集群的资源利用和业务负载的处理效率都会更加灵活。</p>
<p>动态资源分配测试在Spark1.2仅支持Yarn模式，从Spark1.6开始，支持standalone、Yarn、Mesos.这个特性默认是禁用的。<br><a id="more"></a></p>
<h2 id="动态资源分配的思想"><a href="#动态资源分配的思想" class="headerlink" title="动态资源分配的思想"></a>动态资源分配的思想</h2><p>简单来说，就是基于负载来动态调节Spark应用的资源占用，你的应用会在资源空闲的时候将其释放给集群，而后续用到的时候再重新申请。</p>
<h3 id="动态资源分配策略"><a href="#动态资源分配策略" class="headerlink" title="动态资源分配策略"></a>动态资源分配策略</h3><p>其实没有一个固定的方法可以预测一个executor后续是否马上会被分配去执行任务，或者一个新分配的执行器实际上是空闲的，所以我们需要一些试探性的方法，来决定是否申请或移除一个执行器。策略分为<strong>请求策略</strong>与<strong>移除策略</strong>：</p>
<h4 id="请求策略"><a href="#请求策略" class="headerlink" title="请求策略"></a>请求策略</h4><p>开启动态分配策略后，application会在task因没有足够资源被挂起的时候去动态申请资源，这种情况意味着该application现有的executor无法满足所有task并行运行。spark一轮一轮的申请资源，当有task挂起或等待spark.dynamicAllocation.schedulerBacklogTimeout(默认1s)时间的时候，会开始动态资源分配；之后会每隔spark.dynamicAllocation.sustainedSchedulerBacklogTimeout(默认1s)时间申请一次，直到申请到足够的资源。<strong>每次申请的资源量是指数增长的，即1,2,4,8等</strong>。<br>之所以采用指数增长，出于两方面考虑：其一，开始申请的少是考虑到可能application会马上得到满足；其次要成倍增加，是为了如果application需要很多资源，而该方式可以在很少次数的申请之后得到满足。<br>（这段指数增长的策略可以根据实际情况通过修改源码来修改）</p>
<h4 id="资源回收策略"><a href="#资源回收策略" class="headerlink" title="资源回收策略"></a>资源回收策略</h4><p>当application的executor空闲时间超过spark.dynamicAllocation.executorIdleTimeout（默认60s）后，就会被回收。</p>
<h2 id="配置思路"><a href="#配置思路" class="headerlink" title="配置思路"></a>配置思路</h2><h3 id="启动-external-shuffle-service"><a href="#启动-external-shuffle-service" class="headerlink" title="启动 external shuffle service"></a>启动 external shuffle service</h3><p>要使用这一特性有两个前提条件。首先，你的应用必须设置 spark.dynamicAllocation.enabled 为 true。其次，你必须在每个节点上启动一个外部混洗服务（external shuffle service），并在你的应用中将 spark.shuffle.service.enabled 设为true。外部混洗服务的目的就是为了在删除执行器的时候，能够保留其输出的混洗文件（本文后续有更详细的描述）。启用外部混洗的方式在各个集群管理器上各不相同：</p>
<p>在Spark独立部署的集群中，你只需要在worker启动前设置 spark.shuffle.server.enabled 为true即可。</p>
<p>在YARN模式下，混洗服务需要按以下步骤在各个NodeManager上启动：</p>
<ol>
<li>首先按照YARN profile 构建Spark。如果你已经有打好包的Spark，可以忽略这一步。</li>
<li>找到 spark-<version>-yarn-shuffle.jar。如果你是自定义编译，其位置应该在 ${SPARK_HOME}/network/yarn/target/scala-<version>，否则应该可以在 lib 目录下找到这个jar包。</version></version></li>
<li>将该jar包添加到NodeManager的classpath路径中。</li>
<li>配置各个节点上的yarn-site.xml，将 spark_shuffle 添加到 yarn.nodemanager.aux-services 中，然后将 yarn.nodemanager.aux-services.spark_shuffle.class 设为 org.apache.spark.network.yarn.YarnShuffleService，并将 spark.shuffle.service.enabled 设为 true。</li>
<li>最后重启各节点上的NodeManager。</li>
</ol>
<p>所有相关的配置都是可选的，并且都在 spark.dynamicAllocation.<em> 和 spark.shuffle.service.</em> 命名空间下。更详细请参考：<a href="http://spark.apache.org/docs/latest/configuration.html#dynamic-allocation" target="_blank" rel="external">configurations page</a>。</p>
<h3 id="外部混洗服务external-shuffle-service"><a href="#外部混洗服务external-shuffle-service" class="headerlink" title="外部混洗服务external shuffle service"></a>外部混洗服务external shuffle service</h3><p>非动态分配模式下，执行器可能的退出原因有执行失败或者相关Spark应用已经退出。不管是哪种原因，执行器的所有状态都已经不再需要，可以丢弃掉。但在动态分配的情形下，执行器有可能在Spark应用运行期间被移除。这时候，如果Spark应用尝试去访问该执行器存储的状态，就必须重算这一部分数据。因此，Spark需要一种机制，能够优雅地关闭执行器，同时还保留其状态数据。</p>
<p>这种需求对于混洗操作尤其重要。混洗过程中，Spark执行器首先将map输出写到本地磁盘，同时执行器本身又是一个文件服务器，这样其他执行器就能够通过该执行器获得对应的map结果数据。一旦有某些任务执行时间过长，动态分配有可能在混洗结束前移除任务异常的执行器，而这些被移除的执行器对应的数据将会被重新计算，但这些重算其实是不必要的。</p>
<p>要解决这一问题，就需要用到一个外部混洗服务（external shuffle service），该服务在Spark 1.2引入。该服务在每个节点上都会启动一个不依赖于任何Spark应用或执行器的独立进程。一旦该服务启用，Spark执行器不再从各个执行器上获取shuffle文件，转而从这个service获取。这意味着，任何执行器输出的混洗状态数据都可能存留时间比对应的执行器进程还长。</p>
<p>除了混洗文件之外，执行器也会在磁盘或者内存中缓存数。一旦执行器被移除，其缓存数据将无法访问。这个问题目前还没有解决。或许在未来的版本中，可能会采用外部混洗服务类似的方法，将缓存数据保存在堆外存储中以解决这一问题。</p>
<h2 id="配置说明"><a href="#配置说明" class="headerlink" title="配置说明"></a>配置说明</h2><p>配置文件：<br>$SPARK_HOME/conf/spark-defaults.conf<br>$HADOOP_HOME/conf/yarn-site.xml</p>
<h3 id="Spark配置说明"><a href="#Spark配置说明" class="headerlink" title="Spark配置说明"></a>Spark配置说明</h3><p>在spark-defaults.conf 中添加<br><figure class="highlight bash"><figcaption><span>spark-defaults.conf</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">spark.shuffle.service.enabled <span class="literal">true</span>   <span class="comment"># 开启外部shuffle服务，开启这个服务可以保护executor的shuffle文件，安全移除executor，在Yarn模式下这个shuffle服务以org.apache.spark.yarn.network.YarnShuffleService实现</span></div><div class="line">spark.shuffle.service.port 7337 <span class="comment"># Shuffle Service服务端口，必须和yarn-site中的一致</span></div><div class="line">spark.dynamicAllocation.enabled <span class="literal">true</span>  <span class="comment"># 开启动态资源分配</span></div><div class="line">spark.dynamicAllocation.minExecutors 1  <span class="comment"># 每个Application最小分配的executor数</span></div><div class="line">spark.dynamicAllocation.maxExecutors 30  <span class="comment"># 每个Application最大并发分配的executor数</span></div><div class="line">spark.dynamicAllocation.schedulerBacklogTimeout 1s <span class="comment"># 任务待时间（超时便申请新资源)默认60秒</span></div><div class="line">spark.dynamicAllocation.sustainedSchedulerBacklogTimeout 5s <span class="comment">#  再次请求等待时间，默认60秒</span></div><div class="line">spark.dynamicAllocation.executorIdleTimeout <span class="comment"># executor闲置时间（超过释放资源）默认600秒</span></div></pre></td></tr></table></figure></p>
<h3 id="yarn的配置"><a href="#yarn的配置" class="headerlink" title="yarn的配置"></a>yarn的配置</h3><h4 id="添加相应的jar包spark-yarn-shuffle-jar"><a href="#添加相应的jar包spark-yarn-shuffle-jar" class="headerlink" title="添加相应的jar包spark--yarn-shuffle.jar"></a>添加相应的jar包spark-<version>-yarn-shuffle.jar</version></h4><p>如果是自己编译的spark，可以在$SPARK_HOME/network/yarn/target/scala-<version>下面找到<br>是预编译的，直接在$SPARK_HOME/lib/下面找到<br>找到jar包后，将其添加到每个nodemanager的classpath下面(或者直接放到yarn的lib目录中,${HADOOP_HOME}/share/hadoop/yarn/lib/)</version></p>
<h4 id="配置yarn-site-xml文件"><a href="#配置yarn-site-xml文件" class="headerlink" title="配置yarn-site.xml文件"></a>配置yarn-site.xml文件</h4><p>在所有节点的yarn-site.xml中，为yarn.nodemanager.aux-services配置项新增spark_shuffle这个值（注意是新增，在原有value的基础上逗号分隔新增即可）<br><figure class="highlight xml"><figcaption><span>yarn-site.xml</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>spark.shuffle.service.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle,spark_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.spark_shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.spark.network.yarn.YarnShuffleService<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure></p>
<h4 id="重启所有的节点"><a href="#重启所有的节点" class="headerlink" title="重启所有的节点"></a>重启所有的节点</h4><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>当开启了动态资源分配（spark.dynamicAllocation.enabled），num-executor选项将不再兼容，如果设置了num-executor，那么动态资源分配将被关闭</p>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p><a href="http://spark.apache.org/docs/1.6.2/job-scheduling.html#dynamic-resource-allocation" target="_blank" rel="external">spark1.6.2作业调度官网</a><br><a href="http://ifeve.com/spark-schedule/" target="_blank" rel="external">spark1.6.2作业调度翻译版</a><br><a href="http://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/" target="_blank" rel="external">Apache Spark Resource Management and YARN App Models</a><br><a href="https://issues.apache.org/jira/browse/YARN-1197" target="_blank" rel="external">jira/browse/YARN-1197–Support changing resources of an allocated container</a><br><a href="http://hejunhao.me/archives/675" target="_blank" rel="external">Spark集群资源动态分配</a><br><a href="http://blog.sina.com.cn/s/blog_a29dec8d0102vfwx.html" target="_blank" rel="external">spark动态资源分配在yarn（hadoop）的配置</a><br><a href="https://www.linkedin.com/pulse/spark-executors%E5%9C%A8yarn%E4%B8%8A%E7%9A%84%E5%8A%A8%E6%80%81%E5%88%86%E9%85%8D-victor-wang" target="_blank" rel="external">Spark Executors在YARN上的动态分配</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[livy-server初探1——简介与提交脚本以及LivyServer类]]></title>
      <url>http://flume.cn/2016/09/13/livy-server%E5%88%9D%E6%8E%A21%E2%80%94%E2%80%94%E7%AE%80%E4%BB%8B%E4%B8%8E%E6%8F%90%E4%BA%A4%E8%84%9A%E6%9C%AC%E4%BB%A5%E5%8F%8ALivyServer%E7%B1%BB/</url>
      <content type="html"><![CDATA[<p><a href="http://livy.io/" target="_blank" rel="external">Livy server</a>是针对Spark的开源的REST接口，使得我们可以通过REST接口来实现与Spark交互,之前应该是Hue框架的一个功能模块，现在已经独立出来啦。具有如下功能：<br>1） 可以与scala、python、R shell客户端交互，执行一些代码片段<br>2） 可以提交整个Spark Job,支持scala、python、java编写的Spark job。</p>
<h2 id="Welcome-to-Livy"><a href="#Welcome-to-Livy" class="headerlink" title="Welcome to Livy"></a>Welcome to Livy</h2><p>下面是官网文档中我对 Welcome to Livy的翻译：</p>
<p>Livy通过提供REST服务来简化与Spark集群的交互。它可以通过job或者代码片段的方式来提交Spark任务，并同步或者异步地获得任务的结果，以及管理spark context，上述功能通过简单的REST接口或者RPC服务来实现。livy也可以简化Spark与一些应用程序之间的交互，使得Spark可以用于一些web应用(比如Hue)。更多的功能包括：</p>
<ul>
<li>拥有长期运行的Spark Contexts供多用户提交各种的Spark job；</li>
<li>不同的任务和用户可以共享cached RDD或者DataFrames；</li>
<li>多个SC可以按计划同时运行，为了使得SC具有更好的容错性和并发性，可以将SC运行在yarn/Mesos等集群中；</li>
<li>可以通过java/scala客户端的API来提交预编译好的jar包或代码片段</li>
<li>支持一定的安全机制</li>
<li>Apache-licensed 100%开源</li>
</ul>
<p>与ReadMe中的文档结合再补充几条：</p>
<ul>
<li>支持Scala，Python，R Shell的交互；</li>
<li>支持 Scala，Java，Python的批量提交；</li>
<li>不需要你对你自己的代码增加任何改变；</li>
</ul>
<p>官网和github逛了一整子后不禁感叹，新东西总是缺乏底层的文档的，所以要了解它就要阅读源码了。</p>
<h2 id="从-bin-livy-server进入"><a href="#从-bin-livy-server进入" class="headerlink" title="从./bin/livy-server进入"></a>从./bin/livy-server进入</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">usage=<span class="string">"Usage: livy-server (start|stop)"</span></div><div class="line"></div><div class="line"><span class="comment"># 指定LIVY_HOME与LIVY_CONF_DIR，上述 `export LIVY_HOME=$(cd $(dirname $0)/.. &amp;&amp; pwd)`这种写法值得学习，代表将LIVY_HOME环境变量设为本脚本的父目录，通过这种写法，增强了脚本的可移植性，另外注明一点，dirname这个命令在命令行里是不能用的，只有写在脚本中才能起作用。</span></div><div class="line"><span class="built_in">export</span> LIVY_HOME=$(<span class="built_in">cd</span> $(dirname <span class="variable">$0</span>)/.. &amp;&amp; <span class="built_in">pwd</span>)</div><div class="line">LIVY_CONF_DIR=<span class="variable">$&#123;LIVY_CONF_DIR:-"$LIVY_HOME/conf"&#125;</span></div><div class="line"></div><div class="line"><span class="comment"># 运行所有的livy-env.sh中的环境变量，并使用set -a 表示输出所有的环境变量的改变</span></div><div class="line"><span class="keyword">if</span> [ <span class="_">-f</span> <span class="string">"<span class="variable">$&#123;LIVY_CONF_DIR&#125;</span>/livy-env.sh"</span> ]; <span class="keyword">then</span></div><div class="line">  <span class="comment"># Promote all variable declarations to environment (exported) variables</span></div><div class="line">  <span class="built_in">set</span> <span class="_">-a</span></div><div class="line">  . <span class="string">"<span class="variable">$&#123;LIVY_CONF_DIR&#125;</span>/livy-env.sh"</span></div><div class="line">  <span class="built_in">set</span> +a</div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure>
<p>接下来可以看到调用了 start_livy_server，以及stop的代码(其实就是ps -p到livy的那个进程，然后kill掉，值得借鉴)：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">option=<span class="variable">$1</span></div><div class="line"></div><div class="line"><span class="keyword">case</span> <span class="variable">$option</span> <span class="keyword">in</span></div><div class="line"></div><div class="line">  (start)</div><div class="line">    start_livy_server <span class="string">"new"</span></div><div class="line">    ;;</div><div class="line"></div><div class="line">  (<span class="string">""</span>)</div><div class="line">    <span class="comment"># make it compatible with previous version of livy-server</span></div><div class="line">    start_livy_server <span class="string">"old"</span></div><div class="line">    ;;</div><div class="line"></div><div class="line">  (stop)</div><div class="line">    <span class="keyword">if</span> [ <span class="_">-f</span> <span class="variable">$pid</span> ]; <span class="keyword">then</span></div><div class="line">      TARGET_ID=<span class="string">"<span class="variable">$(cat "$pid")</span>"</span></div><div class="line">      <span class="keyword">if</span> [[ $(ps -p <span class="string">"<span class="variable">$TARGET_ID</span>"</span> -o comm=) =~ <span class="string">"java"</span> ]]; <span class="keyword">then</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"stopping livy_server"</span></div><div class="line">        <span class="built_in">kill</span> <span class="string">"<span class="variable">$TARGET_ID</span>"</span> &amp;&amp; rm <span class="_">-f</span> <span class="string">"<span class="variable">$pid</span>"</span></div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"no livy_server to stop"</span></div><div class="line">      <span class="keyword">fi</span></div><div class="line">    <span class="keyword">else</span></div><div class="line">      <span class="built_in">echo</span> <span class="string">"no livy_server to stop"</span></div><div class="line">    <span class="keyword">fi</span></div><div class="line">    ;;</div><div class="line"></div><div class="line">  (*)</div><div class="line">    <span class="built_in">echo</span> <span class="variable">$usage</span></div><div class="line">    <span class="built_in">exit</span> 1</div><div class="line">    ;;</div><div class="line"></div><div class="line"><span class="keyword">esac</span></div></pre></td></tr></table></figure>
<p>接下来就是start_livy_server函数了，它做了下面几件事情：</p>
<ol>
<li>找到livy的jar包；</li>
<li>设置LIVY_CLASSPATH并将SPARK与HADOOP以及YARN的CONF_DIR加入到classpath中；</li>
<li>如果是<code>./bin/livy-server</code>启动的程序，就直接运行 “$RUNNER $LIVY_SERVER_JAVA_OPTS -cp $LIVY_CLASSPATH:$CLASSPATH com.cloudera.livy.server.LivyServer”</li>
<li>如果是<code>./bin/livy-server start</code>启动的程序，则增加了日志记录，以方便查看，所以推荐新版本使用带start参数的方式</li>
</ol>
<h2 id="com-cloudera-livy-server-LivyServer"><a href="#com-cloudera-livy-server-LivyServer" class="headerlink" title="com.cloudera.livy.server.LivyServer"></a>com.cloudera.livy.server.LivyServer</h2><p>从后面进入：原来是创建了一个 LivyServer的server，然后start和join启动</p>
<figure class="highlight scala"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">LivyServer</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">val</span> server = <span class="keyword">new</span> <span class="type">LivyServer</span>()</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      server.start()</div><div class="line">      server.join()</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">      server.stop()</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="LiveServer的属性"><a href="#LiveServer的属性" class="headerlink" title="LiveServer的属性"></a>LiveServer的属性</h3><p>LivyServer的属性不多，（与spark源码相比）：</p>
<figure class="highlight scala"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> <span class="type">LivyConf</span>._</div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">var</span> server: <span class="type">WebServer</span> = _</div><div class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _serverUrl: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></div><div class="line">  <span class="comment">// make livyConf accessible for testing</span></div><div class="line">  <span class="keyword">private</span>[livy] <span class="keyword">var</span> livyConf: <span class="type">LivyConf</span> = _</div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">var</span> kinitFailCount: <span class="type">Int</span> = <span class="number">0</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">var</span> executor: <span class="type">ScheduledExecutorService</span> = _</div></pre></td></tr></table></figure>
<h3 id="start-函数"><a href="#start-函数" class="headerlink" title="start()函数"></a>start()函数</h3><p>然后是start()函数了<br>首先，从配置文件中读取配置信息（这一块内容自己写得时候可以借用）：</p>
<ul>
<li>从配置信息中得到host和port</li>
</ul>
<figure class="highlight scala"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">livyConf = <span class="keyword">new</span> <span class="type">LivyConf</span>().loadFromFile(<span class="string">"livy.conf"</span>)</div><div class="line"><span class="keyword">val</span> host = livyConf.get(<span class="type">SERVER_HOST</span>)</div><div class="line"><span class="keyword">val</span> port = livyConf.getInt(<span class="type">SERVER_PORT</span>)</div><div class="line"># 这个而没有看懂</div><div class="line"><span class="keyword">val</span> multipartConfig = <span class="type">MultipartConfig</span>(</div><div class="line">    maxFileSize = <span class="type">Some</span>(livyConf.getLong(<span class="type">LivyConf</span>.<span class="type">FILE_UPLOAD_MAX_SIZE</span>))</div><div class="line">  ).toMultipartConfigElement</div></pre></td></tr></table></figure>
<ul>
<li>测试SparkHome是否设置成功</li>
</ul>
<p>如下代码，这里使用了require方法对参数进行先决条件检测(值得借鉴)<br><figure class="highlight scala"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Make sure the `spark-submit` program exists, otherwise much of livy won't work.</span></div><div class="line">testSparkHome(livyConf)</div><div class="line">...</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">* Sets the spark-submit path if it's not configured in the LivyConf</div><div class="line">*/</div><div class="line"><span class="keyword">private</span>[server] <span class="function"><span class="keyword">def</span> <span class="title">testSparkHome</span></span>(livyConf: <span class="type">LivyConf</span>): <span class="type">Unit</span> = &#123;</div><div class="line"><span class="keyword">val</span> sparkHome = livyConf.sparkHome().getOrElse &#123;</div><div class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"Livy requires the SPARK_HOME environment variable"</span>)</div><div class="line">&#125;</div><div class="line"></div><div class="line">require(<span class="keyword">new</span> <span class="type">File</span>(sparkHome).isDirectory(), <span class="string">"SPARK_HOME path does not exist"</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<ul>
<li>测试spark-submit命令可用，否则livy无法工作(值得借鉴):</li>
</ul>
<p>这里的代码写得太精彩了！先是定义一个<code>$SPAKR_HOME/bin/spark-sumbit --version</code>的命令，使用java的ProcessBuilder，然后可以得到exitCode和重定向的标准输出结果，如果结果是”version …”的话，就代表执行成功，输出结果；然后对这个version进行正则匹配，如果是1.6到2.0版本之间，就返回true，否则就说明spark版本不支持；</p>
<figure class="highlight scala"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line">testSparkSubmit(livyConf)</div><div class="line">...</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">* Test that the configured `spark-submit` executable exists.</div><div class="line">*</div><div class="line">* @param livyConf</div><div class="line">*/</div><div class="line"><span class="keyword">private</span>[server] <span class="function"><span class="keyword">def</span> <span class="title">testSparkSubmit</span></span>(livyConf: <span class="type">LivyConf</span>): <span class="type">Unit</span> = &#123;</div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">  testSparkVersion(sparkSubmitVersion(livyConf))</div><div class="line">&#125; <span class="keyword">catch</span> &#123;</div><div class="line">  <span class="keyword">case</span> e: <span class="type">IOException</span> =&gt;</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">"Failed to run spark-submit executable"</span>, e)</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">...</div><div class="line"><span class="comment">/**</span></div><div class="line">* Return the version of the configured `spark-submit` version.</div><div class="line">*</div><div class="line">* @param livyConf</div><div class="line">* @return the version</div><div class="line">*/</div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">sparkSubmitVersion</span></span>(livyConf: <span class="type">LivyConf</span>): <span class="type">String</span> = &#123;</div><div class="line"><span class="keyword">val</span> sparkSubmit = livyConf.sparkSubmit()</div><div class="line"><span class="keyword">val</span> pb = <span class="keyword">new</span> <span class="type">ProcessBuilder</span>(sparkSubmit, <span class="string">"--version"</span>)</div><div class="line">pb.redirectErrorStream(<span class="literal">true</span>)</div><div class="line">pb.redirectInput(<span class="type">ProcessBuilder</span>.<span class="type">Redirect</span>.<span class="type">PIPE</span>)</div><div class="line"></div><div class="line"><span class="keyword">if</span> (<span class="type">LivyConf</span>.<span class="type">TEST_MODE</span>) &#123;</div><div class="line">  pb.environment().put(<span class="string">"LIVY_TEST_CLASSPATH"</span>, sys.props(<span class="string">"java.class.path"</span>))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">val</span> process = <span class="keyword">new</span> <span class="type">LineBufferedProcess</span>(pb.start())</div><div class="line"><span class="keyword">val</span> exitCode = process.waitFor()</div><div class="line"><span class="keyword">val</span> output = process.inputIterator.mkString(<span class="string">"\n"</span>)</div><div class="line"></div><div class="line"><span class="keyword">val</span> regex = <span class="string">""</span><span class="string">"version (.*)"</span><span class="string">""</span>.r.unanchored</div><div class="line"></div><div class="line">output <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> regex(version) =&gt; version</div><div class="line">  <span class="keyword">case</span> _ =&gt;</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">f"Unable to determine spark-submit version [<span class="subst">$exitCode</span>]:\n<span class="subst">$output</span>"</span>)</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">* Throw an exception if Spark version is not supported.</div><div class="line">* @param version Spark version</div><div class="line">*/</div><div class="line"><span class="keyword">private</span>[server] <span class="function"><span class="keyword">def</span> <span class="title">testSparkVersion</span></span>(version: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</div><div class="line"><span class="keyword">val</span> versionPattern = <span class="string">""</span><span class="string">"(\d)+\.(\d)+(?:\.\d*)?"</span><span class="string">""</span>.r</div><div class="line"><span class="comment">// This is exclusive. Version which equals to this will be rejected.</span></div><div class="line"><span class="keyword">val</span> maxVersion = (<span class="number">2</span>, <span class="number">0</span>)</div><div class="line"><span class="keyword">val</span> minVersion = (<span class="number">1</span>, <span class="number">6</span>)</div><div class="line"></div><div class="line"><span class="keyword">val</span> supportedVersion = version <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> versionPattern(major, minor) =&gt;</div><div class="line">    <span class="keyword">val</span> v = (major.toInt, minor.toInt)</div><div class="line">    v &gt;= minVersion &amp;&amp; v &lt; maxVersion</div><div class="line">  <span class="keyword">case</span> _ =&gt; <span class="literal">false</span></div><div class="line">&#125;</div><div class="line">require(supportedVersion, <span class="string">s"Unsupported Spark version <span class="subst">$version</span>."</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>通过livy.spark.master是否以yarn开头判断是否需要初始化YarnClient</li>
</ul>
<figure class="highlight scala"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Initialize YarnClient ASAP to save time.</span></div><div class="line"><span class="keyword">if</span> (livyConf.isRunningOnYarn()) &#123;</div><div class="line">  <span class="type">Future</span> &#123; <span class="type">SparkYarnApp</span>.yarnClient &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// SparkYarnApp.scala</span></div><div class="line"><span class="comment">// YarnClient is thread safe. Create once, share it across threads.</span></div><div class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> yarnClient = &#123;</div><div class="line">  <span class="keyword">val</span> c = <span class="type">YarnClient</span>.createYarnClient() <span class="comment">// 这里调用的是yarn提供的API</span></div><div class="line">  c.init(<span class="keyword">new</span> <span class="type">YarnConfiguration</span>())</div><div class="line">  c.start()</div><div class="line">  c</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>接下来启动WebServer</li>
</ul>
<p>这个webServer是Jetty的WebServer，通过设置是否使用ssl的配置来判断启动的是http server还是 https server，也会判断有没有Kerberos，设置IP，端口，日志等。（以后写得时候得查看Jetty的API和文档）</p>
<figure class="highlight scala"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">server = <span class="keyword">new</span> <span class="type">WebServer</span>(livyConf, host, port)</div><div class="line">server.context.setResourceBase(<span class="string">"src/main/com/cloudera/livy/server"</span>)</div><div class="line">server.context.addEventListener(</div><div class="line">  <span class="keyword">new</span> <span class="type">ServletContextListener</span>() <span class="keyword">with</span> <span class="type">MetricsBootstrap</span> <span class="keyword">with</span> <span class="type">ServletApiImplicits</span> &#123;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">mount</span></span>(sc: <span class="type">ServletContext</span>, servlet: <span class="type">Servlet</span>, mappings: <span class="type">String</span>*): <span class="type">Unit</span> = &#123;</div><div class="line">      <span class="keyword">val</span> registration = sc.addServlet(servlet.getClass().getName(), servlet)</div><div class="line">      registration.addMapping(mappings: _*)</div><div class="line">      registration.setMultipartConfig(multipartConfig)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">contextDestroyed</span></span>(sce: <span class="type">ServletContextEvent</span>): <span class="type">Unit</span> = &#123;</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">contextInitialized</span></span>(sce: <span class="type">ServletContextEvent</span>): <span class="type">Unit</span> = &#123;</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        <span class="keyword">val</span> context = sce.getServletContext()</div><div class="line">        context.initParameters(org.scalatra.<span class="type">EnvironmentKey</span>) = livyConf.get(<span class="type">ENVIRONMENT</span>)</div><div class="line">        mount(context, <span class="keyword">new</span> <span class="type">InteractiveSessionServlet</span>(livyConf), <span class="string">"/sessions/*"</span>)</div><div class="line">        mount(context, <span class="keyword">new</span> <span class="type">BatchSessionServlet</span>(livyConf), <span class="string">"/batches/*"</span>)</div><div class="line">        context.mountMetricsAdminServlet(<span class="string">"/"</span>)</div><div class="line">      &#125; <span class="keyword">catch</span> &#123;</div><div class="line">        <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">          error(<span class="string">"Exception thrown when initializing server"</span>, e)</div><div class="line">          sys.exit(<span class="number">1</span>)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">  &#125;)</div></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark在Kerberos环境下指定任意用户在yarn上提交任务]]></title>
      <url>http://flume.cn/2016/09/08/Spark%E5%9C%A8Kerberos%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%8C%87%E5%AE%9A%E4%BB%BB%E6%84%8F%E7%94%A8%E6%88%B7%E5%9C%A8yarn%E4%B8%8A%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1/</url>
      <content type="html"><![CDATA[<p>众所周知，Spark在Kerberos环境下提交任务有两种方式，分别是先kinit的方式和通过 –keytab的方式：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[op]$ spark-submit --keytab <span class="built_in">test</span>Jars/op.keytab --principal op --master <span class="built_in">local</span> --class SparkPi ./<span class="built_in">test</span>Jars/my.jar 4</div></pre></td></tr></table></figure>
<p>Spark在Kerberos环境下可以在提交任务时通过指定用户的keytab和principal来提交任务，比如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 事先进行kinit的方式</span></div><div class="line">[op]$ kinit -kt op.keytab op</div><div class="line">[op]$ spark-submit --master <span class="built_in">local</span> --class SparkPi ./<span class="built_in">test</span>Jars/my.jar 4</div><div class="line"></div><div class="line"><span class="comment"># 提交keytab的方式</span></div><div class="line">[op]$ spark-submit --keytab <span class="built_in">test</span>Jars/op.keytab --principal op --master <span class="built_in">local</span> --class SparkPi ./<span class="built_in">test</span>Jars/my.jar 4</div></pre></td></tr></table></figure>
<p>其实还可以模拟其它用户的方式提交任务，比如使用ts账户提交：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[op]$ spark-submit --keytab <span class="built_in">test</span>Jars/ts.keytab --principal ts@HADOOP.CHINATELECOM.CN --master <span class="built_in">local</span> --class SparkPi ./<span class="built_in">test</span>Jars/my.jar 4</div></pre></td></tr></table></figure>
<p>当然没有那么简单，如果想要使用ts账户执行程序，需要进行如下设置：</p>
<h4 id="模拟其它用户需要的条件"><a href="#模拟其它用户需要的条件" class="headerlink" title="模拟其它用户需要的条件"></a>模拟其它用户需要的条件</h4><ol>
<li>ts要在KDC下生成对应的keytab和principal；</li>
<li>要在hadoop集群的所有机器上创建ts账户：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo groupadd ts</div><div class="line">sudo useradd -g ts ts</div></pre></td></tr></table></figure>
<p>值得注意的是，如果要在yarn中模拟其它用户执行，需要在集群中所有机器上增加该用户。</p>
<p>后期有时间了详细说明原因。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark Web与history web配置与测试]]></title>
      <url>http://flume.cn/2016/09/07/Spark-Web%E4%B8%8Ehistory%E6%B5%8B%E8%AF%95/</url>
      <content type="html"><![CDATA[<h2 id="Spark-Web的查看"><a href="#Spark-Web的查看" class="headerlink" title="Spark Web的查看"></a>Spark Web的查看</h2><ol>
<li>运行任意一个yarn-client或者yarn-cluster模式的spark测试用例</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">cd</span> <span class="variable">$SPARK_HOME</span></div><div class="line">$ spark-submit --keytab <span class="built_in">test</span>Jars/op.keytab --principal op --master yarn-client --class SparkPi ./<span class="built_in">test</span>Jars/my.jar 4</div></pre></td></tr></table></figure>
<ol>
<li>打开<a href="http://yarn-host:8088/cluster页面，找到正在运行的Spark测试用例" target="_blank" rel="external">http://yarn-host:8088/cluster页面，找到正在运行的Spark测试用例</a></li>
</ol>
<img src="/2016/09/07/Spark-Web与history测试/spark-yarn-web1.png" alt="spark-yarn-web1.png" title="">
<p>点击上图所示的AM，就进入了Spark的Web界面：下图就是Spark程序的web界面，值得注意的是，这个web界面会随着spark程序的运行结束而消失<br><img src="/2016/09/07/Spark-Web与history测试/spark-yarn-web2.png" alt="spark-yarn-web2.png" title=""></p>
<h2 id="Spark-history-Web查看测试"><a href="#Spark-history-Web查看测试" class="headerlink" title="Spark history Web查看测试"></a>Spark history Web查看测试</h2><p>在Kerberos环境下要启动spark history配置，需要在 spark -env下面开启如下配置 SPARK_HISTORY_OPTS：</p>
<figure class="highlight bash"><figcaption><span>spark-env.sh</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># history需要的Kerberos配置</span></div><div class="line">SPARK_HISTORY_OPTS=<span class="string">"-Dspark.history.ui.port=8777 -Dspark.history.retainedApplications=10 -Dspark.history.fs.logDirectory=hdfs://ns/user/op/sparkHistoryServer -Dspark.history.kerberos.enabled=true -Dspark.history.kerberos.principal=op @HADOOP.CHINATELECOM.CN -Dspark.history.kerberos.keytab=/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/conf/op.keytab"</span></div></pre></td></tr></table></figure>
<p>然后通过 ./sbin/start-history-server.sh 命令启动history-server<br>然后登录 <a href="http://spark-client-ip:8777/" target="_blank" rel="external">http://spark-client-ip:8777/</a> 即可查看 spark-history-web</p>
<img src="/2016/09/07/Spark-Web与history测试/spark-yarn-web3.png" alt="spark-yarn-web3.png" title="">]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[kerberos下spark客户端的配置]]></title>
      <url>http://flume.cn/2016/09/06/kerberos%E4%B8%8Bspark%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%9A%84%E9%85%8D%E7%BD%AE/</url>
      <content type="html"><![CDATA[<p>Kerberos环境下spark的客户端配置并不是很多，主要需要配置的是spark-history与spark-sql</p>
<p>软件版本：spark-1.6.2</p>
<p>注：正式环境中，需要将spark客户端的路径放入其它短路经，比如 /etc/local/spark 等<br><figure class="highlight bash"><figcaption><span>spark-env.sh</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 由于此处的 hive-site.xml 需要做一定修改，所以需要将hive-site.xml core-site.xml hdfs-site.xml yarn-site.xml等导入conf文件夹下</span></div><div class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/conf</div><div class="line"></div><div class="line"><span class="built_in">export</span> JAVA_HOME=/usr/java/jdk1.7.0_75</div><div class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$LD_LIBRARY_PATH</span>:/usr/lib/hadoop/lib/native/</div><div class="line"><span class="built_in">export</span> SPARK_LIBRARY_PATH=/usr/lib/hadoop/lib/native/:<span class="variable">$SPARK_LIBRARY_PATH</span></div><div class="line"></div><div class="line"><span class="comment"># history需要的Kerberos配置</span></div><div class="line">SPARK_HISTORY_OPTS=<span class="string">"-Dspark.history.ui.port=8777 -Dspark.history.retainedApplications=10 -Dspark.history.fs.logDirectory=hdfs://ns/user/op/sparkHistoryServer -Dspark.history.kerberos.enabled=true -Dspark.history.kerberos.principal=op    @HADOOP.CHINATELECOM.CN -Dspark.history.kerberos.keytab=/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/conf/op.keytab"</span></div></pre></td></tr></table></figure></p>
<h4 id="从hive-keytab-hiveserver创建spark-thrift-server的keytab"><a href="#从hive-keytab-hiveserver创建spark-thrift-server的keytab" class="headerlink" title="从hive.keytab_hiveserver创建spark-thrift-server的keytab"></a>从hive.keytab_hiveserver创建spark-thrift-server的keytab</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">-rw------- 1 hive hive     424 8月  23 09:55 hive.keytab_hiveserver</div><div class="line">-rw------- 1 op   bigdata  424 9月   3 12:25 hive.keytab_sparkthrift</div></pre></td></tr></table></figure>
<h4 id="hive-site的配置"><a href="#hive-site的配置" class="headerlink" title="hive-site的配置"></a>hive-site的配置</h4><p>修改hive-site.xml：</p>
<ul>
<li>增加hive.server2.thrift.bind.host</li>
<li>修改hive.server2.thrift.port为10010</li>
<li>修改hive.server2.authentication.kerberos.keytab为如下</li>
</ul>
<figure class="highlight bash"><figcaption><span>hive-site.xml</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">145 &lt;!-- ZooKeeper conf--&gt;</div><div class="line">146 &lt;property&gt;</div><div class="line">147   &lt;name&gt;hive.server2.enable.doAs&lt;/name&gt;</div><div class="line">148   &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</div><div class="line">149   &lt;description&gt; Impersonate the connected user &lt;/description&gt;</div><div class="line">150 &lt;/property&gt;</div><div class="line">151 &lt;property&gt;</div><div class="line">152   &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</div><div class="line">153   &lt;value&gt;10010&lt;/value&gt;</div><div class="line">154   &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class="line">155 &lt;/property&gt;</div><div class="line">156 </div><div class="line">157 &lt;property&gt;</div><div class="line">158    &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</div><div class="line">159    &lt;value&gt;<span class="built_in">test</span>-bdd-076&lt;/value&gt;</div><div class="line">160    &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class="line">161  &lt;/property&gt;</div><div class="line">162 </div><div class="line">163 &lt;property&gt;</div><div class="line">164   &lt;name&gt;hive.metastore.execute.setugi&lt;/name&gt;</div><div class="line">165   &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</div><div class="line">166 &lt;/property&gt;</div><div class="line">...</div><div class="line">209 &lt;property&gt;</div><div class="line">210    &lt;name&gt;hive.server2.authentication.kerberos.principal&lt;/name&gt;</div><div class="line">211    &lt;value&gt;hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN&lt;/value&gt;</div><div class="line">212  &lt;/property&gt;</div><div class="line">213 &lt;property&gt;</div><div class="line">214   &lt;name&gt;hive.server2.authentication.kerberos.keytab&lt;/name&gt;</div><div class="line">215   &lt;value&gt;/etc/hive/conf/hive.keytab_sparkthrift&lt;/value&gt;</div><div class="line">216 &lt;/property&gt;</div></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[在Kerberos环境下配置hue通过spark-thrift-server访问SparkSql]]></title>
      <url>http://flume.cn/2016/09/05/%E5%9C%A8Kerberos%E7%8E%AF%E5%A2%83%E4%B8%8B%E9%85%8D%E7%BD%AEhue%E9%80%9A%E8%BF%87spark-thrift-server%E8%AE%BF%E9%97%AESparkSql/</url>
      <content type="html"><![CDATA[<p>hue-spark-thriftserver-kerberos</p>
<h3 id="背景说明"><a href="#背景说明" class="headerlink" title="背景说明"></a>背景说明</h3><p>Kerberos项目最后要对基于Hue的TODP平台进行安全测试，在搭建配置的过程中踩了一些坑，现在把其中的配置与步骤进行总结，以免以后忘记。</p>
<p>其中用到以下代号：<br>40机器：hue平台所在的机器<br>76机器：spark thrift服务端口10010，hive-thrift-server服务端口10000<br>74机器：spark thrift服务端口10010，hive-thrift-server服务端口10000<br>TEST-BDD-HIVESERVER机器：负载均衡所在的机器，负载均衡机器需要配合开启10000和10010端口</p>
<p>在kerberos认证下, sparksql的thriftserver连接hiveserver2变得相对复杂，主要是因为各种kerberos认证出现各种问题。后来由于hive使用了负载均衡，所以spark-sql也需加入负载均衡，否则不能使用，就是这个负载均衡服务器的加入使得kerberos认证变得更加复杂，使得不明原理的新手在配置kerberos的keytab与principal时各种不匹配。这里是通过Hue可视化界面调用后台的sparksql,然后sparksql通过JDBC连接Hive的hiveServer2服务。</p>
<h3 id="40机器hue端配置"><a href="#40机器hue端配置" class="headerlink" title="40机器hue端配置"></a>40机器hue端配置</h3><p>进入40机器hue所在的目录<br><figure class="highlight bash"><figcaption><span>hue.ini</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">cd</span> /usr/lib/hue/ </div><div class="line">$ vim desktop/conf/hue.ini</div></pre></td></tr></table></figure></p>
<p>修改hue的配置文件如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">1119 [spark]</div><div class="line">...</div><div class="line">1134   <span class="comment"># spark-sql config</span></div><div class="line">1135   spark_sql_server_host=TEST-BDD-HIVESERVER</div><div class="line">1136   <span class="comment">## spark_sql_server_port=10010</span></div></pre></td></tr></table></figure></p>
<p>由于此处使用了负载均衡，所以上述TEST-BDD-HIVESERVER指向的是负载均衡所在的ip，最终会转发给两个spark-thrift-server</p>
<h3 id="Kerberos服务器端配置"><a href="#Kerberos服务器端配置" class="headerlink" title="Kerberos服务器端配置"></a>Kerberos服务器端配置</h3><p>生成类似 hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN 的keytab，配置了负载均衡后，使用test-bdd-hiveserver</p>
<h3 id="76机器上的配置"><a href="#76机器上的配置" class="headerlink" title="76机器上的配置"></a>76机器上的配置</h3><p>76机器与74机器配置步骤一样，只是hive-site.xml需要改一处，将下面的 076改成 074即可<br><figure class="highlight bash"><figcaption><span>hive-site.xml</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">   &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</div><div class="line">   &lt;value&gt;<span class="built_in">test</span>-bdd-076&lt;/value&gt;</div><div class="line">   &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure></p>
<p>其它都一样，所以在这里只写076的配置步骤</p>
<h4 id="从hive-keytab创建spark的keytab"><a href="#从hive-keytab创建spark的keytab" class="headerlink" title="从hive.keytab创建spark的keytab"></a>从hive.keytab创建spark的keytab</h4><p>然后在/etc/hive/conf/下创建spark需要的keytab，在这里使用hiveserver的keytab，将已有的hive.keytab_hiveserver 拷贝成 hive.keytab_sparkthrift，然后修改权限如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">-rw------- 1 hive hive     424 8月  23 09:55 hive.keytab_hiveserver</div><div class="line">-rw------- 1 op   bigdata  424 9月   3 12:25 hive.keytab_sparkthrift</div></pre></td></tr></table></figure>
<p>修改好后用如下命令检查：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">$ sudo klist -k hive.keytab_sparkthrift </div><div class="line">Keytab name: FILE:hive.keytab_sparkthrift</div><div class="line">KVNO Principal</div><div class="line">---- --------------------------------------------------------------------------</div><div class="line">   1 hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div><div class="line">   1 hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div><div class="line">   1 hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div><div class="line">   1 hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div><div class="line">   1 hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div></pre></td></tr></table></figure>
<p>如果klist是如上结果，就对了</p>
<h4 id="配置spark需要的hive-site-xml"><a href="#配置spark需要的hive-site-xml" class="headerlink" title="配置spark需要的hive-site.xml"></a>配置spark需要的hive-site.xml</h4><p>由于需要修改hive的一些配置，进入76机器spark所在的目录，将<code>/etc/hive/conf/</code>下的<code>hive-site.xml</code>拷贝到spark的conf下，赋予权限并修改<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ sudo cp /etc/hive/conf/hive-site.xml <span class="variable">$SPARK_HOME</span>/conf/</div><div class="line">$ <span class="built_in">cd</span> <span class="variable">$SPARK_HOME</span></div><div class="line">$ sudo chmod op conf/hive-site.xml</div><div class="line">$ vim conf/hive-site.xml</div></pre></td></tr></table></figure></p>
<p>修改hive-site.xml,增加hive.server2.thrift.bind.host</p>
<figure class="highlight bash"><figcaption><span>hive-site.xml</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">&lt;!-- ZooKeeper conf--&gt;</div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;hive.server2.enable.doAs&lt;/name&gt;</div><div class="line">  &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</div><div class="line">  &lt;description&gt; Impersonate the connected user &lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</div><div class="line">  &lt;value&gt;10010&lt;/value&gt;</div><div class="line">  &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line">&lt;property&gt;</div><div class="line">   &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</div><div class="line">   &lt;value&gt;<span class="built_in">test</span>-bdd-076&lt;/value&gt;</div><div class="line">   &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class="line"> &lt;/property&gt;</div><div class="line"></div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;hive.metastore.execute.setugi&lt;/name&gt;</div><div class="line">  &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">   &lt;name&gt;hive.server2.authentication.kerberos.principal&lt;/name&gt;</div><div class="line">   &lt;value&gt;hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN&lt;/value&gt;</div><div class="line"> &lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;hive.server2.authentication.kerberos.keytab&lt;/name&gt;</div><div class="line">  &lt;value&gt;/etc/hive/conf/hive.keytab_sparkthrift&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line"><span class="comment">#### 启动Spark-thrift-server</span></div><div class="line">``` bash</div><div class="line">$ <span class="built_in">cd</span> <span class="variable">$SPARK_HOME</span></div><div class="line">$ ./sbin/start-thriftserver.sh</div></pre></td></tr></table></figure>
<p>可以通过如下日志查看是否启动成功：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ vim logs/spark-op-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-TEST-BDD-076.out</div></pre></td></tr></table></figure></p>
<p>启动成功会看到如下日志:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"> 96 16/09/05 13:41:25 INFO AbstractService: Service:HiveServer2 is started.</div><div class="line"> 97 16/09/05 13:41:25 INFO HiveThriftServer2: HiveThriftServer2 started</div><div class="line"> 98 16/09/05 13:41:25 INFO UserGroupInformation: Login successful <span class="keyword">for</span> user hive/<span class="built_in">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN using keytab file /etc/hive/conf/hive.keytab_sparkthrift</div><div class="line"> 99 16/09/05 13:41:25 INFO AbstractDelegationTokenSecretManager: Updating the current master key <span class="keyword">for</span> generating delegation tokens</div><div class="line">100 16/09/05 13:41:25 INFO TokenStoreDelegationTokenSecretManager: New master key with key id=0</div><div class="line">101 16/09/05 13:41:25 INFO TokenStoreDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)</div><div class="line">102 16/09/05 13:41:25 INFO AbstractDelegationTokenSecretManager: Updating the current master key <span class="keyword">for</span> generating delegation tokens</div><div class="line">103 16/09/05 13:41:25 INFO TokenStoreDelegationTokenSecretManager: New master key with key id=1</div><div class="line">104 16/09/05 13:41:25 INFO ThriftCLIService: Starting ThriftBinaryCLIService on port 10010 with 5...500 worker threads</div></pre></td></tr></table></figure></p>
<h3 id="负载均衡机器的查看"><a href="#负载均衡机器的查看" class="headerlink" title="负载均衡机器的查看"></a>负载均衡机器的查看</h3><p>进入 67.121机器<br>输入 命令 <code>sudo ipvsadm -ln</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">$ sudo ipvsadm -ln</div><div class="line">IP Virtual Server version 1.2.1 (size=4194304)</div><div class="line">Prot LocalAddress:Port Scheduler Flags</div><div class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</div><div class="line">TCP  10.142.67.123:10000 wlc persistent 7200 synproxy</div><div class="line">  -&gt; 10.142.78.74:10000           FullNat 50     3          0         </div><div class="line">  -&gt; 10.142.78.76:10000           FullNat 50     0          0         </div><div class="line">TCP  10.142.67.123:10010 wlc persistent 7200 synproxy</div><div class="line">  -&gt; 10.142.78.74:10010           FullNat 50     0          0         </div><div class="line">  -&gt; 10.142.78.76:10010           FullNat 50     2          0</div></pre></td></tr></table></figure>
<p>就可以看到负载均衡的情况了：</p>
<h3 id="踩坑说明以及解决方案"><a href="#踩坑说明以及解决方案" class="headerlink" title="踩坑说明以及解决方案"></a>踩坑说明以及解决方案</h3><h4 id="缺少配置kerberos认证错误"><a href="#缺少配置kerberos认证错误" class="headerlink" title="缺少配置kerberos认证错误"></a>缺少配置kerberos认证错误</h4><p>需要在hive-site.xml文件中添加kerberos认证相关配置</p>
<h4 id="kerberos认证失败"><a href="#kerberos认证失败" class="headerlink" title="kerberos认证失败"></a>kerberos认证失败</h4><p>1)  在hive-site.xml中配置好kerberos认证，但是op用户下无法读取hive.keytab的问题，出现unable to login …given keytab/principal 以及Unable to obtain password from user。因为hive.keytab 是hive用户创建的，op用户无法读取，导致看似kerberos已经配置好，<br>但是程序没有读取权限，依旧认为没有配置好，这是会有在日志文件中会有NULLPOINT类似的错误提示，说明是没有读取权限。解决方案是复制hive.keytab到op用户下。<br>2）在hue界面连接spark时可能会出现10010端口不能连接的问题，这是sparkthrift没有启动导致的；<br>3）spark thriftserver明明已经启动，但是hue界面仍旧不能连接，出现TTransportException的错误，原因是kerberos配置没有配置正确，即没有配置kerberos认证的keytab与principal。hive/test-bdd-hiveserver必须与hive.keytab_hiveserver配套使用，同理，test-bdd-074或者 test-bdd-076必须与hive/test-bdd-74或者hive/test-bdd-76配套使用，否则出现认证失败的问题。</p>
<h4 id="hue的配置问题。"><a href="#hue的配置问题。" class="headerlink" title="hue的配置问题。"></a>hue的配置问题。</h4><p>在hue的desktop/conf目录下hue.ini文件中，主要配置spark_sql_server_host，也就是spark thriftserver所在主机，这里可以是负载均衡服务器TEST-BDD-HIVESERVER,spark_sql_server_port 是spark thriftserver的服务端口。<br>需要注意的是，加上kerberos认证后，主机名不能是ip地址的形式，需要FQDN的形式。hive的配置需要注意的是hive_server_host，这里绝对不能是hiveserver2的服务器的地址，一定是负载均衡服务器的地址，不然在hue界面连接HIVE时出现<br>Unable to access databases, Query Server or Metastore may be down.的错误以及GSS initial failed的错误，无法访问hive数据库。</p>
<h4 id="metastore的问题"><a href="#metastore的问题" class="headerlink" title="metastore的问题"></a>metastore的问题</h4><p>连接metastore也需要principal的认证。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">20 &lt;property&gt;</div><div class="line">221   &lt;name&gt;hive.metastore.sasl.enabled&lt;/name&gt;</div><div class="line">222   &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</div><div class="line">223   &lt;description&gt;If <span class="literal">true</span>, the metastore thrift interface will be secured with SASL. Clients must authenticate with Kerberos.&lt;/description&gt;</div><div class="line">224 &lt;/property&gt;</div><div class="line">225 &lt;property&gt;</div><div class="line">226   &lt;name&gt;hive.metastore.kerberos.principal&lt;/name&gt;</div><div class="line">227   &lt;value&gt;hive/_HOST@HADOOP.CHINATELECOM.CN&lt;/value&gt;</div><div class="line">228   &lt;description&gt;The service principal <span class="keyword">for</span> the metastore thrift server. The special string _HOST will be replaced automatically with the correct host name.&lt;/description&gt;</div><div class="line">229 &lt;/property&gt;</div></pre></td></tr></table></figure></p>
<p>之所以问题多多，主要原因是对kerberos+Hive+lvs整体原理没有搞清楚，以至于在配置过程中出现各种错误。我们搭建的hive集群有74,76两台主机，spark thriftserver也有74,76两台主机，负载均衡服务器在test-bdd-hiveserver上。在配置时，需要将spark-sql-server-host配置成test-bdd-hiveserver,因为对spark而言，74与76上的hiveserver是一个整体，不能配置成单一的主机，不然lvs可能会将服务分到另外一台主机上，造成主机配置失败。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[spark支持snappy压缩踩坑总结]]></title>
      <url>http://flume.cn/2016/08/15/spark%E6%94%AF%E6%8C%81snappy%E5%8E%8B%E7%BC%A9%E8%B8%A9%E5%9D%91%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<h3 id="配置snappy压缩"><a href="#配置snappy压缩" class="headerlink" title="配置snappy压缩"></a>配置snappy压缩</h3><p>首先在/usr/lib/hadoop/lib/目录下配置lzo相关的包，<br>然后在spark客户端配置如下：</p>
<figure class="highlight bash"><figcaption><span>spark-default.conf</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">spark.driver.extraClassPath /usr/lib/hadoop/lib/*</div><div class="line">spark.driver.extraLibraryPath /usr/lib/hadoop/lib/native</div><div class="line">spark.executor.extraClassPath /usr/lib/hadoop/lib/*</div><div class="line">spark.executor.extraLibraryPath /usr/lib/hadoop/lib/native</div></pre></td></tr></table></figure>
<p>如上配置，即可，但是为了得到这么小小的一点配置，浪费了三天的时间啊，网上的资料都是转载，无法解决问题。最新的官网的配置文件中并没有关于spark.executor.extraClassPath的配置，查了源码才得知，作为教训。以后出现问题要冷静思考，不要简单的去网上搜索，先判断问题出现的原因，知其所以然，必要时要去源码中查询，否则会浪费很多时间，走很多弯路。</p>
<h3 id="踩坑集锦"><a href="#踩坑集锦" class="headerlink" title="踩坑集锦"></a>踩坑集锦</h3><p>首先，会遇到这个错误：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Compression codec com.hadoop.compression.lzo.LzoCodec not found</div></pre></td></tr></table></figure>
<p>原因是spark-env.sh的配置文件缺少关联hadoop的配置语句</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> SPARK_LIBRARY_PATH=<span class="variable">$SPARK_LIBRARY_PATH</span>:/usr/lib/hadoop/lib/native/:/usr/lib/hadoop/lib/*</div></pre></td></tr></table></figure>
<p>然后yarn-cluster模式下snappy压缩总会报错：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="number">16</span>/<span class="number">08</span>/<span class="number">08</span> <span class="number">19</span>:<span class="number">05</span>:<span class="number">03</span> DEBUG util.NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib</div><div class="line"><span class="number">16</span>/<span class="number">08</span>/<span class="number">08</span> <span class="number">19</span>:<span class="number">05</span>:<span class="number">03</span> WARN util.NativeCodeLoader: Unable to load <span class="keyword">native</span>-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</div><div class="line"><span class="number">1</span>）</div><div class="line"><span class="number">16</span>/<span class="number">08</span>/<span class="number">08</span> <span class="number">19</span>:<span class="number">05</span>:<span class="number">03</span> DEBUG util.PerformanceAdvisory: Both <span class="keyword">short</span>-circuit local reads and UNIX domain socket are disabled.</div><div class="line"><span class="number">16</span>/<span class="number">08</span>/<span class="number">08</span> <span class="number">19</span>:<span class="number">05</span>:<span class="number">03</span> DEBUG sasl.DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration <span class="keyword">for</span> dfs.data.transfer.protection</div><div class="line"><span class="number">16</span>/<span class="number">08</span>/<span class="number">08</span> <span class="number">19</span>:<span class="number">05</span>:<span class="number">03</span> ERROR lzo.GPLNativeCodeLoader: Could not load <span class="keyword">native</span> gpl library</div><div class="line">java.lang.UnsatisfiedLinkError: no gplcompression in java.library.path</div><div class="line"> at java.lang.ClassLoader.loadLibrary(ClassLoader.java:<span class="number">1886</span>)</div><div class="line"> at java.lang.Runtime.loadLibrary0(Runtime.java:<span class="number">849</span>)</div><div class="line"> at java.lang.System.loadLibrary(System.java:<span class="number">1088</span>)</div><div class="line"> at com.hadoop.compression.lzo.GPLNativeCodeLoader.&lt;clinit&gt;(GPLNativeCodeLoader.java:<span class="number">32</span>)</div><div class="line"> at com.hadoop.compression.lzo.LzoCodec.&lt;clinit&gt;(LzoCodec.java:<span class="number">71</span>)</div><div class="line"> at java.lang.Class.forName0(Native Method)</div><div class="line"> at java.lang.Class.forName(Class.java:<span class="number">274</span>)</div><div class="line"> at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:<span class="number">2013</span>)</div><div class="line"> at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:<span class="number">1978</span>)</div><div class="line"> at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:<span class="number">128</span>)</div><div class="line"> at org.apache.hadoop.io.compress.CompressionCodecFactory.&lt;init&gt;(CompressionCodecFactory.java:<span class="number">175</span>)</div><div class="line"> at org.apache.hadoop.mapred.TextInputFormat.configure(TextInputFormat.java:<span class="number">45</span>)</div><div class="line"> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</div></pre></td></tr></table></figure>
<p>这个究其原因就是程序运行的那个节点找不到lzo解压的包导致的，官网中只说明了 spark.driver.extraClassPath，但并没有说明配置spark.executor.extraClassPath 与 spark.executor.extraLibraryPath，导致不管怎么根据网上博客或者官网配置配，executor还是找不到lzo压缩相关的包，后来聪哥通过源码查看才发现有这么一个参数配置，只是各类文档中都没有，加上就ok了~</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Java中的“钩子”]]></title>
      <url>http://flume.cn/2016/07/18/Java%E4%B8%AD%E7%9A%84%E2%80%9C%E9%92%A9%E5%AD%90%E2%80%9D/</url>
      <content type="html"><![CDATA[<p>最近看银辉大哥写的对hdfs中小文件打包成大文件的程序的时候，发现他在代码中巧妙地运用了“钩子”，是用匿名内部类来实现的，感觉很酷，所以决定好好向大神学习一下使用匿名内部类实现钩子的用法：</p>
<h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>为了让我能够快速了解这个方法的使用，银辉大哥首先给我出个题：<br>比如上医院看病，一般会有　挂号，问诊，开药，付费，拿药　几个过程，但是不同的病科室不同，大夫不同，药方不同，付费方式不同，取药方式不同。写一个程序，打印不同的看病流程：如一个人感冒：挂呼吸科，看张大夫，开了砒霜，支付宝支付，快递拿药。<br>另外一个人胃痛，挂了内科，看了王大夫，开了鹤顶红，没有付钱，直接抢药。</p>
<h4 id="模板方法"><a href="#模板方法" class="headerlink" title="模板方法"></a>模板方法</h4><p>模板方法模式（Template Method）：定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。该模式使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。<br>使用场景：<br>1、一次性实现一个算法的不变的部分，并将可变的行为留给子类来实现。<br>2、各子类中公共的行为应被提取出来并集中到一个公共父类中以避免代码重复。即“重分解以一般化”，首先识别现有代码中的不同之处，并且将不同之处分离为新的操作。最后，用一个调用这些新的操作的模板方法来替换这些不同的代码。<br>3、控制子类扩展。模板方法只在特定点调用“Hook Method（钩子方法）”操作，这样就只允许在这些点进行扩展。</p>
<img src="/2016/07/18/Java中的“钩子”/hook.jpg" alt="hook.jpg" title="">
<h4 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h4><h5 id="interface-GoHospital-java"><a href="#interface-GoHospital-java" class="headerlink" title="interface GoHospital.java:"></a>interface GoHospital.java:</h5><pre><code>/**
 * Created by Adam on 2016/5/27.
 */
public interface GoHospital extends Runnable{
    /**
     * 挂号
     * @return 科室名
     */
    void onRegister(ActionHook register);

    /**
     * 问诊
     * @return 病名
     */
    void onInterview(ActionHook interview);

    /**
     * 开药
     * @return 药名
     */
    void onMedicine(ActionHook medicine);

    /**
     * 付费
     * @return 付了多少钱
     */
    void onPay(ActionHook pay);

    /**
     * 返回取药方式
     * @return 取药方式
     */
    void onGetMedicine(ActionHook getMedicine);

}
</code></pre><h6 id="AbsGoHospital-java"><a href="#AbsGoHospital-java" class="headerlink" title="AbsGoHospital.java"></a>AbsGoHospital.java</h6><pre><code>/**
 * Created by Adam on 2016/5/27.
 */
abstract class AbsGoHospital implements GoHospital{
    private ActionHook register;
    private ActionHook intterview;
    private ActionHook medicine;
    private ActionHook pay;
    private ActionHook getMedicine;

    public void run(){
        if (register != null) {
            boolean isRegisterOk = register.exec();
            if (isRegisterOk &amp;&amp; intterview != null) {
                boolean isIntterviewOk = intterview.exec(isRegisterOk);
                if (isIntterviewOk &amp;&amp; medicine != null) {
                    boolean isMedicineOK = medicine.exec();
                    if (isMedicineOK &amp;&amp; pay != null) {
                        boolean isPayOk = pay.exec();
                        if (isPayOk &amp;&amp; getMedicine != null) {
                            boolean isGetMedicineOk = getMedicine.exec();
                        }
                    }
                }
            }
        }

    }



    /**
     * 挂号
     * @return 科室名
     */
    public void onRegister(ActionHook register) {
        this.register = register;
    }

    /**
     * 问诊
     * @return 病名
     */
    public void onInterview(ActionHook interview) {
        this.intterview = interview;
    }

    /**
     * 开药
     * @return 药名
     */
    public void onMedicine(ActionHook medicine) {
        this.medicine = medicine;
    }

    /**
     * 付费
     * @return 付了多少钱
     */
    public void onPay(ActionHook pay) {
        this.pay = pay;
    }

    /**
     * 返回取药方式
     * @return 取药方式
     */
    public void onGetMedicine(ActionHook getMedicine) {
        this.getMedicine = getMedicine;
    }

}
</code></pre><h6 id="定义一个钩子"><a href="#定义一个钩子" class="headerlink" title="定义一个钩子"></a>定义一个钩子</h6><pre><code>/**
 * Created by Adam on 2016/5/27.
 */
public interface ActionHook {
    /**
     * 钩子逻辑
     * @param args 任意参数
     */
    boolean exec(Object ... args);
}
</code></pre><h6 id="定义GoHostpital的实现类"><a href="#定义GoHostpital的实现类" class="headerlink" title="定义GoHostpital的实现类"></a>定义GoHostpital的实现类</h6><pre><code>/**
 * Created by Adam on 2016/5/27.
 */
public class XiaogangGoHopital extends AbsGoHospital {
    // todo 这里的字段，方法都可以用enum扩展，由于本例为了学习，所以略过
    private String name;
    private String diease;
    private String paymentPre;
    private String getMedicineWay;

    public XiaogangGoHopital(String name, String diease, String paymentPre, String getMedicineWay) {
        this.name = name;
        this.diease = diease;
        this.paymentPre = paymentPre;
        this.getMedicineWay = getMedicineWay;
    }

    @Override
    public String toString() {
        return &quot;XiaogangGoHopital{&quot; +
                &quot;name=&apos;&quot; + name + &apos;\&apos;&apos; +
                &quot;, diease=&apos;&quot; + diease + &apos;\&apos;&apos; +
                &quot;, paymentPre=&apos;&quot; + paymentPre + &apos;\&apos;&apos; +
                &quot;, getMedicineWay=&apos;&quot; + getMedicineWay + &apos;\&apos;&apos; +
                &apos;}&apos;;
    }

    public String getDiease() {
        return diease;
    }

    public void setDiease(String diease) {
        this.diease = diease;
    }

    public String getPaymentPre() {
        return paymentPre;
    }

    public void setPaymentPre(String paymentPre) {
        this.paymentPre = paymentPre;
    }

    public String getGetMedicineWay() {
        return getMedicineWay;
    }

    public void setGetMedicineWay(String getMedicineWay) {
        this.getMedicineWay = getMedicineWay;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    @Override
    public void onRegister(final ActionHook register) {
        super.onRegister(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                register.exec(args);
                System.out.println(name + &quot;骑电瓶车去的&quot;);
                return true;
            }
        });
    }
}
</code></pre><h6 id="LiGoHopital-实现类"><a href="#LiGoHopital-实现类" class="headerlink" title="LiGoHopital 实现类"></a>LiGoHopital 实现类</h6><pre><code>/**
 * Created by Adam on 2016/5/27.
 */
public class LiGoHopital extends AbsGoHospital {
    // todo 这里的字段，方法都可以用enum扩展，由于本例为了学习，所以略过
    private String name;
    private String diease;
    private String paymentPre;
    private String getMedicineWay;

    public LiGoHopital(String name, String diease, String paymentPre, String getMedicineWay) {
        this.name = name;
        this.diease = diease;
        this.paymentPre = paymentPre;
        this.getMedicineWay = getMedicineWay;
    }

    @Override
    public String toString() {
        return &quot;XiaogangGoHopital{&quot; +
                &quot;name=&apos;&quot; + name + &apos;\&apos;&apos; +
                &quot;, diease=&apos;&quot; + diease + &apos;\&apos;&apos; +
                &quot;, paymentPre=&apos;&quot; + paymentPre + &apos;\&apos;&apos; +
                &quot;, getMedicineWay=&apos;&quot; + getMedicineWay + &apos;\&apos;&apos; +
                &apos;}&apos;;
    }

    public String getDiease() {
        return diease;
    }

    public void setDiease(String diease) {
        this.diease = diease;
    }

    public String getPaymentPre() {
        return paymentPre;
    }

    public void setPaymentPre(String paymentPre) {
        this.paymentPre = paymentPre;
    }

    public String getGetMedicineWay() {
        return getMedicineWay;
    }

    public void setGetMedicineWay(String getMedicineWay) {
        this.getMedicineWay = getMedicineWay;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    @Override
    public void onRegister(final ActionHook register) {
        super.onRegister(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                register.exec(args);
                System.out.println(name + &quot;ta开车去的&quot;);
                return true;
            }
        });
    }

    @Override
    public void onPay(final ActionHook pay) {
        super.onPay(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                pay.exec();
                System.out.println(&quot;他不喜欢付钱&quot;);
                return true;
            }
        });
    }
}
</code></pre><h6 id="mainz测试函数"><a href="#mainz测试函数" class="headerlink" title="mainz测试函数"></a>mainz测试函数</h6><pre><code>/**
 * Created by Adam on 2016/5/27.
 */
public class Main {
    public static void main(String[] args) {
        XiaogangGoHopital xiaogangGoHopital = new XiaogangGoHopital(&quot;Gang&quot;, &quot;jiba&quot;, &quot;zhifubao&quot;, &quot;shunfeng&quot;);
        xiaogangGoHopital.onRegister(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                System.out.println(&quot;=====\n外科&quot;);
                return true;
            }
        });
        xiaogangGoHopital.onInterview(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                System.out.println(args[0]);
                System.out.println(&quot;右臂肌肉损伤&quot;);
                return true;
            }
        });
        xiaogangGoHopital.onMedicine(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                System.out.println(&quot;钙片&quot;);
                return true;
            }
        });
        xiaogangGoHopital.onPay(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                System.out.println(&quot;支付宝付了 123 元&quot;);
                return true;
            }
        });
        xiaogangGoHopital.onGetMedicine(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                System.out.println(&quot;顺风快递&quot;);
                return true;
            }
        });

        Thread thread = new Thread(xiaogangGoHopital);
        thread.start();



        LiGoHopital liGoHopital = new LiGoHopital(&quot;Li&quot;, &quot;jiba&quot;, &quot;zhifubao&quot;, &quot;shunfeng&quot;);
        liGoHopital.onRegister(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                System.out.println(&quot;=====\n外科&quot;);
                return true;
            }
        });
        liGoHopital.onInterview(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                System.out.println(&quot;右臂肌肉损伤&quot;);
                return true;
            }
        });
        liGoHopital.onMedicine(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                System.out.println(&quot;钙片&quot;);
                return true;
            }
        });
        liGoHopital.onPay(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                System.out.println(&quot;支付宝付了 123 元&quot;);
                return true;
            }
        });
        liGoHopital.onGetMedicine(new ActionHook() {
            @Override
            public boolean exec(Object... args) {
                System.out.println(&quot;顺风快递&quot;);
                return true;
            }
        });

        Thread thread2 = new Thread(liGoHopital);
        thread2.start();

    }
}
</code></pre><h4 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a>测试结果</h4><p>结果如下：</p>
<pre><code>=====
外科
Gang骑电瓶车去的
true
右臂肌肉损伤
钙片
支付宝付了 123 元
顺风快递
=====
外科
Lita开车去的
右臂肌肉损伤
钙片
支付宝付了 123 元
他不喜欢付钱
顺风快递
</code></pre><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>由于工作任务蛮重，所以实现地很简单，详细代码可以参考hdfs小文件问题的归档程序；<br>理解设计模式，或者实现技巧，才是第一步，以后能够把它熟练运用才是最重要的！</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用scaladiagrams工具构建scala项目的UML图]]></title>
      <url>http://flume.cn/2016/06/13/%E4%BD%BF%E7%94%A8scaladiagrams%E5%B7%A5%E5%85%B7%E6%9E%84%E5%BB%BAscala%E9%A1%B9%E7%9B%AE%E7%9A%84UML%E5%9B%BE/</url>
      <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>阅读spark源码到storage这一块的时候，由于类的继承，调用之间的关系比较复杂，想要画一下UML图，idea自带的diagrams方法对java支持很好，但对scala的一些继承关系支持不佳，因此google了一下有没有可以画scala UML类图的工具，还真找到了：</p>
<p>我是在x64 windows10下面，使用gitbash工具作为shell命令行，亲测可用</p>
<h3 id="clone开源项目scaladiagrams并安装"><a href="#clone开源项目scaladiagrams并安装" class="headerlink" title="clone开源项目scaladiagrams并安装"></a>clone开源项目scaladiagrams并安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/mikeyhu/scaladiagrams.git</div><div class="line"><span class="built_in">cd</span> scaladiagrams</div><div class="line">./build</div></pre></td></tr></table></figure>
<h3 id="安装graphviz工具"><a href="#安装graphviz工具" class="headerlink" title="安装graphviz工具"></a>安装graphviz工具</h3><p>graphviz是一个开源的图形可视化软件，矢量图生成工具，与其他图形软件所不同，它的理念是“所想即所得”，通过dot语言来描述并绘制图形。<br><a href="http://www.graphviz.org/Download_windows.php" target="_blank" rel="external">http://www.graphviz.org/Download_windows.php</a><br>如上链接下载，然后安装即可，将安装路径加入path中，该工具的目的是通过scaladiagrams工具生成的依赖关系画图；</p>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><h4 id="生成依赖关系文件dotFile"><a href="#生成依赖关系文件dotFile" class="headerlink" title="生成依赖关系文件dotFile"></a>生成依赖关系文件dotFile</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./scaladiagrams --source <span class="string">"D:\spark-1.6.0\core\src\main\scala\org\apache\spark\storage"</span> &gt; dotFile</div></pre></td></tr></table></figure>
<p>dotFile文件就是依赖关系的文件：<br>官方命名为 dot语言，是一个表示图的语言，挺好玩的：</p>
<figure class="highlight scala"><figcaption><span>dot语言</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">digraph diagram &#123;</div><div class="line"><span class="string">"BlockException"</span> [style=filled, fillcolor=burlywood]</div><div class="line">  <span class="string">"BlockException"</span> -&gt; <span class="string">"Exception"</span>;</div><div class="line"></div><div class="line"><span class="string">"BlockFetchException"</span> [style=filled, fillcolor=burlywood]</div><div class="line">  <span class="string">"BlockFetchException"</span> -&gt; <span class="string">"SparkException"</span>;</div><div class="line"></div><div class="line"><span class="string">"BlockId"</span> [style=filled, fillcolor=darkorange]</div><div class="line">  </div><div class="line"></div><div class="line"><span class="string">"RDDBlockId"</span> [style=filled, fillcolor=burlywood]</div><div class="line">  <span class="string">"RDDBlockId"</span> -&gt; <span class="string">"BlockId"</span>;</div><div class="line"></div><div class="line">  。。。</div></pre></td></tr></table></figure>
<h4 id="使用graphviz工具画图"><a href="#使用graphviz工具画图" class="headerlink" title="使用graphviz工具画图"></a>使用graphviz工具画图</h4><p>生成svg文件，文件比较大的话建议用这个<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat dotFile | dot -Tsvg &gt; spark_storage.svg</div></pre></td></tr></table></figure></p>
<p>生成png文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat dotFile | dot -Tpng &gt; spark_storage.png</div></pre></td></tr></table></figure></p>
<p>画的效果部分截图如下（就是图有点扁平）：<br><img src="spark_storage_part.png" alt="spark_storage_part.png"></p>
<p>因为好玩，又画了一个spark_core的类图，太大了，不好看，为了部分解决这个问题，只要在dot文件的第一行加入<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rankdir=RL;</div></pre></td></tr></table></figure></p>
<p>即可使得图片稍微好看一点<br><img src="spark_storage_part2.png" alt="spark_storage_part2.png"></p>
<p>引用：<br><a href="http://stackoverflow.com/questions/7227952/generating-uml-diagram-from-scala-sources" target="_blank" rel="external">http://stackoverflow.com/questions/7227952/generating-uml-diagram-from-scala-sources</a><br><a href="https://github.com/mikeyhu/scaladiagrams" target="_blank" rel="external">https://github.com/mikeyhu/scaladiagrams</a><br><a href="http://www.graphviz.org/About.php" target="_blank" rel="external">http://www.graphviz.org/About.php</a><br><a href="http://www.graphviz.org/pdf/dotguide.pdf" target="_blank" rel="external">http://www.graphviz.org/pdf/dotguide.pdf</a><br><a href="http://www.tonyballantyne.com/graphs.html" target="_blank" rel="external">http://www.tonyballantyne.com/graphs.html</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Scala中_的用法]]></title>
      <url>http://flume.cn/2016/06/13/Scala%E4%B8%AD-%E7%9A%84%E7%94%A8%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>在看Spark源码的过程中，遇到了很多对 下划线_的运用，后来经过查阅资料总结如下（感谢万能的知乎,StackOverFlow）：</p>
<ul>
<li><p>作为“通配符”，类似Java中的*。如import scala.math._</p>
</li>
<li><p>:<em>*作为一个整体，告诉编译器你希望将某个参数当作参数序列处理！例如val s = sum(1 to 5:</em><em>)就是将1 to 5当作参数序列处理。向函数或方法传入可变参数时不能直接传入Range或集合或数组对象，需要使用:_</em>转换才可传入</p>
</li>
<li><p>指代一个集合中的每个元素。例如我们要在一个Array a中筛出偶数，并乘以2，可以用以下办法：<br>a.filter(<em>%2==0).map(2*</em>)。<br>又如要对缓冲数组ArrayBuffer b排序，可以这样：<br>val bSorted = b.sorted(_</p>
</li>
<li><p>在元组中，可以用方法_1, _2, _3访问组员。如a._2。其中句点可以用空格替代。</p>
</li>
<li><p>使用模式匹配可以用来获取元组的组员，例如]</p>
</li>
</ul>
<p>val (first, second, third) = t<br>但如果不是所有的部件都需要，那么可以在不需要的部件位置上使用<em>。比如上一例中val (first, second, </em>) = t</p>
<ul>
<li><p>还有一点，下划线_代表的是某一类型的默认值。<br>对于Int来说，它是0。<br>对于Double来说，它是0.0<br>对于引用类型，它是null。</p>
</li>
<li><p>访问tuple变量的某个元素时通过索引_n来取得第n个元素</p>
</li>
<li><p>类的setter方法，比如类A中定义了var f，则相当于定义了setter方法f<em>=，当然你可以自己定义f</em>=方法来完成更多的事情，比如设置前作一些判断或预处理之类的操作</p>
</li>
<li><p>用于将方法转换成函数，比如val f=sqrt _，以后直接调用f(250)就能求平方根了</p>
</li>
<li><p>Spark源码中，私有变量约定俗成以 _开头，比如： </p>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> _conf: <span class="type">SparkConf</span> = _</div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> _eventLogDir: <span class="type">Option</span>[<span class="type">URI</span>] = <span class="type">None</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> _eventLogCodec: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> _env: <span class="type">SparkEnv</span> = _</div></pre></td></tr></table></figure>
<p>引用：</p>
<p><a href="https://www.zhihu.com/question/21622725/" target="_blank" rel="external">https://www.zhihu.com/question/21622725/</a><br><a href="http://stackoverflow.com/questions/8000903/what-are-all-the-uses-of-an-underscore-in-scala" target="_blank" rel="external">http://stackoverflow.com/questions/8000903/what-are-all-the-uses-of-an-underscore-in-scala</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Scala学习资料总结]]></title>
      <url>http://flume.cn/2016/05/20/Scala%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<p>还在路上，持续更新</p>
<p>以前对函数式编程很陌生，初学scala，感觉scala很难学，学习途中走了许多弯路，虽然目前本人只是连scala入门都算不上，希望能够分享一些资料和读后感给和我一样初学scala的同学，少一些搜集资料的时间，也希望能够抛砖引玉，也请更多高手们多多指教，随便修改本帖，有好资料随便贴上。</p>
<hr>
<h4 id="书籍推荐"><a href="#书籍推荐" class="headerlink" title="书籍推荐"></a>书籍推荐</h4><ul>
<li>1.1 快学Scala(Scala for the Impatient)</li>
</ul>
<p>只推荐这一本书，因为比较薄，看得时候也能看懂，但只是大概懂一点点<br>这本纸质书是李总买的公共书籍，也是本人学习scala的第一本书籍，因为读了一遍后并没有直接使用，故刚读完就把里面的知识给忘光了。直到最近需要模块地方知识翻第二遍的时候，才发现这本书写得蛮好的，从简单到困难都有提及，对我来说可以当成工具书。推荐给喜欢纸质书的同学。</p>
<p>本人这里有我从网上收集的每一章的课后答案，供大家下载使用：密码：7ry4</p>
<p><a href="http://pan.baidu.com/s/1cynlgU" target="_blank" rel="external">快学Scala 课后习题答案集合.rar</a></p>
<ul>
<li>1.2 Programming Scala 2nd edition (2015年出版）</li>
</ul>
<p>很有名，没看过，对我来说看起来太厚太慢了</p>
<h4 id="网站推荐"><a href="#网站推荐" class="headerlink" title="网站推荐"></a>网站推荐</h4><p>个人觉得scala的好的学习资料都在网上，只分享个人觉得好的，将我所知分享给各位：</p>
<ul>
<li>2.1 <a href="http://twitter.github.io/scala_school/" target="_blank" rel="external">Scala School</a></li>
</ul>
<p>吐血推荐，twitter的scala教学网站，言简意赅，真的非常非常好，我几经周转看到这个资料，有种相见恨晚的感觉。重要的是有中文版。</p>
<ul>
<li>2.2 <a href="http://www.scala-tour.com/#/welcome" target="_blank" rel="external">scala-tour</a></li>
</ul>
<p>网友做的scala学习网站，之前中文版有bug，现在中文版也很好用。是一个交互式的学习网站，右边是概念，左边直接是代码示例，可以直接修改并运行，很清晰，适合假期在家里学习。</p>
<ul>
<li>2.3 <a href="http://www.scala-lang.org/api/current/" target="_blank" rel="external">scala-api</a></li>
</ul>
<p>scala 官方API不解释，作为浏览器书签</p>
<ul>
<li>2.4 <a href="http://docs.scala-lang.org/cheatsheets/" target="_blank" rel="external">scala快查</a></li>
</ul>
<p>scala官方出的让你快速查阅的网页，就一页涵盖了基本所有的操作</p>
<ul>
<li>2.5 <a href="http://twitter.github.io/effectivescala/index-cn.html" target="_blank" rel="external">effective scala</a></li>
</ul>
<p>twitter的scala资深玩家讲述了一些scala编程需要注意的问题，在纠结用哪一种编程方式或者数据结构好的时候，这是一个很好的参考，有中文版</p>
<ul>
<li>2.6 <a href="http://hongjiang.info/scala/" target="_blank" rel="external">scala 说点什么</a></li>
</ul>
<p>hongjiang大神对scala的一些理解，写得很深入，之前很多scala的英文文档都是hongjiang大神翻译的</p>
<ul>
<li>2.7 <a href="http://www.tuicool.com/articles/2QFRZfE" target="_blank" rel="external">scala编程规范</a></li>
</ul>
<p>国人总结的scala编程规范，写得很全很详细，希望大家能够从中受到启发，写出优质的代码</p>
<ul>
<li>2.8 <a href="http://www.ibm.com/developerworks/cn/java/j-scala01228.html" target="_blank" rel="external">面向 Java 开发人员的 Scala 指南</a><br>面向 Java 开发人员的 Scala 指南</li>
</ul>
<h4 id="工具推荐"><a href="#工具推荐" class="headerlink" title="工具推荐"></a>工具推荐</h4><ul>
<li>3.1 Intellij Idea</li>
</ul>
<p>强烈推荐Intellij Idea（缺点是比较占内存），其对scala的支持是极好的，可以自动检测出你的代码中的一些问题，帮你看想看的源码，极好的单元测试的支持，遵循scala规范的自动格式化等功能</p>
<pre><code>IntelliJ IDEA https://www.jetbrains.com/idea/  
(请支持正版，下载后24小时内删除) http://idea.lanyus.com
</code></pre><p>Idea有一个scala学习的插件，左边是代码，右边是命令结果，学习效果比在shell中好很多。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[flume的http监控参数说明]]></title>
      <url>http://flume.cn/2016/05/18/flume%E7%9A%84http%E7%9B%91%E6%8E%A7%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E/</url>
      <content type="html"><![CDATA[<p>如果要启用http监控，需要在启动flume的时候添加如下命令：</p>
<pre><code>bin/flume-ng agent --conf conf --conf-file conf/avroToHdfs2.conf --name a1 -Dflume.monitoring.type=http -Dflume.monitoring.port=8533 -Dflume.root.logger=INFO,console
</code></pre><p>这样，在flume所在IP的8533端口，就可以接收到如下json串，可以在chrome浏览器上安装 JSONView这个插件，使得阅读这个json串更加方便。</p>
<p>对于一个source，一个channel和一个sink的agent监控如下：</p>
<p>其中 channel的ChannelFillPercentage值是比较具有明显特征的属性，代表channel满时的百分比，重点就在这个值，一般情况下，这个值小于0.01就代表很通畅，sink的速度比source的速度快，如果这个值超过了5，就代表肯定是sink的速度不够快，需要对sink进行调优，或者需要控制source的速率。如果这个值缓慢增加，增加到一定程度，就会出现数据丢失的情况。</p>
<p>结果: 其中src-1是子自定义的source名称</p>
<pre><code>{
&quot;SOURCE.src-1&quot;:{
    &quot;OpenConnectionCount&quot;:&quot;0&quot;,        //目前与客户端或sink保持连接的总数量(目前只有avro source展现该度量)
    &quot;Type&quot;:&quot;SOURCE&quot;,                    
    &quot;AppendBatchAcceptedCount&quot;:&quot;1355&quot;,    //成功提交到channel的批次的总数量
    &quot;AppendBatchReceivedCount&quot;:&quot;1355&quot;,    //接收到事件批次的总数量
    &quot;EventAcceptedCount&quot;:&quot;28286&quot;,    //成功写出到channel的事件总数量，且source返回success给创建事件的sink或RPC客户端系统
    &quot;AppendReceivedCount&quot;:&quot;0&quot;,        //每批只有一个事件的事件总数量(与RPC调用中的一个append调用相等)
    &quot;StopTime&quot;:&quot;0&quot;,            //source停止时自Epoch以来的毫秒值时间
    &quot;StartTime&quot;:&quot;1442566410435&quot;,    //source启动时自Epoch以来的毫秒值时间
    &quot;EventReceivedCount&quot;:&quot;28286&quot;,    //目前为止source已经接收到的事件总数量
    &quot;AppendAcceptedCount&quot;:&quot;0&quot;        //单独传入的事件到Channel且成功返回的事件总数量
},
&quot;CHANNEL.ch-1&quot;:{
    &quot;EventPutSuccessCount&quot;:&quot;28286&quot;,    //成功写入channel且提交的事件总数量
    &quot;ChannelFillPercentage&quot;:&quot;0.0&quot;,    //channel满时的百分比，重点就在这个值，一般情况下，这个值小于0.01就代表很通畅，sink的速度比source的速度快，如果这个值超过了5，就代表肯定是sink的速度不够快，需要对sink进行调优，或者需要控制source的速率。
    &quot;Type&quot;:&quot;CHANNEL&quot;,
    &quot;StopTime&quot;:&quot;0&quot;,            //channel停止时自Epoch以来的毫秒值时间
    &quot;EventPutAttemptCount&quot;:&quot;28286&quot;,    //Source尝试写入Channe的事件总数量
    &quot;ChannelSize&quot;:&quot;0&quot;,            //目前channel中事件的总数量
    &quot;StartTime&quot;:&quot;1442566410326&quot;,    //channel启动时自Epoch以来的毫秒值时间
    &quot;EventTakeSuccessCount&quot;:&quot;28286&quot;,    //sink成功读取的事件的总数量
    &quot;ChannelCapacity&quot;:&quot;1000000&quot;,       //channel的容量
    &quot;EventTakeAttemptCount&quot;:&quot;313734329512&quot; //sink尝试从channel拉取事件的总数量。这不意味着每次事件都被返回，因为sink拉取的时候channel可能没有任何数据
},
&quot;SINK.sink-1&quot;:{
    &quot;Type&quot;:&quot;SINK&quot;,
    &quot;ConnectionClosedCount&quot;:&quot;0&quot;,    //下一阶段或存储系统关闭的连接数量(如在HDFS中关闭一个文件)
    &quot;EventDrainSuccessCount&quot;:&quot;28286&quot;,    //sink成功写出到存储的事件总数量
    &quot;KafkaEventSendTimer&quot;:&quot;482493&quot;,    
    &quot;BatchCompleteCount&quot;:&quot;0&quot;,        //与最大批量尺寸相等的批量的数量
    &quot;ConnectionFailedCount&quot;:&quot;0&quot;,    //下一阶段或存储系统由于错误关闭的连接数量（如HDFS上一个新创建的文件因为超时而关闭）
    &quot;EventDrainAttemptCount&quot;:&quot;0&quot;,    //sink尝试写出到存储的事件总数量
    &quot;ConnectionCreatedCount&quot;:&quot;0&quot;,    //下一个阶段或存储系统创建的连接数量（如HDFS创建一个新文件）
    &quot;BatchEmptyCount&quot;:&quot;0&quot;,        //空的批量的数量，如果数量很大表示souce写数据比sink清理数据慢速度慢很多
    &quot;StopTime&quot;:&quot;0&quot;,            
    &quot;RollbackCount&quot;:&quot;9&quot;,            //
    &quot;StartTime&quot;:&quot;1442566411897&quot;,
    &quot;BatchUnderflowCount&quot;:&quot;0&quot;        //比sink配置使用的最大批量尺寸更小的批量的数量，如果该值很高也表示sink比souce更快
}
}
</code></pre>]]></content>
    </entry>
    
  
  
    
    <entry>
      <title></title>
      <url>http://flume.cn/about/index.html</url>
      <content type="html"><![CDATA[<p>hi，你好</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[timeline]]></title>
      <url>http://flume.cn/timeline/index.html</url>
      <content type="html"></content>
    </entry>
    
  
</search>
